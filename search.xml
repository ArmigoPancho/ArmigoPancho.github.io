<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[AdaBoost简介]]></title>
    <url>%2F2019%2F03%2F14%2FAdaBoost%2F</url>
    <content type="text"><![CDATA[1.基本概念 我们可以对集成学习的思想做一个概括。对于训练集数据，我们通过训练若干个个体学习器，通过一定的结合策略，就可以最终形成一个强学习器。 个体学习器 由一些异质集成的个体学习器由不同算法构成，通常叫做“组件学习器”，有时候直接叫做个体学习器。 基学习器 就是同质的（同一个算法或者相似算法）学习算法集成在一起个体学习器叫做“基学习器”。 比如下图三个基学习器通过一定的结合策略，组合成一个强学习器。 弱分类器 就是通常是指的是泛化性能略优于随机猜测的学习器，基学习器有时候也被称为弱学习器，集成后，通常来讲组合的弱学习器有一个很好的提升。 对基学习器的期望 我们希望每个基学习器好而不同，就是性能很好，并且保持多样性。比如下图（a）中的三个基学习器在不同样本间分类正确（打勾表示分类正确，否则错误），集成后子分类器的准确率是66%，但是集成后是100%。效果提升。（b）并没有提升最终效果。（c）效果从33%降到了0，起负作用了。 2.AdaBoost介绍2.1 思想集成学习大致分为2类，一个是个体学习器之间存在强的依赖关系、必须串行生成的序列方法；另一种是个体学习器之间不存在强依赖关系、可同时生成的并行化方法。前者代表是AdaBoost算法，后者代表是Bagging和随即森林。 Boosting是一族可以将弱学习器提升为强学习器的算法。这族算法的工作机制类似：先从初始训练集训练出一个基学习器，使得先前的基学习器做错的训练样本在后面受到更加多的关注，然后基于调整过后的样本分布来训练下一个基学习器；如此往复进行，直到基学习器数目达到事先预定的数量T值，最终我们将这些T个基学习器进行加权结合。 注意2点：AdaBoost有两种权值的调整，一种是针对分类错误的样本，会把分错的样本在下一个基分类器训练前提高权重；另一个是针对基分类ht，对分类错误率较高的基分类器也会降低权重。参考下图：显然这是个串行的方式，每一次对样本有一个权重调整后再放入下一个弱学习器训练，权重是根据分类的误差率来调整的；最后把所有带权重的弱分类器线性总和即为我们集成的强分类器。有点三个臭裨将顶个诸葛亮的意思。 2.2 算法框架 上面是周志华《机器学习》P174页的算法框架，大致和前面说的差不多但是会有疑惑的点是错误率如何来的？还有最后的αt和数据集Dt分布如何更新？针对这3个问题，有了第三小节。 2.3 公式推导参考自机器学习P175页，错误率就是每次样本分错权重的归一化值。对于第6行怎么来的，参考下图：由于是二分类问题，yi的预测值不是1就是-1，所以只分为2种情况，即预测准确的期望值和预测错误的期望值。 对于第7行怎么来的，看下图：这里参考了索引1，点击这里访问。但是他里面有2个错误，一个是红色虚线部分，一个最后求L的最后第2步，杠掉Wm,i部分。图中已更正!字丑，désolé。 上图是对基分类器用另一种方式的写法。 one more thing：热爱生活，并为之砥砺前行。 3.参考资料 1.AdaBoost原理详：https://www.cnblogs.com/ScorpioLu/p/8295990.html 老刘的博客：https://www.cnblogs.com/pinard/p/6133937.html?utm_source=tuicool&amp;utm_medium=referral 周志华《机器学习》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>AdabBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Word2vec]]></title>
    <url>%2F2019%2F03%2F04%2Fword2vec%2F</url>
    <content type="text"><![CDATA[1.Word2vec向量是什么？1.1独热编码表示（One Hot Representation）Word2vec，由 Google 于 2013 年发表，是一种神经网络实现，可以学习单词的分布式表示。在此之前已经提出了用于学习单词表示的其他深度或循环神经网络架构，但是这些的主要问题是训练模型所需时长间。 Word2vec 相对于其他模型学习得快。Word2Vec 不需要标签来创建有意义的表示。这很有用，因为现实世界中的大多数数据都是未标记的。如果给网络足够的训练数据（数百亿个单词），它会产生特征极好的单词向量。具有相似含义的词出现在簇中，并且簇具有间隔，使得可以使用向量数学来再现诸如类比的一些词关系。着名的例子是，通过训练好的单词向量，“国王 - 男人 + 女人 = 女王”。上面kaggle对word2vec的介绍，看完还是不了解具体它做了什么。理解Wordvec之前先了解One hot representation模式：向量中每一个元素都关联着词库中的一个单词，指定词的向量表示为：其在向量中对应的元素设置为1，其他的元素设置为0。比如下面图所示：只有1处表示当前单词queen，其他单词的位置都是0，one-hot就是这样子表示每一个单词。所以每个单词相互正交，所以也不会有单词之间的联系了。 再如考虑下面的三个特征： [“male”, “female”] [“from Europe”, “from US”, “from Asia”] [“uses Firefox”, “uses Chrome”, “uses Safari”, “uses Internet Explorer”] 将它换成独热编码后，应该是： feature1=[01,10] feature2=[001,010,100] feature3=[0001,0010,0100,1000] 优点：一是解决了分类器不好处理离散数据的问题，二是在一定程度上也起到了扩充特征的作用。 缺点：在文本特征表示上有些缺点就非常突出了。首先，它是一个词袋模型，不考虑词与词之间的顺序（文本中词的顺序信息也是很重要的）；其次，它假设词与词相互独立（在大多数情况下，词与词是相互影响的）；最后，它得到的特征是离散稀疏的。 1.2分布式表示（Distributed Representation）Distributed representation可以解决One hot representation的维度灾难和稀疏问题，它的思路是通过训练，将每个词都映射到一个较短的词向量上来。比如下图我们将词汇表里的词用”Royalty”,”Masculinity”, “Femininity”和”Age”4个维度来表示，King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)(0.99,0.99,0.05,0.7)。当然在实际情况中，我们并不能对词向量的每个维度做一个很好的解释。 有了上面的表示方式，我们就可以用向量来分析词语词之间的关系。比如下面一个有趣的公式： \vec {King} - \vec {Man} + \vec {Woman} = \vec {Queen}怎么训练得到合适的词向量呢？一个很常见的方法是使用神经网络语言模型。 2.神经网络语言模型神经网络语言模型采用的方法一般是一个三层的神经网络结构（当然也可以多层），分为输入层，隐藏层和输出层(softmax层)。这个模型是如何定义数据的输入和输出呢？Word2vec模式中一般分为CBOW(Continuous Bag-of-Words 与Skip-Gram两种模型。CBOW模型根据中心词W(t)的周围词来预测中心词，Skip-gram模型则根据中心词W(t)来预测周围词。两者类似相反的过程。 2.1CBOW模型（Continuous Bag-of-Words ）先看一个网上的例子： 假设我们现在的Corpus是这一个简单的只有四个单词的document：{I drink coffee everyday}我们选coffee作为中心词，window size设为2也就是说，我们要根据”I”,”drink”和”everyday”三个周围词来预测一个中心单词，并且我们希望这个单词是coffee。 （1）首先，初始化周围词的独热码。”I”,”drink”和”everyday”三个周围词的独热码是作为输入层的3个向量（单词向量空间dim为1 x V），目标是预测”coffee”的独热码。上图中的window size：表示当前词与预测词在一个句子中的最大距离是多少，其他参数请参考：https://blog.csdn.net/szlcw1/article/details/52751314 （2）初始化输入权重矩阵W（单词向量空间dim为1 x V，W的维度是V x N，N为自己设定的数），再把所有one hot分别乘以共享的输入权重矩阵W得到向量Vi，图中的hi就是这里的Vi。 （3）然后把所有每个分量Vi相加求平均值得到一个隐层向量V，维度是Nx1，行维度与W的行维度一致。 （4）初始化输出权重W’ （维度N x V），由W’ x V = U可得到我们的中间矩阵U，为后面的输出矩阵做准备。 （5）最后一步就是中间矩阵U输入到softmax分类函数里面去得到最后的输出向量y。其中的每一维斗代表着一个单词，概率最大的index所指示的单词为预测出的中间词（target word）。 （6）预测出的中间词与true label的onehot做比较，误差越小越好。 2.2Skip-Gram模型2.2.1 Skip-Gram介绍以下翻译自CS 224D: Deep Learning for NLP。Skip-Gram Model就是与CBOW相反的情况，根据某个中心词来预测周围c/2个词，就是向左c/2个词，向右c/2个词。总的框架如下图所示： 符号标记：一个中心词$x$；输出向量是$y^j$；V是输入单词矩阵（维度n x V）；$v_i$是$V$的第$i$列，表示单词$w_i$的输入向量；$U$代表输出矩阵（维度n x V）；$u_i$表示$U$的第$i$行，表示单词$w_i$的输出向量。 步骤1：生成输入向量的独热码x 步骤2：我们由上下文$v_c = V x$得到嵌入词单向量 步骤3：由于没有平均值，只需设置$\hat{v}= vc$ 步骤4：用$ u = U v_c=$来计算$u_{c-m}, . . . , u_{c-1}, u_{c+1}, . . . , u_{c+m}$，以此生成2 x m个分数向量 步骤5：用公式$y = softmax(u)$把每一个分数转为概率 步骤6：用我们预测的概率向量$y^{(c-m)}, . . . , y^{(c-1)}, y^{(c+1)}, . . . , y^{(c+m)}$来和真实的独热码进行比较，越接近越好 2.2.2 隐层细节训练模型的真正目的是获得模型基于训练数据学得的隐层权重。为了得到这些权重，我们首先要构建一个完整的神经网络作为我们的“Fake Task”。模型的输入如果为一个10000维的向量，那么输出也是一个10000维度（词汇表的大小）的向量，它包含了10000个概率，每一个概率代表着当前词是输入样本中output word的概率大小。由于输入向量X是one-hot编码，那么只有输入权重向量W中的非零元素才能对隐藏层产生输入。比如下面公式： h = x^TW=W_{k,.}:=v_{wI}\tag{$1$}看不懂的话，看下图，由于左边的独热码矩阵在维度很高时候做相乘的操作时，非常消耗资源，我们这里只要根据独热码不为0的列号取输入矩阵W所对应的行，所以模型中的隐层权重矩阵便成了一个”查找表“（lookup table）。 2.2.3模型优化目标在CBOW中，我们需要设定一个目标函数来估计我们的模型。但是这里不同的是，因为这里假设是用朴素贝叶斯来断开概率之间的联系，就是说，这里假设中心词一旦给定，那所有输出的单词都是相互独立的。故有如下优化目标函数J： 要是得周围词是最佳的词，就是所有预测出来单词的概率是最大，P(Wc-m,…,Wc+m | Wc)是概率在0-1之间，所以底数&gt;1的情况下，log(P(Wc-m,…,Wc+m | Wc))&lt;0，加上负号表示概率越小，损失越大，周围词就越不好。相反概率越大越好。由于所有输出单词都是假设独立的，所以每个单词概率简单的相乘就是总体的概率。第四个等号后面是用softmax函数对概率进行处理了。最后一步拆开来即可。在上面的优化目标下，这里采用随机梯度下降（Stochastic Gradient Descent）来计算未知参数的梯度。 简单的总体讲一下，就是三层网络模型，第一层是输入层，中间一层是隐藏结点，最后一层是输出层。模型需要训练的参数是输入权重和输出权重。建议看论文原文，skip-gram写的不好，没理解透。其实还有针对模型提出的3个训练方法，有兴趣的可以在刘建平的这3篇博客了解。 3.参考1.CBOW：https://www.zhihu.com/question/44832436/answer/266068967 2.CS224D笔记，百度网盘链接：https://pan.baidu.com/s/1UZygZEVFbPO5fcdncJDvow提取码：kxu1 3.刘建平的CBOW与Skip-Gram模型基础：https://www.cnblogs.com/pinard/p/7160330.html) 4.skip介绍： http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/ 5.skip-gram视频，需要翻墙： https://www.youtube.com/watch?v=EHqXB6P1PzA]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[情感分析案例]]></title>
    <url>%2F2019%2F02%2F27%2FSentiment%20Analysis%2F</url>
    <content type="text"><![CDATA[１．数据集介绍本案例是在来自Kaggle网站，所有详细信息均可以参考网址： https://www.kaggle.com/c/word2vec-nlp-tutorial ，本次案例是Bag of Words Meets Bags of Popcorn，我翻译成词带模型爱上爆米花，anyway，数据集有四个文件一个有类标的训练集labeledTrainData如下图所示，就是三列：id、sentiment、review。每条评论是对某电影的评论，只是简单的二分类问题，1表示IMDB评分&gt;=7，若某评论的IMDB评级&lt;5会导致情绪评分为0，这里已经打好了类标记。 testData是一个没有类标的测试集，预测提交到kaggle自然会打分，sampleSubmission是以正确格式的逗号分隔的示例提交文件，还有unlabeledTrainData**是另外50,000个IMDB评论没有提供任何评级标签。 2.TF-IDF和Word2vec在贝叶斯、逻辑回归方法中的比较下面两段代码都是来自kaggle网上和其他参考资料，已经过不动脑子的誊写 :）+运行，把数据放在一个py文件同目录下即可： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110import reimport pandas as pdfrom bs4 import BeautifulSoupimport numpy as npfrom sklearn.model_selection import cross_val_predictfrom sklearn.feature_extraction.text import TfidfVectorizer as TFIDFfrom sklearn.naive_bayes import MultinomialNB as MNBfrom sklearn.linear_model import LogisticRegression as LRfrom sklearn.grid_search import GridSearchCVfrom sklearn.model_selection import cross_val_score# 参考：http://blog.csdn.net/longxinchen_ml/article/details/50629613def review_to_wordlist(review): ''' 把IMDB的评论转成词序列 参考：http://blog.csdn.net/longxinchen_ml/article/details/50629613 ''' # 去掉HTML标签，拿到内容 review_text = BeautifulSoup(review, "html.parser").get_text() # 用正则表达式取出符合规范的部分 review_text = re.sub("[^a-zA-Z]", " ", review_text) # 小写化所有的词，并转成词list words = review_text.lower().split() # 返回words return wordsroot_dir = "../word2vec-nlp-tutorial"train = pd.read_csv('%s/%s' % (root_dir, 'labeledTrainData.tsv'), header=0, delimiter="\t", quoting=3)test = pd.read_csv('%s/%s' % (root_dir, 'testData.tsv'), header=0, delimiter="\t", quoting=3)# 预处理数据label = train['sentiment']train_data = []for i in range(len(train['review'])): train_data.append(' '.join(review_to_wordlist(train['review'][i])))test_data = []for i in range(len(test['review'])): test_data.append(' '.join(review_to_wordlist(test['review'][i])))"""特征处理直接丢给计算机这些词文本，计算机是无法计算的，因此我们需要把文本转换为向量，有几种常见的文本向量处理方法，比如：1)单词计数2)TF-IDF向量3)Word2vec向量我们先使用TF-IDF来试一下min_df: 最小支持度为2（词汇出现的最小次数）max_features: 默认为None，可设为int，对所有关键词的term frequency进行降序排序，只取前max_features个作为关键词集strip_accents: 将使用ascii或unicode编码在预处理步骤去除raw document中的重音符号analyzer: 设置返回类型token_pattern: 表示token的正则表达式，需要设置analyzer == 'word'，默认的正则表达式选择2个及以上的字母或数字作为token，标点符号默认当作token分隔符，而不会被当作tokenngram_range: 词组切分的长度范围use_idf: 启用逆文档频率重新加权use_idf：默认为True，权值是tf*idf，如果设为False，将不使用idf，就是只使用tf，相当于CountVectorizer了。smooth_idf: idf平滑参数，默认为True，idf=ln((文档总数+1)/(包含该词的文档数+1))+1，如果设为False，idf=ln(文档总数/包含该词的文档数)+1sublinear_tf: 默认为False，如果设为True，则替换tf为1 + log(tf)stop_words: 设置停用词，设为english将使用内置的英语停用词，设为一个list可自定义停用词，设为None不使用停用词，设为None且max_df∈[0.7, 1.0)将自动根据当前的语料库建立停用词表"""tfidf = TFIDF(min_df=2, max_features=None, strip_accents='unicode', analyzer='word', token_pattern=r'\w&#123;1,&#125;', ngram_range=(1, 3), # 二元文法模型 use_idf=1, smooth_idf=1, sublinear_tf=1, stop_words = 'english') # 去掉英文停用词# 合并训练和测试集以便进行TFIDF向量化操作data_all = train_data + test_datalen_train = len(train_data)tfidf.fit(data_all)data_all = tfidf.transform(data_all)# 恢复成训练集和测试集部分train_x = data_all[:len_train]test_x = data_all[len_train:]print('TF-IDF处理结束.')print("train: \n", np.shape(train_x[0]))print("test: \n", np.shape(test_x[0]))"""1.朴素贝叶斯训练模型"""model_NB = MNB()MNB(alpha = 1.0,class_prior=None,fit_prior=True)model_NB.fit(train_x,label)print("多项式贝叶斯模型10折交叉验证得分：",np.mean( cross_val_score(model_NB, train_x, label, cv = 10, scoring ='roc_auc')))"""2.使用逻辑回归模型来测试一遍"""#设定网格搜索参数grid_values = &#123;'C':[30]&#125;#设定打分为roc_aucmodel_LR = GridSearchCV(LR(penalty='l2',dual=True,random_state =0),grid_values,scoring = 'roc_auc',cv=2)#放入数据model_LR.fit(train_x,label)print("训练结果：",model_LR.grid_scores_, '\n', model_LR.best_params_, model_LR.best_score_)#预测结果test_results = np.array(model_LR.predict(test_x))print("预测结果是")submission_df = pd.DataFrame(data=&#123;'id':test['id'],'sentiment':test_results&#125;)print(submission_df.head(10))submission_df.to_csv('../submission_br.csv',columns = ['id','sentiment'], index = False) 下面是Word2vec在GNB中的运用： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140"""3.高斯贝叶斯+Word2vec训练"""import gensimimport nltkimport reimport numpy as npimport pandas as pdfrom nltk.corpus import stopwordsfrom bs4 import BeautifulSoupimport timefrom gensim.models import Word2Vecfrom sklearn.naive_bayes import GaussianNB as GNBfrom sklearn.cross_validation import cross_val_scoreimport warningswarnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')import gensim#文本预处理root_dir = "../word2vec-nlp-tutorial"train = pd.read_csv('%s/%s' % (root_dir, 'labeledTrainData.tsv'), header=0, delimiter="\t", quoting=3)test = pd.read_csv('%s/%s' % (root_dir, 'testData.tsv'), header=0, delimiter="\t", quoting=3)tokenizer = nltk.data.load('file:D:/nltk_data/tokenizers/punkt/english.pickle') #若没有file：开头，误以为是http协议开头，自然找不到或格式错误了 def review_to_wordlist(review, remove_stopwords=False): # review = BeautifulSoup(review, "html.parser").get_text() review_text = re.sub("[^a-zA-Z]"," ", review) words = review_text.lower().split() if remove_stopwords: stops = set(stopwords.words("english")) words = [w for w in words if not w in stops] return(words)def review_to_sentences( review, tokenizer, remove_stopwords=False ): ''' 将评论段落转换为句子，返回句子列表，每个句子由一堆词组成 ''' raw_sentences = tokenizer.tokenize(review.strip()) sentences = [] for raw_sentence in raw_sentences: if len(raw_sentence) &gt; 0: # 获取句子中的词列表 sentences.append( review_to_wordlist( raw_sentence, remove_stopwords )) return sentences# 预处理数据label = train['sentiment']train_data = []for i in range(len(train['review'])): train_data.append(' '.join(review_to_wordlist(train['review'][i])))test_data = []for i in range(len(test['review'])): test_data.append(' '.join(review_to_wordlist(test['review'][i])))sentences = []for i, review in enumerate(train["review"]): # print(i, review) sentences += review_to_sentences(review, tokenizer, True)print(np.shape(train["review"]))print(np.shape(sentences)) # 模型参数num_features = 300 # Word vector dimensionality min_word_count = 40 # 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5 num_workers = 4 # Number of threads to run in parallelcontext = 10 # Context window size downsampling = 1e-3 # Downsample setting for frequent words# 训练模型#word2vec参数解释： https://blog.csdn.net/laobai1015/article/details/86540813print("训练模型中...")model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count=min_word_count, window=context, sample=downsampling)print("训练完成")print('保存模型...')model.init_sims(replace=True)model_name = "%s/%s" % (root_dir, "300features_40minwords_10context.txt")model.save(model_name)print('保存结束')def makeFeatureVec(words, model, num_features): ''' 对段落中的所有词向量进行取平均操作 ''' featureVec = np.zeros((num_features,), dtype="float32") nwords = 0. # Index2word包含了词表中的所有词，为了检索速度，保存到set中 index2word_set = set(model.wv.index2word) for word in words: if word in index2word_set: nwords = nwords + 1. featureVec = np.add(featureVec, model[word]) # 取平均 featureVec = np.divide(featureVec, nwords) return featureVecdef getAvgFeatureVecs(reviews, model, num_features): ''' 给定一个文本列表，每个文本由一个词列表组成，返回每个文本的词向量平均值 ''' counter = 0 reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype="float32") for review in reviews: if counter % 5000 == 0: print("Review %d of %d" % (counter, len(reviews))) reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features) counter = counter + 1 return reviewFeatureVecstrainDataVecs = getAvgFeatureVecs(train_data, model, num_features)print(np.shape(trainDataVecs))testDataVecs = getAvgFeatureVecs(test_data, model, num_features)print(np.shape(testDataVecs))model_GNB = GNB()model_GNB.fit(trainDataVecs,label)print("高斯贝叶斯分类器10折交叉验证得分: ", np.mean(cross_val_score(model_GNB, trainDataVecs, label, cv=10, scoring='roc_auc')))print("保存结果。。。")result = model_GNB.predict(testDataVecs)submission_df = pd.DataFrame(data=&#123;'id':test['id'],'sentiment':result&#125;)print(submission_df.head(10))submission_df.to_csv('C:/Users/asus1/Desktop/gnb_word2vec.csv',columns = ['id','sentiment'], index = False)]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP，情感分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《亡徵》]]></title>
    <url>%2F2018%2F12%2F24%2Fperdition%2F</url>
    <content type="text"><![CDATA[1.《韩非子》战国 (公元前475年 - 公元前221年)时期，百家争鸣，百花齐放，实在是难得。韩非（约公元前280年—公元前233年），战国时期韩国都城新郑（今河南省郑州市新郑市）人 [1] ，法家代表人物，杰出的思想家、哲学家和散文家。韩王之子，荀子学生，李斯同门师兄。下面是《四部丛刊初编》中第350～352册。景上海涵芬楼藏景宋钞校本，本书二十卷，完整版见此链接： https://ctext.org/library.pl?if=gb&amp;file=77707&amp;page=4&amp;remap=gb 。 2.《亡徵》原文​ 凡人主之国小而家大，权轻而臣重者，可亡也。简法禁而务谋虑，荒封内而恃交援者，可亡也。群臣为学，门子好辩，商贾外积，小民右仗者，可亡也。好宫室台榭陂池，事车服器玩好，罢露百姓，煎靡货财者，可亡也。用时日，事鬼神，信卜筮，而好祭祀者，可亡也。听以爵不待参验，用一人为门户者，可亡也。官职可以重求，爵禄可以货得者，可亡也。缓心而无成，柔茹而寡断，好恶无决，而无所定立者，可亡也。饕贪而无餍，近利而好得者，可亡也。喜淫而不周于法，好辩说而不求其用，滥于文丽而不顾其功者，可亡也，浅薄而易见，漏泄而无藏，不能周密，而通群臣之语者，可亡也。很刚而不和，愎谏而好胜，不顾社稷而轻为自信者，可亡也。恃交援而简近邻，怙强大之救，而侮所迫之国者，可亡也。羁旅侨士，重帑在外，上闲谋计，下与民事者，可亡也。民信其相，下不能其上，主爱信之而弗能废者，可亡也。境内之杰不事，而求封外之士，不以功伐课试，而好以名问举错，羁旅起贵以陵故常者，可亡也。轻其适正，庶子称衡，太子未定而主即世者，可亡也。大心而无悔，国乱而自多，不料境内之资而易其邻敌者，可亡也。国小而不处卑，力少而不畏强，无礼而侮大邻，贪愎而拙交者，可亡也。太子已置，而娶于强敌以为后妻，则太子危，如是，则群臣易虑，群臣易虑者，可亡也。怯慑而弱守，蚤见而心柔懦，知有谓可，断而弗敢行者，可亡也。出君在外而国更置，质太子未反而君易子，如是则国携，国携者，可亡也，挫辱大臣而狎其身，刑戮小民而逆其使，怀怒思耻而专习则贼生，贼生者，可亡也。大臣两重，父兄众强，内党外援以争事势者，可亡也。婢妾之言听，爱玩之智用，外内悲惋而数行不法者，可亡也。简侮大臣，无礼父兄，劳苦百姓，杀戮不辜者，可亡也。好以智矫法，时以行集公，法禁变易，号令数下者，可亡也。无地固，城郭恶，无畜积，财物寡，无守战之备而轻攻伐者，可亡也。种类不寿，主数即世，婴儿为君，大臣专制，树羁旅以为党，数割地以待交者，可亡也。太子尊显，徒属众强，多大国之交，而威势蚤具者，可亡也。变褊（biǎn，狭隘）而心急，轻疾而易动发，心悁忿（yuān fèn，怨怒）而不訾（zī，估量、思考）前后者，可亡也。主多怒而好用兵，简本教而轻战攻者，可亡也。贵臣相妒，大臣隆盛，外藉敌国，内困百姓，以攻怨雠，而人主弗诛者，可亡也。君不肖而侧室贤，太子轻而庶子伉，官吏弱而人民桀，如此则国躁，国躁者，可亡也。藏怒而弗发，悬罪而弗诛，使群臣阴憎而愈忧惧，而久未可知者，可亡也。出军命将太重，边地任守太尊，专制擅命，径为而无所请者，可亡也。后妻淫乱，主母畜秽，外内混通，男女无别，是谓两主，两主者，可亡也。后妻贱而婢妾贵，太子卑而庶子尊，相室轻而典谒重，如此则内外乖，内外乖者，可亡也。大臣甚贵，偏党众强，壅塞主断而重擅国者，可亡也。私门之官用，马府之世，乡曲之善举，官职之劳废，贵私行而贱公功者，可亡也。公家虚而大臣实，正户贫而寄寓富，耕战之士困，末作之民利者，可亡也。见大利而不趋，闻祸端而不备，浅薄于争守之事，而务以仁义自饰者，可亡也。不为人主之孝，而慕匹夫之孝，不顾社稷之利，而听主母之令，女子用国，刑馀用事者，可亡也。辞辩而不法，心智而无术，主多能而不以法度从事者，可亡也。亲臣进而故人退，不肖用事而贤良伏，无功贵而劳苦贱，如是则下怨，下怨者，可亡也。父兄大臣禄秩过功，章服侵等，宫室供养太侈，而人主弗禁，则臣心无穷，臣心无穷者，可亡也。公婿公孙与民同门，暴傲其邻者，可亡也。亡徵者，非曰必亡，言其可亡也。夫两尧不能相王，两桀不能相亡，亡王之机，必其治乱、其强弱相踦者也。木之折也必通蠹，墙之坏也必通隙。然木虽蠹，无疾风不折；墙虽隙，无大雨不坏。万乘之主，有能服术行法以为亡徵之君风雨者，其兼天下不难矣。 3.《亡徵》译文​ 凡属君主国家弱小而臣下强大的，君主权轻而臣下权重的，可能灭亡。轻视法令而好用计谋，荒废内政而依赖外援的，可能灭亡。群臣喜欢私学，贵族子弟喜欢辩术，商人在外囤积财富，百姓崇尚私斗的，可能灭亡。嗜好宫殿楼阁池塘，爱好车马服饰玩物，喜欢让百姓疲劳困顿，压榨挥霍钱财的，可能灭亡。选吉日，信鬼神，迷信占卜而讲究祭祀的，国家就可能灭亡。听凭有爵位的人的意见而不考核验证，只听一个人的话而且由他去办的，国家就可能灭亡。官职可以依靠重臣求得，爵禄可以用财宝换取的，国家就可能灭亡。办事拖拉而无成效，优柔寡断，好坏不分又无主见、行动不定的，国家就可能灭亡。贪得无厌，唯利是图的，国家就可能灭亡。喜欢玩弄词藻但不合乎法规，讲究巧辩而不求实用，沉溺于华美的文采而不顾它的功效的，国家就可能灭亡。君主不学无术而且轻易表露情态，泄露机密而不知隐藏，不能严密防范而又把臣下的进言透露出去的，国家就可能灭亡。凶狠暴虐而不平和，不听别人的忠谏而逞强好胜，不顾国家安危而轻率自信的，国家就可能灭亡。仰仗外国的支援而怠慢近邻，依靠强国的救援而侮辱邻国的，国家就可能灭亡。寄居国外的游客，把大批财宝存放在国外，对上刺探国家机密，对下干预民事的，国家就可能灭亡。百姓相信相国，臣下轻视君主，君主宠爱相国，相信他又不能废黜的，国家就可能灭亡。国内的杰出人才闲置不用，反而去寻求国外的人，不按功劳大小去考核，而喜欢用虚名，把寄寓作客的人破格起用而凌驾故旧的，国家就可能灭亡。轻视嫡子而庶出的掌权，太子还没有确定而君主过世的，国家就可能灭亡。君主狂妄自大而不悔悟，国家混乱而自以为美，不估量本国的实力而看轻邻近敌国的，国家就可能灭亡。国家弱小而又不卑躬谦下，国力不足而又不服强国，没有礼貌而又侮辱强大的邻国，贪婪任性而不善于外交的，国家就可能灭亡。已经有了太子，而又从强大的敌国娶来正妻，那太子就有危险了，这样群臣也会变心；群臣变心的，国家就可能灭亡。胆小怕事而又不敢坚持己见，问题早已发现而因内心懦弱，也知道可以解决，决定了却又不敢执行的，国家就可能灭亡。出国的君主还在国外而国内已另立新君，在国外做人质的太子还没有回国而君主又另立太子，这样群臣就会有二心，群臣有二心的，国家就可能灭亡。打击侮辱大臣而又戏弄他们，杀戮小民而又一反常规地役使他们，小民心怀怨恨牢记耻辱，而君主又专心一意地宠幸而又戏辱他们，于是劫杀的事就会发生；发生劫杀之事的，国家就可能灭亡。两个大臣同时当权，君主的叔伯兄弟又多又强，国内结党谋取外援而争夺权势的，国家就可能灭亡。 ​ 只听侍从妃妾的话，只听宠幸近臣的计策，朝野内外悲愁怨恨而又屡行不法的，国家就可能灭亡。怠慢侮辱大臣，对叔伯兄弟又没有礼貌，使百姓劳乏困苦，杀戮无辜的，国家就可能灭亡。喜欢用自己的小聪明来改变法规，时常把自己的私利混杂在公务之中，法制、禁令常常改动，号令下达无数的，国家就可能灭亡。没有险要的地势可守，城墙不坚固，无粮食储备，财力物力贫乏，没有防守和攻战的准备，又轻举妄动去进攻的，国家就可能灭亡。君主的族人寿命不长，君主又接连死去，婴儿做了君主，奸臣专权，拉拢国外的游勇结为私党，一再割地以求得敌国青睐的，国家就可能灭亡。太子受到尊重而名声显赫，侍从、属下及其党羽众多而且强盛，又与许多大国友好交往，而且很早就有威势的，国家就可能灭亡。心情偏邪狭小而又急躁，轻浮而又极易冲动，当他愤怒时从不慎重思考的，国家就可能灭亡。君主多动怒而好用兵打仗，放松农业生产和平时练兵的，国家就可能灭亡。贵臣互相妒嫉，重臣权势又大，在外借重敌国的势力，对内压榨百姓，用以攻击和自己有怨仇的人，而君主不加诛戮的，国家就可能灭亡。君主不贤而庶出贤，太子轻薄而庶子高尚，官吏懦弱而人民豪横，这样的国家就会动荡不安；动荡不安的国家就可能灭亡。心藏怨恨不敢发作，对罪臣迟迟不予惩办，使群臣在暗中怀恨而更加忧虑畏惧，过了很长时间也不知会有什么结果的，国家就可能灭亡。发兵任命的将领权势太重，驻守边疆的官员地位太高，他们独断专行，直接处理问题而不向君主请示的，国家就可能灭亡。太后淫乱，主母养奸，内外不分，男女无别，这就形成后党一方和君主一方的两种势力、两个主子；有了两个主子的，国家就可能灭亡。王后卑微而婢妾尊贵，太子卑微而庶子尊贵，丞相权轻而掌宾客的官吏权重，这样就会里外不分而轻重颠倒；里外不分、轻重颠倒的，国家就可能灭亡。大臣异常显贵，私党人多势众，阻闭君主的视听，阻碍君主决断而独揽大权的，国家就可能灭亡。豪门贵族私人的属下可以被任用，立过军功的后代可以被排挤，偏僻乡村里的有善名的被举荐，在职官吏的功劳被埋没，重视谋私利的人而看不起为国立功的，国家就可能灭亡。国家的府库空虚而大臣却很富厚，本地居民贫苦而客居之人却很富足，耕作出征之家困乏而工商之家得利的，国家就可能灭亡。看到国家的利益而不赶紧去办，听到有祸乱的苗头而不去及早设防，征战守备之事流于轻浮，只是想着用仁义装扮自己的，国家就可能灭亡。不是想着祖先的社稷，只是羡慕小民对父母的孝敬，不顾国家的利益，而听从王后的意旨，妇女掌政，宦官弄权的，国家就可能灭亡。能言善辩而不合乎法度，聪明伶俐而没有法术，君主多才多艺却不按法度行事的，国家就可能灭亡。新任之臣晋升，原有之臣被辞退，无德无才的人管事，有德有才的靠边，没有功劳的人地位显贵，劳苦为国的人地位卑贱，这样臣下就会怨恨；臣下怨恨的，国家就可能灭亡。君主的叔伯兄弟以及大臣的俸禄品位高过他们的功劳，服饰高过他们的等级，宫室华丽以及供养、消费太奢侈，而君主并不禁止，臣下的贪求就无止境；臣下贪求无止境的，国家就可能灭亡。驸马或公子王孙与百姓同住在一条小巷里，对邻里强横残暴的，国家就可能灭亡。 ​ 说它有灭亡的征兆，并不是说它必然就会灭亡，只是说它有灭亡的可能。两个唐尧不可能相互统一天下，两个夏桀不可能相互灭亡；灭亡与统一天下的机遇，就要看国家的治与乱、强与弱的两端哪一头重了。大树的折断，必定是由于蛀虫蛀蚀的结果；大墙的倒塌，必定是由于缝隙裂开的缘故。然而大树虽说被蛀蚀，没有大风也不会折断；大墙虽然有了裂缝，没有大雨也不会倒塌。万乘大国君主，如果有推行法术的人协助，一定会像暴风骤雨那样，很容易就能摧毁有灭亡的征兆的国家，而兼并天下也就不是什么困难的事了! ​ 一共48种”可亡也”的征兆，古人思想之深远，志虑之广袤，非常人所比。]]></content>
      <categories>
        <category>火花</category>
      </categories>
      <tags>
        <tag>亡徵，韩非子</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[D3.JS]]></title>
    <url>%2F2018%2F12%2F08%2FD3.JS%2F</url>
    <content type="text"><![CDATA[1.可缩放矢量图形（SVG） SVG（Scalable Vector Graphics） 可被非常多的工具读取和修改（比如记事本）,由于使用xml格式定义，所以可以直接被当作文本文件打开，看里面的数据 SVG 与 JPEG 和 GIF 图像比起来，尺寸更小，且可压缩性更强，SVG 图就相当于保存了关键的数据点，比如要显示一个圆，需要知道圆心和半径，那么SVG 就只保存圆心坐标和半径数据，而平常我们用的位图都是以像素点的形式根据图片大小保存对应个数的像素点，因而SVG尺寸更小 SVG 是可伸缩的，平常使用的位图拉伸会发虚，压缩会变形，而SVG格式图片保存数据进行运算展示，不管多大多少，可以不失真显示。作图例子如下： （1）平面上一个圈圈：包涵圈和圈内、圈外。发现这个可缩放的矢量图是在SVG模块里面再添加一个circle模块的：’’svgCircleTutorial’’是svg的id名字、xmlns是XML命名空间，区别开在不同文件中的相同的标志、height=”250”是SVG的高度；cx=”55” cy=”55”表示坐标（55,55）、r是半径、fill是填充圈内颜色、stroke是轮廓的颜色、stroke-width是轮廓宽度。把下面代码保存到txt文件，修改文件名字为circle.html 用任意浏览器打开即可显示下图。 123&lt;svg id="svgCircleTutorial" height="250" xmlns="http://www.w3.org/2000/svg"&gt; &lt;circle id="myCircle" cx="100" cy="100" r="60" fill="#219E3E" stroke="#17301D" stroke-width="10" /&gt;&lt;/svg&gt; （2）rect 元素的 width 和 height 属性可定义矩形的高度和宽度、style属性用来定义CSS属性、CSS 的 fill 属性定义矩形的填充颜色（rgb 值、颜色名或者十六进制值）、CSS 的 stroke-width 属性定义矩形边框的宽度、CSS 的 stroke 属性定义矩形边框的颜色，如下： 1234&lt;svg&gt; &lt;rect width="150" height="150" style="fill:rgb(255,255,0);stroke-width:12;stroke:rgb(0,150,0)"/&gt;&lt;/svg&gt; （3）路径：设置线段坐标和图像填充，fill为white时在背景也是白色的情况下只有楼梯线段了。 123456&lt;svg&gt; &lt;polyline points="0 40,40 40,40 80,80 80,80 120,120 120,120 160" style="fill:blue; stroke:gray; stroke-width:3" /&gt;&lt;/svg&gt; 2.HTML例子由于html也是前端的核心，暂时了解下，直接例子理解可能好些。 （1）title和内容的位置不同： 123456789101112&lt;html&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;我是标题，我在title标签内，我显示在浏览器窗口最顶部&lt;/title&gt;&lt;/head&gt;&lt;body&gt;我是内容，后面是我的博客地址：&lt;a href="http://www.armigo.fun"&gt;这是一个链接&lt;/a&gt;下面是一张图片：&lt;br&gt;&lt;img src="http://pic.qiantucdn.com/58pic/18/06/80/21N58PICBfS_1024.jpg" width="787.2" height="492" /&gt;&lt;/body&gt;&lt;/html&gt; （2）绘制表格： 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt;&lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;每个表格从一个 table 标签开始。每个表格行从 tr 标签开始。每个表格的数据从 td 标签开始。&lt;/p&gt;&lt;h4&gt;一列:&lt;/h4&gt;&lt;table border="2"&gt;&lt;tr&gt; &lt;td&gt;100&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;h4&gt;一行三列:&lt;/h4&gt;&lt;table border="1"&gt;&lt;tr&gt; &lt;td&gt;100&lt;/td&gt; &lt;td&gt;200&lt;/td&gt; &lt;td&gt;300&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;h4&gt;2行3列&lt;/h4&gt;&lt;table border="1"&gt;&lt;tr&gt; &lt;td&gt;11&lt;/td&gt; &lt;td&gt;22&lt;/td&gt; &lt;td&gt;33&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td&gt;14&lt;/td&gt; &lt;td&gt;25&lt;/td&gt; &lt;td&gt;36&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt; （3）页面布局： 12345678910111213141516171819202122&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;title&gt;Armigo&lt;/title&gt; &lt;/head&gt;&lt;body&gt;&lt;div id="container" style="width:500px"&gt;&lt;div id="header" style="background-color:#FFA500;"&gt;&lt;h1 style="margin-bottom:0;text-align:center"&gt;网页标题:(*´▽｀)ノノ&lt;/h1&gt;&lt;/div&gt;&lt;div id="menu" style="background-color:#FFD700;height:200px;width:100px;float:right;"&gt;&lt;b&gt;菜单&lt;/b&gt;&lt;br&gt;HTML&lt;br&gt;SVG&lt;br&gt;JavaScript&lt;/div&gt;&lt;div id="content" style="background-color:#EEEEEE;height:200px;width:400px;float:left;"&gt;内容：你好帅！&lt;/div&gt;&lt;div id="footer" style="background-color:#FFA500;clear:both;text-align:center;"&gt;版权 © ARMIGO.FUN&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3.CSS样式 CSS 指层叠样式表 (Cascading Style Sheets) 样式定义如何显示 HTML 元素 样式通常存储在样式表中 把样式添加到 HTML 4.0 中，是为了解决内容与表现分离的问题 外部样式表可以极大提高工作效率 外部样式表通常存储在 CSS 文件中 多个样式定义可层叠为一 4.D3.JSD3 的全称是（Data-Driven Documents），顾名思义可以知道是一个被数据驱动的文档。说简单一点，其实就是一个 JavaScript 的函数库，使用它主要是用来做数据可视化的。 （1）文字展示方式： 12345678910111213141516&lt;body&gt;&lt;p1&gt;Hello World 1&lt;/p1&gt;&lt;br&gt;&lt;p2&gt;Hello World 2&lt;/p2&gt;&lt;script src=&quot;http://d3js.org/d3.v3.min.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;script&gt;d3.select(&apos;body&apos;).selectAll(&apos;p&apos;);//选择&lt;body&gt;中所有的&lt;p&gt;，其文本内容为practise，选择集保存在变量 p 中var p=d3.select(&apos;body&apos;) .select(&apos;p1&apos;) .text(&apos;practise&apos;);//修改段落颜色和字体大小p.style(&quot;color&quot;,&quot;red&quot;) .style(&quot;font-size&quot;,&quot;36px&quot;)&lt;/script&gt;&lt;/body&gt;]]></content>
      <categories>
        <category>可视化工具</category>
      </categories>
      <tags>
        <tag>D3.JS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯模型]]></title>
    <url>%2F2018%2F12%2F03%2FNB%2F</url>
    <content type="text"><![CDATA[1.朴素贝叶斯公式1.1概念简述简介下三个重要的条件概率的公式： \begin{align*} &(1)条件概率公式: P(A|B)=P(AB)/P(B)\ ，其中P(B)>0 \\ &(2)乘法公式：P(A∩B)=P(AB)=P(A|B)P(B)=P(B|A)P(A)\\ &\quad\ 推广：P(A1A2...An)=P(A1)P(A2|A1)P(A3|A1A2)...P(An|A1...An-1)\\ &(3)全概率公式：P(A)=\sum_{i=1}^{n}P(A|B_{i})P(B_{i}) \end{align*} （1）条件概率公式设A,B是两个事件，且P(B)&gt;0,则在事件B发生的条件下，计算事件A发生的条件概率。在条件概率中，最本质的变化是样本空间缩小了——由原来的整个样本空间缩小到了给定条件的样本空间。 （2）乘法公式表示A、B同时发生的概率可以看作两个概率的乘积。推广时候，就是多个事件交集的概率可以拆成单个事件Ai的条件概率的乘积形式。 （3）全概率公式就是下图中把整个样本空间S给划分为n块B区域，其中有一块彩色A事件，那么A事件可由每个事件Bi条件下A的部分概率求和而成。 1.2贝叶斯公式（Naive Bayes）废话不多说，先上朴素贝叶斯公式： \begin{align*} P(c_i|x) &= \frac{P(x|c_i)P(c_i)}{P(c_i)}\\ &= \frac{P(x|c_i)P(c_i)}{\sum_i P(x|c_i)P(c_i)} \end{align*}其中，第一个没有代入全概率公式。$P(c_i)$是先验概率（Prior probability），就是在条件x发生之前对类c有一个经验上的估计值，这个估计值是主观的、人为的，但是也是依据一定实际观察或者经验所得。由于主观的猜测值没有令人信服的证明，所以被频率学派所疑虑。比如我们可以猜想对抛1次硬币得到正面的概率为0.5，因为不是正面就是反面（假设没有竖立情况），这就是根据的经验值，因为你并没有去做大量（无穷）的实验来证明。 $P(c_i|x)$是后验概率（Posterior probability）。比如有十个不透明盒子，只有一个盒子里有球，则猜中有球的概率为1/10。当事件 x：打开一个盒子发现没有球，发生后，此时若再打开一个盒子有球的概率就变成了1/9。后验概率是建立在先验概率的基础上，通过贝叶斯公式建立起来的。它是一种“知果求因”的思路，这里打开后的“结果”发现没有球，此时才知道“原因”的概率是1/9。 $P(x|c_i)$是似然函数（Likelyhood），在机器学习中更多的时候它表示了第i个类C中出现属性x的概率，我们希望是使得类c中出现属性x的概率越大越好，因为反过来$P(c_i|x)$概率也越大。统计学中似然函数用符号$f(x|\theta)$，其中θ是需要估计的参数θ(事先假设已经确定有唯一θ)，x是随机变量x，是假设参数θ已知后我们观察到的样本应该是什么样子的，即在此参数θ下样本似然真实样本的程度，旨在最大化下面的似然函数以求需要估计的参数θ： L(\theta) = \prod_{i=1}^{n} f(x_i|\theta)上面就是大名鼎鼎的最大似然估计（MLE），是用来估计参数的方法，还有一种叫矩估计。最大似然估计和矩估计都是频率学派的经典方法。最大似然估计就是每个变量对应似然函数$f(x_i|\theta)$的乘积结果最大化即可停止，可获估计的参数θ。关于参数估计深入一点，有一个置信区间和可信区间区别，假设我们以一批样本可能服从高斯分布，通过多个样本X估计得到参数μ、σ后，置信区间表示有90%的置信程度是当前样本下的高斯分布是以这组参数μ、σ而构造，这里的置信区间只与样本相关，与参数θ无关，频率学派 认为模型的参数是固定的,不会随着样本的变化而变化，只是当前样本产生的某个参数可信度或大或小，抽样越多，频率越接近于真实概率，估计得到的参数越可信，即假定参数是唯一的，只是我们不知道，通过不断的抽样去估计这个参数罢了。而可信区间表示我们有很多个这样的参数，这些参数组成一个可信区间，从中抽取一个概率大一点的使用，比如90%下的参数。置信区间只有一组或一个参数，而可信区间有多组或多个参数。再提一下贝叶斯学派 观点：贝叶斯学派则认为参数不是唯一的，是一个随机变量，服从一个先验分布，而样本是固定的，通过先验分布、后验分布来获取参数。 分母部分$\sum_i{P(x|c_i)P(c_i)}$叫做”证据因子“（Evidence )，以保证各类别的后验概率总和为1从而满足概率条件，表示各个类别的出现属性x的概率总和。发现贝叶斯公式后验概率与分子部分成正比： P(c_i|x)∝P(x|c_i)P(c_i)1.2贝叶斯例子例子1： 症状 职业 疾病 打喷嚏 护士 感冒 打喷嚏 农夫 过敏 头疼 建筑工人 脑震荡 头疼 建筑工人 感冒 打喷嚏 教师 感冒 头疼 教师 脑震荡 某个医院早上收了六个门诊病人，如上表，现在又来了第七个病人，是一个打喷嚏的建筑工人。请问他患上感冒的概率有多大？ 根据公式$P(c_i|x) = \frac{P(x|c_i)P(c_i)}{P(c_i)}$ 可知： P(感冒|打喷嚏，建筑工人)=\frac{P(打喷嚏，建筑工人|感冒)\ast P(感冒)}{P(打喷嚏，建筑工人)}假设打喷嚏和建筑工人是相互独立的，则公式变为： P(感冒|打喷嚏，建筑工人)=\frac{P(打喷嚏|感冒)\ast P(建筑工人|感冒)\ast P(感冒)}{P(打喷嚏)\ast P(建筑工人)}其中：$P(感冒)=\frac{1}{2}$，$P(打喷嚏|感冒)=\frac{2}{3}$，$P(建筑工人|感冒)=\frac{1}{3}$，$P(打喷嚏) \ast P(建筑工人)=\frac{1}{2}\ast\frac{1}{3}=\frac{1}{6}$ ，代入上式子即可得： P(感冒|打喷嚏，建筑工人)=\frac{\frac{2}{3}*\frac{1}{3}*{\frac{1}{2}}}{\frac{1}{6}}=\frac{2}{3}\approx0.666说明此打喷嚏的建筑工人患感冒的概率为60.6%，依次计算打喷嚏、建筑工人的条件下患过敏、脑震荡的概率，然后有三个类别的后验概率，取最大的值作为该情况的预测值即可，这种模型常用于文本分类等。 例子2： 一种癌症，得了这个癌症的人被检测出为阳性的几率为90%，未得这种癌症的人被检测出阴性的几率为90%，而人群中得这种癌症的几率为1%，一个人被检测出阳性，问这个人得癌症的几率为多少？猛地一看，被检查出阳性，而且得癌症的话阳性的概率是90%，概率蛮大的，算下来看看。 我们用$A$表示事件 “测出为阳性”, 用$B_1$表示“得癌症”, $B_2$表示“未得癌症”。根据题目，我们知道如下信息: P(A|B_{1}) = 0.9, P(A|B_{2}) = 0.1, P(B_{1}) = 0.01, P(B_{2}) = 0.99那么我们现在想得到的是已知为阳性的情况下，得癌症的几率$P(B_{1},A)：$ P(B_{1},A) = P(B_{1}) \cdot P(A|B_{1}) = 0.01 \times 0.9 = 0.009表示1000人中只有9个人检测为阳性的是真的得癌症的。再来看看检测出阳性但是没有得癌症的概率： P(B_{2},A) = P(B_{2}) \cdot P(A|B_{2}) = 0.99 \times 0.1 = 0.099即1000人中间只有99个人检测为阳性但是没有得癌症的。那么在检测出阳性的前提下得癌症的概率，即把上面得值归一化即可： P(B_{1}|A)=\frac{0.009}{0.099 + 0.009} \approx 0.083显然，检测为阳性不得癌症的概率约是0.917，那么我们检测为阳性后，也不必万念俱灰，至少Thomas Bayes告诉你，你还有救。在这点上，他果然是上帝。 通过例子知道，贝叶斯的分母可以有2种计算方式，一种是以每个事件都是独立的，连乘积的形式。另一种是全概率公式展开的形式，依据不同情况活用。具体的算法流程请参考： https://blog.csdn.net/dataningwei/article/details/54140537 ，注意到此链接中因为分母对每类后验概率一样，所以后验概率与分子成正比，所以直接最大化分子部分即可。 2.多项式朴素贝叶斯朴素贝叶斯模型最大的特点是基于贝叶斯公式和特征的条件概率是独立的，这就给我们很大的简化空间和计算的方便。多项式朴素贝叶斯（Multinomial Naive Bayes） 比如对于文本分类来说，我们的目标就是获得最大的某一类的后验概率，这里我们假设每个特征之间是独立的： f(x)=argmax_{c_{k}}\ P(c_{k}|x)=argmax_{c_{k}} \frac{P(c_{k})\prod_{i=1}^{n}P(x_{i}|c_{k})}{\sum_{k}P(c_{k})\prod_{i=1}^{n}P(x_{i}|c_{k})}由于所有类的分子一样，所以直接做大化分母即可： f(x)=argmax \ P(c_{k})\ast\prod_{i=1}^{n}P(x_{i}|c_{k})对于$P(c_k),P(x_{i}|c_{k})$计算方法如下： P(c_{k})=\frac{N_{c_{k}}+\alpha}{N+k\alpha}N是总的样本个数，k是总的类别个数，$N_{c_k}$是类别为$c_k$的样本个数，$α$是平滑值。 P(x_{i}|c_{k})=\frac{N_{c_{k},x_{i}}+\alpha}{N_{c_{k}}+n\alpha}$N_{c_{k}}$是第k类$c_{k}$的样本个数，n是特征x的取值个数，$N_{c_{k},x_{i}}$是类别为$c_k$的样本中，第i维特征的值是$x_i$的样本个数，$α$是平滑值。因为若在预测时候，若预测中有特征没有出现在训练集中，那么$P(x_{i}|c_{k})$是0，所以整个乘积都是0,这样就没有意义了。为了防止概率为0，加上一个平滑，让它较好的反映原始概率。 当α=1时，称作Laplace平滑，当0&lt;α&lt;1时，称作Lidstone平滑，α=0时不做平滑。 例子可以参考这篇文章： https://blog.csdn.net/u012162613/article/details/48323777 ，里面还有其他2种模式的解释，不错哦。 3.NB和其他NB的区别（1）相同点：都是以贝叶斯模型为基础。 （2）朴素贝叶斯和多项式贝叶斯的区别： ​ 先看完此链接，有一个基本的认识： https://blog.csdn.net/gaotihong/article/details/78803197 ，主要对NB和MNB进行了概括，可以知道不同点有词向量的表示方式不同，NB是用0或1表示词在字典中出现与否，并且每个垃圾邮件的词向量一样长；但是MNB词向量是以每篇词长度为维度，词在字典中的序号为标记表示向量。另一个不同点我自己看的，在于似然函数的表示不同，我认为这才是真正的本质区别： NB算法图： MNB算法图： 可以看到上面第一张图NB算法中最下面似然函数的表示，垃圾邮件中词的概率：P（某词 j|垃圾邮件=1）=词j在几个垃圾邮件中出现/垃圾邮件的总个数 第二张图中MNB算法的似然函数：P（词 j |1=垃圾）= 所有垃圾邮件中词j出现的次数/所有垃圾邮件的词总量 可以发现似然函数不同。总结下：个别论文说是朴素贝叶斯只关心特征（词）出现还是不出现在一个类别中，但是多项式贝叶斯以每个特征（词）在样本集里的频次来决定类标。这样对NB和MNB有一个更加清晰的认识。 （3）朴素贝叶斯和各个贝叶斯之间的区别： （1）朴素贝叶斯的假设前提有两个第一个为：各特征彼此独立；第二个：对被解释变量的影响一致，不能进行变量筛选。贝叶斯显然是没有独立的前提，即各个特征之间可以有依赖的，也可以独立。 （2）GaussianNB、MultinomialNB和BernoulliNB的似然函数的计算方式不同。GaussianNB就是似然函数$P( x_{i} | c_{k})$为高斯分布的朴素贝叶斯，MultinomialNB就是似然函数为多项式分布的朴素贝叶斯，而BernoulliNB就是似然函数为伯努利分布的朴素贝叶斯。 （3）一般来说，如果样本特征的分布大部分是连续值，使用GaussianNB会比较好。如果如果样本特征的分大部分是多元离散值，使用MultinomialNB比较合适。而如果样本特征是二元离散值或者很稀疏的多元离散值，应该使用BernoulliNB。 4.参考（1）贝叶斯的三种模型 （2）例子1来源 （3）贝叶斯和MCMC博客 （4）贝叶斯网络与朴素贝叶斯的区别]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Naive Bayes</tag>
        <tag>多项式朴素贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[向量空间模型（VSM）]]></title>
    <url>%2F2018%2F11%2F28%2F%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[1.词袋模型(BOW)​ 简单理解下词带模型，顾名思义就是一个袋里装了很多单词，可知特点：（1）假设词语无序 （2）假设词与词之间独立。Bag-of-words模型是信息检索领域常用的文档表示方法。忽略了它的单词顺序和语法、句法等要素，将其仅仅看作是若干个词汇的集合。 它具体是如何表示的？如下有一个句子： I have a pear and a strawberry. 于是可以构建一个字典，如下： {“i”:0,”have”:1,”a”:2,”pear”:3,”and”:4,”strawberry”:5} ，这个字典里面的字符可表示出所有的词，且唯一。但是字典里面词的顺序和原句子词序并不相关，这里只是特例。于是可以建立一个词向量如下： [1,1,2,1,1,1] ，这里是字典里面的词在原句子里出现的次数的统计，一一对应。于是若有若干句子组成一篇文档，则可以构建一个对应的字典，然后有一个N维的向量来表示这篇文档（词频的统计）。若要比较2篇文档相似度，也可以比较2个向量之间的余弦相似度。 2.向量空间模型(VSM)2.1VSM概念​ 向量空间模型（Vector Space Model）和词袋模型很像（个别说是一样的），都是把一个文档表示成向量的模式，而向量空间模型运用较为广泛的是TF-IDF方法。 第1：把一篇文档表示成词向量：D=[W_1,W_2,W_3,…,W_m],某D文档中有m个项。这里可以是的单个词，可以是词组等等。 第2：计算每个项的贡献度：Q =[Q_1,Q_2,Q_3,…,Q_m] ,这里的Q_i是每个项的贡献度，名字不一，也可以理解为每一项（每个词）的比重，权重。计算贡献度的方法不一。 于是每个文档便由一组词向量和对应的贡献度向量表示而成。 2.2TF-IDF方法其实就是第2步的算法不同，这里的TF-IDF分为TF（每一项的频率：Term Frequency）和IDF（逆文档率：Inverse Document Frequency），这里的TF，就是每个词在文档中的频次比例。如下： TF =\frac{N_i}{M},N_i是第i个词在某一篇文档的频次，M是该文档的总词量。IDF就是这个词在哪些文档出现了，因为语料库由K篇文档组成，每篇文档M个词。计算如下： IDF =log(\frac{K}{L+1}),K表示语料库有k篇文档，L表示包含该词的文档数量，L+1防止分母为0。那TF-IDF计算公式如下： TF-IDF =\frac{N_i}{M}*log(\frac{K}{L+1})=\frac{N_i}{M}*log(\frac{1}{D_i}),D_i是某个词出现在所有文档的中的频次比例。发现：$log_a^b​$（底数a&gt;1）函数是单调增函数，且定义域在(0,1]时值域为负值，大于1时为单调正数。由于概率是在[0,1]的，倒数就是大于等于1，这里分母取不到0，倒数就不会是正无穷 ，log(b)值就是一个正数，并且b越大IDF就越大。这里其实K/L是某个词的在语料库中的文档率的倒数。所以当K和L的比值越大，IDF就越大，说明K和L比例差的越大，对我们越有利。比如有20篇文档，若一个词在所有文档中出现了1次，K/(L+1)=20/(1+1)=10,log(10)=3.32，相对其他来说是一个蛮大的比重。那这个词对我们来说也确实是重要的，在做文档相似度的时候作用就很大，因为只要这个词出现说明该词有很好的区分性。相反，若K/L的比值很小，比如K/(L+1)=1时，log1=0,比重就很小。为什么该词比重需要设置的小？因为K/(L+1)==1，说明K和L相近，即这个词在每个文档都出现，那这个词很可能是介词、连词等没有区分性的词，比如’the’，’for’,’的’，’在’这些词很大概率出现在所有文档中，所以不重要，权重就小。当然，也可以通过设置停用词的方法来筛掉这些词，这个是数据预处理的部分了。 比如语料库中共有10篇文档，每篇800词。某一篇文档中的”战斗机” 和 “的”都出现了20次，但是”战斗机” 在2篇文档中出现，而”的”在9篇文档中出现，”的”字符的IDF=TF*log1=0，这样算下来”战斗机” 的TF-IDF的值肯定比0大，故”战斗机” 这个词更加重要。然后排序后，可以取前n个重要的词放入分类器做分类也可，计算相似度也可。其实，TF-IDF计算方法类似于交叉熵，贡献度的计算法子还有很多，按下不表。 补充一下我在weka包中遇到的问题：weka里面的TF和IDF可以分开来设置，若两者都是True,则表示TF-IDF=log（1+某文档中某个词的频次） x log（逆文档率），按照这种计算出来得值可以大于1，之前一直没想明白，因为我把TF的频次算成了频率所以一直小于1。 BOW和VSM两者的区别其实很小，我自己看来，就在于贡献度不一。 2.3熵的计算方法这里其实也算是特征选择，本文只给出了特征选择的计算方法。特征选择是一个系统的工程，有自己的方法和技术。本文特指章节2.1里面第2步骤中贡献度的计算方法。 （1）信息熵 前面提到了交叉熵，先理解什么是熵。熵是由香农提出的，记得有位科学家说：若自己余生也发明一个类似“熵”这样的概念，这一生也值了。可见其重要程度，后来证实熵在计算机领域，通信领域，信息论等等都是举足轻重的。到底什么是熵？直接给出离散变量x的熵公式： H(X)=-\sum\limits_{x\in\mathcal{X}}p(x)\log p(x)作用： 用来度量信息的不确定程度。举个栗子：若你在3个盒子中抽一个奖，设每个盒子抽到奖的概率=1/3，代入熵公式=- 1/3 x log(1/3) - 1/3 x log(1/3) - 1/3 x log(1/3) =0.47712。但是第二个人由于商家作弊，使得第2个盒子抽到的概率上升为0.8，其他2个都是0.1,H(X) = -0.1 x log(0.1) x 2 - 0.8 x log(0.8) = 0.277528。显然这个第二个熵变小了，但是确实符合它反映的信息量。因为第二个事件的信息量更加大。信息熵的特点是熵越小，表示信息越纯，即信息量越大。所以，这个值和我们的事实是符合的。 （2）条件熵 定义：在一个条件下，随机变量的不确定性。 ​ 条件熵公式：$H(X|Y)=-\sum\limits_{x,y}{P(X,Y)*log(P(X|Y))}$ 证明如下图： 下面证明 条件熵=联合熵-单独的熵：$H(Y|X)=H(X,Y)-H(X)$： 举个栗子吧： https://zhuanlan.zhihu.com/p/26551798 ,这篇便有。 （3）信息增益（Information Gain） IG=熵 - 条件熵即为： Grain(Y,X)=H(Y)-H(Y|X)例子请参考周志华《机器学习》P75。 （4）交叉熵 交叉熵又叫KL散度，是用来衡量2个概率分布的差异程度。交叉熵即KL散度的计算公式： D_{KL}(p||q)=\sum_{i=1}^np(x_i)log(\frac{p(x_i)}{q(x_i)}) \tag{1}这里的p是真实分布，比如我们遇到一个复杂的p分布，要做的事就是用一个简单的分布q来代替复杂的、不好计算的真实分布p，KL散度就是用来衡量这2个分布的差异。发现log后面p(xi)和q(xi)越接近时候，KL散度值近乎为0。就是说KL散度值越小，两个分布越接近，即可以用后面的q分布来代替真实分布p。在变分推断里会遇到这个概念，我等只能敬而远之。交叉熵在部分问题时候，也可以做为loss函数。比如：https://blog.csdn.net/tsyccnh/article/details/79163834 大致把熵过了一遍，说的好不如做得好，还是多实践吧。$Vouloir\ \ c’est\ \ pouvoir$（有志者，事竟成）。 3.参考1.词袋模型简介 2.逆文档频率法]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>向量空间模型</tag>
        <tag>词袋模型</tag>
        <tag>信息熵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性空间的定义]]></title>
    <url>%2F2018%2F11%2F19%2F%E7%BA%BF%E6%80%A7%E7%A9%BA%E9%97%B4%E5%AE%9A%E4%B9%89%2F</url>
    <content type="text"><![CDATA[做了一个关于线性空间的思维导图，纯粹练练思维导图的作图技术。希望有启发。 Bonne chance.]]></content>
      <categories>
        <category>矩阵分析</category>
      </categories>
      <tags>
        <tag>线性空间定义</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归和python3实现]]></title>
    <url>%2F2018%2F11%2F10%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[1.逻辑回归笔记​ 逻辑回归就是在线性回归的基础上套一个分类函数，当然也可以不用线性回归。下面是我根据周志华的逻辑回归写的笔记。具体请参考周志华的《机器学习》P57-60。第1张图是说明了逻辑回归的预测模型，第2张图是优化目标L的由来，第3张图是详细的求出一阶导数和二阶导数。我觉得这里一阶导数和二阶导数周志华这里写的不好，矩阵计算写的不清晰明朗。当然这也是自己基础不牢靠所致，共勉吧。如下： 图1：得到两个后验概率的表达式P0和P1，用极大似然估计法找出优化目标L。 图2：再把P0，P1和转化后等价的似然项代入对数似然L,简化后即可得最终优化目标L。 图3：L对2个参数的求导。 最后由牛顿法得到迭代参数公式和优化目标L。 ​ 在之前计算的时候出现了奇异矩阵，即矩阵的行列式值为0，想了下可能是之前的数据之间存在线性相关性，所以导致矩阵的秩不满秩，所以也就不可逆了，即变成了奇异矩阵。若我不做实验，就不会知道奇异矩阵，也就记不起来以前的秩了。想着一些小算法，可以重复造轮子，但是复杂的算法就算了，没必要。人生奇美，怎可废！ ​ 2.python3实现用的python3.5.1+pycharm,将就着看吧。只用了5个样本，准确率不高的，简单的实现了预测，没有深究。下面是伪代码： 上面的终止条件就是人为设定的次数或者是优化目标L达到可接受的范围，否则一直迭代。 具体实现如下,代码在这里排列的不好，但是可以运行： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from numpy import *from math import *import numpy as npfrom numpy.linalg import invdef sigmoid(x): return 1./(1.+np.exp(-x))def logistic_regression(): # 牛顿方法构建Logistic Regression , 对西瓜是否为好瓜或坏进行分类, #Y &lt;&lt;--预测为-- Sigmoid(w1*x1+w2*x2 + bias) X = np.array([[0.697,0.774,0.634,0.666,0.243], [0.46,0.376,0.264,0.091,0.267], [1,1,1,1,1]]) Y =np.array([1,1,1,0,0]) beta = np.array([[0], [0], [1]]) maxCycles = 20 #迭代次数 j = 0 #累积次数 old_L = 0 while j &lt;= maxCycles: #循环20次后者损失函数已经达到合理小的范围就停止迭代 beta_T_x = dot(beta.T[0], X) # 计算β_TX L = 0 # 损失函数 for i in range(5): L = L +(-Y[i]*beta_T_x[i] + log(1 + exp(beta_T_x[i]))) if abs(L-old_L)&lt;= 0.00001: break print('最优化目标L=',L) old_L = L grad = 0 # 一阶导数 H = 0 # 二阶导数 # 计算5个训练样本的优化目标,公式（3.27) for i in range(5): # 梯度: ∂L(β)/∂β，P60公式（3.30）。 grad = grad - dot(array([X[:,i]]).T,(Y[i] - array([[exp(beta_T_x[i])/(1+exp(beta_T_x[i]))]]))) #X[i]是横向量，需要转制，grad结果是1个数 H = H + dot(array([X[:,i]]).T,array([X[:,i]]).T.T)*((exp(beta_T_x[i])/(1+exp(beta_T_x[i]))) * (1-exp(beta_T_x[i])/(1+exp(beta_T_x[i])))) beta = beta - dot(inv(H),grad) #参数迭代公式（3.29） j += 1 #预测部分 print('beta=', beta) x1 = float(input('请输入西瓜的密度:')) x2 = float(input('请输入西瓜的含糖率:')) z = beta[0][0] * x1 + beta[1][0] * x2 + beta[2][0] print('预测值为：',sigmoid(z)) f = sigmoid(z) if(f==1): #P66公式（3.47)正、反例的比值&gt;观测的正反比例即认为正例，否则反例 print('分类器预测此为好瓜。') else: print('分类器预测此为坏瓜。')logistic_regression() 预测结果如下：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Logistic Regression</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bonjour!]]></title>
    <url>%2F2018%2F09%2F13%2FSVM%2F</url>
    <content type="text"><![CDATA[1.SVM总体简单介绍首先，分类的时候我们通常有时候遇到一个分类情况是，多类数据交融，无法通过简单的线性分类器区分。那SVM（Support Vector Machine）就是把数据映射到高维，然后根据支持向量找出一个最佳分类面，使得数据可以分开。这就是支持向量机干的事情。参考下面视频： 2.硬间隔SVM首先清楚四个概念：超平面，函数间隔，几何间隔，支持向量。 （1）超平面在二维空间里面是：Ax+By+C=0 ,是一条直线。在三维空间里面是：Ax+By+Cz+D=0，是一个平面。在三维以上：Ax1+Bx2+Cx3+Dx4+Ex5+Fx6+….+K=0 =&gt; 可看作是2个n维向量W和X的内积：$W^TX+b = 0$,$b$可以认为是截距，是代表了到原点的距离；W是法向量，代表了超平面的方向。这就是它的定义了。 （2）函数间隔定义为：预测函数与实际类标的乘积。我们下面提到的都是简化版的情况，即只讨论在平面情况下的分类。有一些类标$y^i$,只有正例和反例：1或-1，预测的值是：$W^TX^i+b$，函数间隔就是把这2个乘起来： \hat{γ}=y^{i}(W^{T}X^{i}+b)，γ上面有一个hat的就表示函数间隔。发现$y^i$和 $(W^TX^i+b)$是同号的话就是分类正确了。如果真实类标$y^{i}=1$，要令$(W^{T}X^{i}+b)$远大于0，这样才越准确，如果真实类标$y^{i}=-1$,则要$(W^{T}X^{i}+b)$远远小于0才越准确。就是说函数间隔可以看作是描述分类准确与否的一个指标。 （3）几何间隔L几何间隔对比函数间隔是一个更好的指标，因为，若超平面是$W^TX+b = 0$的情况下，同时放大W和b的值为原来的n倍，后发现，函数间隔$y^{i}(W^{T}X^{i}+b)$变大了 n倍，但是超平面$W^TX+b=0$没有变化，这就是函数间隔的缺点。先来看看下面的图片：可以看到，超平面L把数据很好的分开来，其中有某个样本$A(x_i,y_i)$，$B$点在超平面上面，单位向量$W$除以自己的模，得到单位长度为$\frac{W}{||W||}$。有一个$γ^i$与向量AB的长度一样，则$B$点位置的$x$就等于$A$点的$X_i$减去$AB$的长度（$AB$带有方向性），而$AB=γ_i\cdot\frac{W}{||W||}$，即有：$x = x_i - γ_i\cdot\frac{W}{||W||}$,又因为$B$点的$x$在超平面L上，满足L的表达式，于是有： W^{T}(X^{i}-γ^{i}\frac{W}{||w||})+b=0简化提出$γ_i$后得到几何间隔的表达式： γ^{i}=y^{i}[\frac{W}{||w||}X^{i}+\frac{b}{||w||}]可以知道，2点：第一：当$||W||=1$，$\hat{γ^{i}}=γ^{i}$第二：几何间隔就等于函数间隔除以一个$||W||$：$γ^{i}=\frac{\hat{γ^{i}}}{||w||}$ （4）支持向量SVM的目标是在多个分类中找到最佳的超平面，看上面的图，外面的直线上面的点，就是支持向量，这些点与超平面最近且与超平面保持一定的函数距离。中间的虚线就是我们要找的最佳的超平面，这样的超平面可以确保就算是有些点在直线上面或者里面，也可以很好地区分。 3.最优间隔分类器问题从上面图可知，2条直线之间的距离越大越好，即几何间隔越大越好，这样可以使更多的数据很好的分开来。同时，找到唯一的最好超平面，所有样本点与超平面有一定的函数间隔（即分类准确）： \begin{align*} &\max_{w,b,γ^{i}}\quad \frac{\hat{γ}}{||w||}\\ & \begin{array}{r@{\quad}r@{}l@{\quad}l} s.t.&y(W^{T}X+b)>=\hat{γ}\\ \end{array} \end{align*}发现上面的优化问题是非凸优化，所以为了简化问题，思路是先把此问题转化为对偶问题，然后通过求解对偶问题的解，来解出原问题的解。这就是基本思路。为了简化计算，可以把函数间隔$\hat{γ}$令为1，因为函数间隔扩大n倍时，底下的||W||也扩大n倍，所以并不影响优化。上面分母变成常数1后，问题可以转为最小化分子的问题。||W||加了1/2的平方是便于后面求导，无碍： \begin{align*} &\min_{w,b}\quad \frac{1}{2}||w||^{2}\\ & \begin{array}{r@{\quad}r@{}l@{\quad}l} s.t.&y^{i}(W^{T}X^{i}+b)>=1\\ \end{array} \end{align*}根据凸优化理论，该问题可以通过拉格朗日函数将目标函数转化为无约束的目标函数进行求解： L(w,b,α)=\frac{1}{2}||w||^{2}-\sum\limits_{i=1}^{m}α_{i}(y^{i}(W^{T}X^{i}+b)-1)L对αi和b分别求导并令为0后得： W=\sum\limits_{i=1}^{m}α_{i}y^{i}x^{i}\sum\limits_{i=1}^{m}α_{i}y^{i}=0然后把上面的2个式子代入L： L=\sum\limits_{i=1}^{m}α_{i}-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^my^{i}y^{j}α_{i}α_{j}\quad\quad(1)可以令上面L为新的W(α)函数，后面的尖括号里面的就是xi和xj的内积表达式。 4.对偶问题4.1原始问题先来看原始问题，什么是原始问题？假设$f(x)$,$c_i(x)$,$h_j(x)$是定义在$R^n$上的连续可微函数，考虑约束最优化问题时候： \begin{align} ​ \min_{x \in R^n}\quad &f(x) \\ ​ s.t.\quad&c_i(x) \le 0 , i=1,2,\ldots,k\\ ​ &h_j(x) = 0 , j=1,2,\ldots,k ​ \end{align}上面就是一个有约束条件的原始问题。我们一般的思路就是一脑子求导=0，然后解出x代入原式即可(因为极值处的斜率为0 )。可是这里有约束条件，怎么办？于是拉格朗日先生便一跃而出，他说可以干掉这个纸老虎。看看他怎么解决的？ \mathcal{L}(x,\alpha,\beta) = f(x) + \sum_{i=1}^k \alpha_ic_i(x) + \sum_{j=1}^l \beta_jh_j(x)没错，他也是一股脑的把所有式子塞到一起，起个名字叫做广义拉格朗日函数。其中的α和β都是参数，特别要求αi&gt;=0。直接把上面式子求导代入原式即可。就是上面公式(1)的求解结果。发现上面的式子若不满足约束条件，就至无穷大了，若满足约束条件，则令L的最大值为： \theta_P(x) = \max_{\alpha,\beta:\alpha_i \ge 0}\mathcal{L}(x,\alpha,\beta)发现原始问题就等价于最小化上面的式子： \min_{x}max \mathcal{L}(x,\alpha,\beta)=\min_{x} \theta_P(x)=\min_{x} f(x)\quad\quad(2)因为αi&gt;=0,ci(x)&lt;=0,h(x)=0，所以θp的最大值是f(x)。 4.2对偶问题上面先最大化再最小化问题，对偶问题就是先最小化再最大化问题，即对偶问题为： \max_{\alpha,\beta:\alpha_i\ge0}\theta_D(\alpha,\beta)=\max_{\alpha,\beta:\alpha_i\ge0}\min_x\mathcal{L}(x,\alpha,\beta)\quad\quad(3)在一般情况下公式(2)要&gt;=公式(3),例如，对于同一个问题，x∈{0,1},y∈(0,1)，min max(1(x=y))&gt;=max min(1(x=y)),这里的1(x=y)为示性函数:若括号里的为真，则返回1，否则返回0。左边最大的1=1，再取最小值为1，右式最小的0=0，再取最大的值为0,1&gt;0，故可以看出左式&gt;=右式。感兴趣的可以自己搜下证明。那么，什么时候等式才成立呢？这是个好问题，答：满足KKT条件的时候。这样，我们就可以把原始问题转化为对偶问题，通过解出对偶问题的参数，也就解出原始问题的部分参数，这样就简化了计算。 4.3KKT条件对于KKT条件,由定理：对于原始问题和对偶问题，假设函数$f(x)$和$c_i(x)$是凸函数，$h_i(x)$是仿射函数（即由一阶多项式构成的函数，f(x)=Ax + b, A是矩阵，x，b是向量）；并且假设不等式约束$c_i(x)$是严格可行的，即存在x，对所有i有$c_i(x)&lt;0$，则x*,α*,β*分别是原始问题和对偶问题的最优解的充分必要条件是x*,α*,β*满足下面的Karush-Kuhn-Tucker(KKT)条件： \begin{align} & \nabla_xL(x^*,\alpha^*,\beta^*)=0\\ &\nabla_\alpha L(x^*,\alpha^*,\beta^*)=0\\ &\nabla_\beta L(x^*,\alpha^*,\beta^*)=0\\ &\alpha_i^*c_i(x)=0,i=1,2,\ldots,k\quad\quad(a)\\ &\alpha_i^*\ge0,i=1,2,\ldots,k\quad\quad\quad\quad(b)\\ &c_i(x)\le0,i=1,2,\ldots,k\quad\quad\quad (c)\\ &h_j(x^*)=0,j=1,2,\ldots,l\quad\quad\quad(d) \end{align}前三个求偏导是保证驻点的存在，后面的式子(a)、(b)是拉格朗日乘子需要满足的约束，(c)、(d)是原始问题的约束条件。特别注意当αi*&gt;0时，由KKT对偶互补条件可知:ci(x)=0，这个后面会用到。 5.软间隔SVM第3章节主要讲的是硬间隔最大化，那什么是软间隔最大化？硬间隔呢就是支持向量不能超过第二幅图的直线，顶多在线上面，但是软间隔可以容忍少量的支持向量落在直线内，于是我们的约束条件$y_i(W^TX+b)\ge1$就要修改为$y_i(W^TX+b)\ge1-ξ_i$，即允许部分支持向量间隔不足1,$ξ_i&gt;=0$。我们原始的最大化约束问题转为： \begin{align} \min_{w,b}\quad &\frac{1}{2}||w||^{2}+C\sum\limits_{i=1}^{m}ξ_i\\ s.t.\quad&y^{i}(W^{T}X^{i}+b)\ge1-ξ_i\\ &ξ_i\ge0 \end{align}由于加了松弛变量ξi，优化目标后面也加上惩罚项，其中C值大，对误分类的惩罚越大。前面的平方项希望函数间隔越大，后面的惩罚项希望误分类的样本点越少。通过拉格朗日函数将目标函数转化为无约束的目标函数进行求解： \mathcal{L}(w,\alpha,\beta) = \frac{1}{2}||w||^{2} + C\sum\limits_{i=1}^{m}ξ_i-\sum_{i=1}^m \alpha_i[y^{i}(W^{T}X^{i}+b)-1+ξ_i] -\sum_{j=1}^mu_jξ_i\quad\quad(4)把上面L函数分别对W, b, ξi求偏导=0可得： w = \sum_{i=1}^mα_iy_ix_i\quad\quad(5) \sum_{i=1}^mα_iy_i=0 C - α_i - u_i = 0把上面三个式子代入公式（4），可得到我们的对偶问题的优化目标为： \begin{align} \max_\alpha\quad &\sum\limits_{i=1}^{m}α_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^my_iy_jα_iα_j\\ s.t.\quad&\sum_{i=1}^mα_iy_i=0\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad(6)\\ & 0\leα_i\le C,i=1,2,\dots,m \end{align}有上式可求出α，再由α求出W和b。公式（5）可求出W，而b是所有支持向量满足$y_i(W^TX_i+b)=1$的值b的均值。由KKT条件： α_i[y^{i}(W^{T}X^{i}+b)-1+ξ_i]=0\\ u_iξ_i=0上面第一个式子正是由于αi&gt;0,由KKT推断$y_i(W^TX_i+b)-1+ξ_i=0$所得。可知：满足$0\leα_i\le C$的样本点$(x_s,y_s)$，即为支持向量。详细请点击底部的参考1。 5.1核函数对于线性可分数据用硬间隔和软间隔可以区分数据，但对于非线性的数据怎么办？于是，我们的核函数粉墨登场（这里引用原意：演员化妆演戏）。它干的事情直观感受就是上面最开始的视频里干的部分事：把非线性的数据映射到线性可分的维度里面。来，举个栗子八。就是我们有一组非线性的2维模型： f(X_1,X_2) = A_0+A_1X_1+A_2X_2+A_3X_1^2+A_4X_2^2+A_5X_1^3+A_6X_2^3若令： X_1=X_1,X_2=X_2,X_1^2=X_3,X_2^2=X_4,X_1^3=X_5,X_2^3=X_6则可以转化为6维模型： f(X_1,X_2，X_3，X_4，X_5，X_6) = A_0+A_1X_1+A_2X_2+A_3X_3+A_4X_4+A_5X_5+A_6X_6就是说：低维度不可分的数据模型可以映射到高维度后，变成线性可分的模型。下面的核函数就是干的这个事情，就是有核函数k(x,z)把低维度的φ(xi)和φ(zj)之间的内积在低纬度计算掉： k(x,z)=φ(x)φ(z)常见的核函数有： （1）高斯核函数（Gaussian Kernel） \kappa(x, x_i) = exp(-\frac{||x - x_i||^2}{\delta^2})高斯径向基函数是一种局部性强的核函数，其可以将一个样本映射到一个更高维的空间内，该核函数是应用最广的一个，无论大样本还是小样本都有比较好的性能，而且其相对于多项式核函数参数要少。 （2）线性核函数（Exponential Kernel） \kappa(x,x_i) = x \cdot x_i线性核，主要用于线性可分的情况，我们可以看到特征空间到输入空间的维度是一样的，其参数少速度快，对于线性可分数据，其分类效果很理想，因此我们通常首先尝试用线性核函数来做分类，看看效果如何，如果不行再换别的。 （3）多项式核函数（Polynomial kernel function） \kappa(x, x_i) = ((x\cdot x_i) + 1)^d多项式核函数可以实现将低维的输入空间映射到高纬的特征空间，但是多项式核函数的参数多，当多项式的阶数比较高的时候，核矩阵的元素值将趋于无穷大或者无穷小，计算复杂度会大到无法计算。这几个核函数可以优先使用高斯核函数，据说Andrew Ng只用高斯核函数，可见性能确实优越。当然，还是要根据自己的模型找最适合的核函数。还有很多核函数就不介绍了，感兴趣的自行搜索。 6.SMO算法6.1求解$α_1,α_2$了解了核函数后，上面式子（6）对偶问题的内积换成核函数后，提个负号出来整体变为最小化问题就等价于： \begin{align} \min_\alpha\quad &\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^my_iy_jα_iα_jk(x_i,x_j)-\sum\limits_{i=1}^{m}α_i\\ s.t.\quad&\sum_{i=1}^mα_iy_i=0\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad(7)\\ & 0\leα_i\le C,i=1,2,...,m \end{align}SVM的“核“是什么呢？就是SMO（Sequential Minimal Optimization）算法，他是用来求α的，前面说过：求出了α，就可以求出W和b,接着超平面也就找到了。它的主要思想就是：α有m个变量，但是直接根据优化问题来求解很困难。所以，SMO算法每次只对2个变量更新，其他变量看成是常数，而常数项可以在优化目标中省略。 上面的是具体α求解步骤，稍微讲解一下，公式展开来自公式（7）。min后面优化目标这么长，不要吓坏了，只是把α1和α2变量部分展开来，其余看作常数。下面的限制条件原本是$\sum_{i=1}^mα_iy_i=0$，因为$y_i$只能取-1和1，所以就看作是m个正负$α_i$相加=0，拿出2个变量：α1和α2出来玩玩，其余的和为常数-k。需要注意的是2个相同$y_i$的乘积为1，这里都会用到以简化计算。然后得到第1部分的优化目标和约束条件。(图中的$α_2^{new,uncut}$不对，应该是$α_2^{new,unc}$，后面的其实是unclip) 第2部分，是讲解的α的整个迭代过程。过程上面图片有，讲一下L和H的范围是怎么来的。以左边双红线图为例：α本身就有限制[0,C]，故有图中的正方形。坐标轴的横坐标是α1，纵坐标是α2,由α1-α2=K可得2条红色的线，因为已经限制了α1和α2的范围，只能在方框中。可得到线①、线②。可知线①的α2范围是[-K,C]，线②的α2范围是[0,C-K]，再把α1-α2=K代入即可得到L，H的范围。另一种情况也一样可得。 这一步主要是求解未经剪辑的α2，思路是先简化优化目标函数W(α1，α2)，然后代入α1和α2的关系式消去α1，使得表达式全部是关于α2。接着W（α2）对α2求导=0即可。最后可以得到迭代式子： α_2^{new,unc} = α_2^{old} + \frac{y_2(E_1-E_2)}{k_{11}+k_{22}+k_{12}}6.2求解b当$0&lt;α_1^{new}&lt;C$时,根据KKT条件$y_i(W^TX_i+b)=1$: y_1(\sum_{j=1}^{m}α_jy_jk_{1j}+b_1)=1\\ y_1\cdot y_1(\sum_{j=1}^{m}α_jy_jk_{1j}+b_1)=1\cdot y_1 \\ 即为：\sum_{j=1}^{m}α_jy_jk_{1j}+b_1-y_1=0\\ 迭代得：b_1^{new}=y_1-α_1^{new}y_1k_{11}-α_2^{new}y_2k_{12}-\sum_{j=3}^{m}α_jy_jk_{1j}\\ 本轮迭代中E_1: E_1=g(x_1)-y_1\\ y_1代入上面的b_1^{new}可得：\\ b_1^{new}=b^{old}-E_1+y_1k_{11}(α_1^{old}-α_1^{new})+y_2k_{12}(α_2^{old}-α_2^{new})\\ b_2^{new}同理：\\ b_2^{new}=b^{old}-E_2+y_1k_{12}(α_1^{old}-α_1^{new})+y_2k_{22}(α_2^{old}-α_2^{new})\\ 故：b^{new}=\frac{b_1^{new}+b_2^{new}}{2}最后更新Ei为下一轮迭代做准备: E_i = \sum_{S}α_iy_ik_{ij}+b^{new}-y_i最终的b是： b = y_s-\sum_{S}α_iy_ik(x_i,x_s)即所有支持向量的b的均值即为最终的b值。 6.3求分类超平面即为： \sum_{i=1}^mα_iy_ik(x_i,x_s)+b=07.总结SMO算法部分强烈推荐参考：点击此处，我的多数都是来自于此。这篇讲解的非常仔细，比我不知高到哪里去了。最后参考链接第5个是他这个系列的总体，一共5部分，包括代码实现。SVM优点：泛化错误率低，计算开销不大，结果易解释。缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。适用数据类型：数值型和标称型数据。 非常感谢那些乐于分享知识的人。这篇写了整整2天，算上搭建blog的话有一个礼拜了，也算是对最近的一个梳理和总结吧。最后，推荐一款Markdown编辑器：Typora,做到了所谓的“所见即所得”，这是我听过的世界上最好的宣传语(≧∇≦)ﾉ，Au revoir, chère amie。 参考链接： 1.支持向量机Part2—线性支持向量机 2.svm常用核函数 3.从超平面到SVM（三） 4.最优间隔分类器问题 5.支持向量机Part5—Python实现]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>SVM</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
