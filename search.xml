<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[朴素贝叶斯模型]]></title>
    <url>%2F2018%2F12%2F03%2FNB%2F</url>
    <content type="text"><![CDATA[1.朴素贝叶斯公式1.1概念简述简介下三个重要的条件概率的公式： \begin{align*} &(1)条件概率公式: P(A|B)=P(AB)/P(B)\ ，其中P(B)>0 \\ &(2)乘法公式：P(A∩B)=P(AB)=P(A|B)P(B)=P(B|A)P(A)\\ &\quad\ 推广：P(A1A2...An)=P(A1)P(A2|A1)P(A3|A1A2)...P(An|A1...An-1)\\ &(3)全概率公式：P(A)=\sum_{i=1}^{n}P(A|B_{i})P(B_{i}) \end{align*} （1）条件概率公式设A,B是两个事件，且P(B)&gt;0,则在事件B发生的条件下，计算事件A发生的条件概率。在条件概率中，最本质的变化是样本空间缩小了——由原来的整个样本空间缩小到了给定条件的样本空间。 （2）乘法公式表示A、B同时发生的概率可以看作两个概率的乘积。推广时候，就是多个事件交集的概率可以拆成单个事件Ai的条件概率的乘积形式。 （3）全概率公式就是下图中把整个样本空间S给划分为n块B区域，其中有一块彩色A事件，那么A事件可由每个事件Bi条件下A的部分概率求和而成。 1.2贝叶斯公式（Naive Bayes）废话不多说，先上朴素贝叶斯公式： \begin{align*} P(c_i|x) &= \frac{P(x|c_i)P(c_i)}{P(c_i)}\\ &= \frac{P(x|c_i)P(c_i)}{\sum_i P(x|c_i)P(c_i)} \end{align*}其中，第一个没有代入全概率公式。$P(c_i)$是先验概率（Prior probability），就是在条件x发生之前对类c有一个经验上的估计值，这个估计值是主观的、人为的，但是也是依据一定实际观察或者经验所得。由于主观的猜测值没有令人信服的证明，所以被频率学派所疑虑。比如我们可以猜想对抛1次硬币得到正面的概率为0.5，因为不是正面就是反面（假设没有竖立情况），这就是根据的经验值，因为你并没有去做大量（无穷）的实验来证明。 $P(c_i|x)$是后验概率（Posterior probability）。比如有十个不透明盒子，只有一个盒子里有球，则猜中有球的概率为1/10。当事件 x：打开一个盒子发现没有球，发生后，此时若再打开一个盒子有球的概率就变成了1/9。后验概率是建立在先验概率的基础上，通过贝叶斯公式建立起来的。它是一种“知果求因”的思路，这里打开后的“结果”发现没有球，此时才知道“原因”的概率是1/9。 $P(x|c_i)$是似然函数（Likelyhood），在机器学习中更多的时候它表示了第i个类C中出现属性x的概率，我们希望是使得类c中出现属性x的概率越大越好，因为反过来$P(c_i|x)$概率也越大。统计学中似然函数用符号$f(x|\theta)$，其中θ是需要估计的参数θ(事先假设已经确定有唯一θ)，x是随机变量x，是假设参数θ已知后我们观察到的样本应该是什么样子的，即在此参数θ下样本似然真实样本的程度，旨在最大化下面的似然函数以求需要估计的参数θ： L(\theta) = \prod_{i=1}^{n} f(x_i|\theta)上面就是大名鼎鼎的最大似然估计（MLE），是用来估计参数的方法，还有一种叫矩估计。最大似然估计和矩估计都是频率学派的经典方法。最大似然估计就是每个变量对应似然函数$f(x_i|\theta)$的乘积结果最大化即可停止，可获估计的参数θ。关于参数估计深入一点，有一个置信区间和可信区间区别，假设我们以一批样本可能服从高斯分布，通过多个样本X估计得到参数μ、σ后，置信区间表示有90%的置信程度是当前样本下的高斯分布是以这组参数μ、σ而构造，这里的置信区间只与样本相关，与参数θ无关，频率学派 认为模型的参数是固定的,不会随着样本的变化而变化，只是当前样本产生的某个参数可信度或大或小，抽样越多，频率越接近于真实概率，估计得到的参数越可信，即假定参数是唯一的，只是我们不知道，通过不断的抽样去估计这个参数罢了。而可信区间表示我们有很多个这样的参数，这些参数组成一个可信区间，从中抽取一个概率大一点的使用，比如90%下的参数。置信区间只有一组或一个参数，而可信区间有多组或多个参数。再提一下贝叶斯学派 观点：贝叶斯学派则认为参数不是唯一的，是一个随机变量，服从一个先验分布，而样本是固定的，通过先验分布、后验分布来获取参数。 分母部分$\sum_i{P(x|c_i)P(c_i)}$叫做”证据因子“（Evidence )，以保证各类别的后验概率总和为1从而满足概率条件，表示各个类别的出现属性x的概率总和。发现贝叶斯公式后验概率与分子部分成正比： P(c_i|x)∝P(x|c_i)P(c_i)1.2贝叶斯例子例子1： 症状 职业 疾病 打喷嚏 护士 感冒 打喷嚏 农夫 过敏 头疼 建筑工人 脑震荡 头疼 建筑工人 感冒 打喷嚏 教师 感冒 头疼 教师 脑震荡 某个医院早上收了六个门诊病人，如上表，现在又来了第七个病人，是一个打喷嚏的建筑工人。请问他患上感冒的概率有多大？ 根据公式$P(c_i|x) = \frac{P(x|c_i)P(c_i)}{P(c_i)}$ 可知： P(感冒|打喷嚏，建筑工人)=\frac{P(打喷嚏，建筑工人|感冒)\ast P(感冒)}{P(打喷嚏，建筑工人)}假设打喷嚏和建筑工人是相互独立的，则公式变为： P(感冒|打喷嚏，建筑工人)=\frac{P(打喷嚏|感冒)\ast P(建筑工人|感冒)\ast P(感冒)}{P(打喷嚏)\ast P(建筑工人)}其中：$P(感冒)=\frac{1}{2}$，$P(打喷嚏|感冒)=\frac{2}{3}$，$P(建筑工人|感冒)=\frac{1}{3}$，$P(打喷嚏) \ast P(建筑工人)=\frac{1}{2}\ast\frac{1}{3}=\frac{1}{6}$ ，代入上式子即可得： P(感冒|打喷嚏，建筑工人)=\frac{\frac{2}{3}*\frac{1}{3}*{\frac{1}{2}}}{\frac{1}{6}}=\frac{2}{3}\approx0.666说明此打喷嚏的建筑工人患感冒的概率为60.6%，依次计算打喷嚏、建筑工人的条件下患过敏、脑震荡的概率，然后有三个类别的后验概率，取最大的值作为该情况的预测值即可，这种模型常用于文本分类等。 例子2： 一种癌症，得了这个癌症的人被检测出为阳性的几率为90%，未得这种癌症的人被检测出阴性的几率为90%，而人群中得这种癌症的几率为1%，一个人被检测出阳性，问这个人得癌症的几率为多少？猛地一看，被检查出阳性，而且得癌症的话阳性的概率是90%，概率蛮大的，算下来看看。 我们用$A$表示事件 “测出为阳性”, 用$B_1$表示“得癌症”, $B_2$表示“未得癌症”。根据题目，我们知道如下信息: P(A|B_{1}) = 0.9, P(A|B_{2}) = 0.1, P(B_{1}) = 0.01, P(B_{2}) = 0.99那么我们现在想得到的是已知为阳性的情况下，得癌症的几率$P(B_{1},A)：$ P(B_{1},A) = P(B_{1}) \cdot P(A|B_{1}) = 0.01 \times 0.9 = 0.009表示1000人中只有9个人检测为阳性的是真的得癌症的。再来看看检测出阳性但是没有得癌症的概率： P(B_{2},A) = P(B_{2}) \cdot P(A|B_{2}) = 0.99 \times 0.1 = 0.099即1000人中间只有99个人检测为阳性但是没有得癌症的。那么在检测出阳性的前提下得癌症的概率，即把上面得值归一化即可： P(B_{1}|A)=\frac{0.009}{0.099 + 0.009} \approx 0.083显然，检测为阳性不得癌症的概率约是0.917，那么我们检测为阳性后，也不必万念俱灰，至少Thomas Bayes告诉你，你还有救。在这点上，他果然是上帝。 通过例子知道，贝叶斯的分母可以有2种计算方式，一种是以每个事件都是独立的，连乘积的形式。另一种是全概率公式展开的形式，依据不同情况活用。具体的算法流程请参考： https://blog.csdn.net/dataningwei/article/details/54140537 ，注意到此链接中因为分母对每类后验概率一样，所以后验概率与分子成正比，所以直接最大化分子部分即可。 2.多项式朴素贝叶斯朴素贝叶斯模型最大的特点是基于贝叶斯公式和特征的条件概率是独立的，这就给我们很大的简化空间和计算的方便。多项式朴素贝叶斯（Multinomial Naive Bayes） 比如对于文本分类来说，我们的目标就是获得最大的某一类的后验概率，这里我们假设每个特征之间是独立的： f(x)=argmax_{c_{k}}\ P(c_{k}|x)=argmax_{c_{k}} \frac{P(c_{k})\prod_{i=1}^{n}P(x_{i}|c_{k})}{\sum_{k}P(c_{k})\prod_{i=1}^{n}P(x_{i}|c_{k})}由于所有类的分子一样，所以直接做大化分母即可： f(x)=argmax \ P(c_{k})\ast\prod_{i=1}^{n}P(x_{i}|c_{k})对于$P(c_k),P(x_{i}|c_{k})$计算方法如下： P(c_{k})=\frac{N_{c_{k}}+\alpha}{N+k\alpha}N是总的样本个数，k是总的类别个数，$N_{c_k}$是类别为$c_k$的样本个数，$α$是平滑值。 P(x_{i}|c_{k})=\frac{N_{c_{k},x_{i}}+\alpha}{N_{c_{k}}+n\alpha}$N_{c_{k}}$是第k类$c_{k}$的样本个数，n是特征x的取值个数，$N_{c_{k},x_{i}}$是类别为$c_k$的样本中，第i维特征的值是$x_i$的样本个数，$α$是平滑值。因为若在预测时候，若预测中有特征没有出现在训练集中，那么$P(x_{i}|c_{k})$是0，所以整个乘积都是0,这样就没有意义了。为了防止概率为0，加上一个平滑，让它较好的反映原始概率。 当α=1时，称作Laplace平滑，当0&lt;α&lt;1时，称作Lidstone平滑，α=0时不做平滑。 例子可以参考这篇文章： https://blog.csdn.net/u012162613/article/details/48323777 ，里面还有其他2种模式的解释，不错哦。 3.NB和其他NB的区别相同点：都是以贝叶斯模型为基础。 朴素贝叶斯和各个贝叶斯之间的区别： （1）朴素贝叶斯的假设前提有两个第一个为：各特征彼此独立；第二个：对被解释变量的影响一致，不能进行变量筛选。贝叶斯显然是没有独立的前提，即各个特征之间可以有依赖的，也可以独立。 （2）GaussianNB、MultinomialNB和BernoulliNB的似然函数的计算方式不同。GaussianNB就是似然函数$P( x_{i} | c_{k})$为高斯分布的朴素贝叶斯，MultinomialNB就是似然函数为多项式分布的朴素贝叶斯，而BernoulliNB就是似然函数为伯努利分布的朴素贝叶斯。 一般来说，如果样本特征的分布大部分是连续值，使用GaussianNB会比较好。如果如果样本特征的分大部分是多元离散值，使用MultinomialNB比较合适。而如果样本特征是二元离散值或者很稀疏的多元离散值，应该使用BernoulliNB。 4.参考（1）贝叶斯的三种模型 （2）例子1来源 （3）贝叶斯和MCMC博客 （4）贝叶斯网络与朴素贝叶斯的区别]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Naive Bayes</tag>
        <tag>多项式朴素贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[向量空间模型（VSM）]]></title>
    <url>%2F2018%2F11%2F28%2F%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[1.词袋模型(BOW)​ 简单理解下词带模型，顾名思义就是一个袋里装了很多单词，可知特点：（1）假设词语无序 （2）假设词与词之间独立。Bag-of-words模型是信息检索领域常用的文档表示方法。忽略了它的单词顺序和语法、句法等要素，将其仅仅看作是若干个词汇的集合。 它具体是如何表示的？如下有一个句子： I have a pear and a strawberry. 于是可以构建一个字典，如下： {“i”:0,”have”:1,”a”:2,”pear”:3,”and”:4,”strawberry”:5} ，这个字典里面的字符可表示出所有的词，且唯一。但是字典里面词的顺序和原句子词序并不相关，这里只是特例。于是可以建立一个词向量如下： [1,1,2,1,1,1] ，这里是字典里面的词在原句子里出现的次数的统计，一一对应。于是若有若干句子组成一篇文档，则可以构建一个对应的字典，然后有一个N维的向量来表示这篇文档（词频的统计）。若要比较2篇文档相似度，也可以比较2个向量之间的余弦相似度。 2.向量空间模型(VSM)2.1VSM概念​ 向量空间模型（Vector Space Model）和词袋模型很像（个别说是一样的），都是把一个文档表示成向量的模式，而向量空间模型运用较为广泛的是TF-IDF方法。 第1：把一篇文档表示成词向量：D=[W_1,W_2,W_3,…,W_m],某D文档中有m个项。这里可以是的单个词，可以是词组等等。 第2：计算每个项的贡献度：Q =[Q_1,Q_2,Q_3,…,Q_m] ,这里的Q_i是每个项的贡献度，名字不一，也可以理解为每一项（每个词）的比重，权重。计算贡献度的方法不一。 于是每个文档便由一组词向量和对应的贡献度向量表示而成。 2.2TF-IDF方法其实就是第2步的算法不同，这里的TF-IDF分为TF（每一项的频率：Term Frequency）和IDF（逆文档率：Inverse Document Frequency），这里的TF，就是每个词在文档中的频次比例。如下： TF =\frac{N_i}{M},N_i是第i个词在某一篇文档的频次，M是该文档的总词量。IDF就是这个词在哪些文档出现了，因为语料库由K篇文档组成，每篇文档M个词。计算如下： IDF =log(\frac{K}{L+1}),K表示语料库有k篇文档，L表示包含该词的文档数量，L+1防止分母为0。那TF-IDF计算公式如下： TF-IDF =\frac{N_i}{M}*log(\frac{K}{L+1})=\frac{N_i}{M}*log(\frac{1}{D_i}),D_i是某个词出现在所有文档的中的频次比例。发现：$log_a^b​$（底数a&gt;1）函数是单调增函数，且定义域在(0,1]时值域为负值，大于1时为单调正数。由于概率是在[0,1]的，倒数就是大于等于1，这里分母取不到0，倒数就不会是正无穷 ，log(b)值就是一个正数，并且b越大IDF就越大。这里其实K/L是某个词的在语料库中的文档率的倒数。所以当K和L的比值越大，IDF就越大，说明K和L比例差的越大，对我们越有利。比如有20篇文档，若一个词在所有文档中出现了1次，K/(L+1)=20/(1+1)=10,log(10)=3.32，相对其他来说是一个蛮大的比重。那这个词对我们来说也确实是重要的，在做文档相似度的时候作用就很大，因为只要这个词出现说明该词有很好的区分性。相反，若K/L的比值很小，比如K/(L+1)=1时，log1=0,比重就很小。为什么该词比重需要设置的小？因为K/(L+1)==1，说明K和L相近，即这个词在每个文档都出现，那这个词很可能是介词、连词等没有区分性的词，比如’the’，’for’,’的’，’在’这些词很大概率出现在所有文档中，所以不重要，权重就小。当然，也可以通过设置停用词的方法来筛掉这些词，这个是数据预处理的部分了。 比如语料库中共有10篇文档，每篇800词。某一篇文档中的”战斗机” 和 “的”都出现了20次，但是”战斗机” 在2篇文档中出现，而”的”在9篇文档中出现，”的”字符的IDF=TF*log1=0，这样算下来”战斗机” 的TF-IDF的值肯定比0大，故”战斗机” 这个词更加重要。然后排序后，可以取前n个重要的词放入分类器做分类也可，计算相似度也可。其实，TF-IDF计算方法类似于交叉熵，贡献度的计算法子还有很多，按下不表。 BOW和VSM两者的区别其实很小，我自己看来，就在于贡献度不一。 2.3熵的计算方法这里其实也算是特征选择，本文只给出了特征选择的计算方法。特征选择是一个系统的工程，有自己的方法和技术。本文特指章节2.1里面第2步骤中贡献度的计算方法。 （1）信息熵 前面提到了交叉熵，先理解什么是熵。熵是由香农提出的，记得有位科学家说：若自己余生也发明一个类似“熵”这样的概念，这一生也值了。可见其重要程度，后来证实熵在计算机领域，通信领域，信息论等等都是举足轻重的。到底什么是熵？直接给出离散变量x的熵公式： H(X)=-\sum\limits_{x\in\mathcal{X}}p(x)\log p(x)作用： 用来度量信息的不确定程度。举个栗子：若你在3个盒子中抽一个奖，设每个盒子抽到奖的概率=1/3，代入熵公式=- 1/3 x log(1/3) - 1/3 x log(1/3) - 1/3 x log(1/3) =0.47712。但是第二个人由于商家作弊，使得第2个盒子抽到的概率上升为0.8，其他2个都是0.1,H(X) = -0.1 x log(0.1) x 2 - 0.8 x log(0.8) = 0.277528。显然这个第二个熵变小了，但是确实符合它反映的信息量。因为第二个事件的信息量更加大。信息熵的特点是熵越小，表示信息越纯，即信息量越大。所以，这个值和我们的事实是符合的。 （2）条件熵 定义：在一个条件下，随机变量的不确定性。 ​ 条件熵公式：$H(X|Y)=-\sum\limits_{x,y}{P(X,Y)*log(P(X|Y))}$ 证明如下图： 下面证明 条件熵=联合熵-单独的熵：$H(Y|X)=H(X,Y)-H(X)$： 举个栗子吧： https://zhuanlan.zhihu.com/p/26551798 ,这篇便有。 （3）信息增益（Information Gain） IG=熵 - 条件熵即为： Grain(Y,X)=H(Y)-H(Y|X)例子请参考周志华《机器学习》P75。 （4）交叉熵 交叉熵又叫KL散度，是用来衡量2个概率分布的差异程度。交叉熵即KL散度的计算公式： D_{KL}(p||q)=\sum_{i=1}^np(x_i)log(\frac{p(x_i)}{q(x_i)}) \tag{1}这里的p是真实分布，比如我们遇到一个复杂的p分布，要做的事就是用一个简单的分布q来代替复杂的、不好计算的真实分布p，KL散度就是用来衡量这2个分布的差异。发现log后面p(xi)和q(xi)越接近时候，KL散度值近乎为0。就是说KL散度值越小，两个分布越接近，即可以用后面的q分布来代替真实分布p。在变分推断里会遇到这个概念，我等只能敬而远之。交叉熵在部分问题时候，也可以做为loss函数。比如：https://blog.csdn.net/tsyccnh/article/details/79163834 大致把熵过了一遍，说的好不如做得好，还是多实践吧。$Vouloir\ \ c’est\ \ pouvoir$（有志者，事竟成）。 3.参考1.词袋模型简介 2.逆文档频率法]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>向量空间模型</tag>
        <tag>词袋模型</tag>
        <tag>信息熵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性空间的定义]]></title>
    <url>%2F2018%2F11%2F19%2F%E7%BA%BF%E6%80%A7%E7%A9%BA%E9%97%B4%E5%AE%9A%E4%B9%89%2F</url>
    <content type="text"><![CDATA[做了一个关于线性空间的思维导图，纯粹练练思维导图的作图技术。希望有启发。 Bonne chance.]]></content>
      <categories>
        <category>矩阵分析</category>
      </categories>
      <tags>
        <tag>线性空间定义</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归和python3实现]]></title>
    <url>%2F2018%2F11%2F10%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[1.逻辑回归笔记​ 逻辑回归就是在线性回归的基础上套一个分类函数，当然也可以不用线性回归。下面是我根据周志华的逻辑回归写的笔记。具体请参考周志华的《机器学习》P57-60。第1张图是说明了逻辑回归的预测模型，第2张图是优化目标L的由来，第3张图是详细的求出一阶导数和二阶导数。我觉得这里一阶导数和二阶导数周志华这里写的不好，矩阵计算写的不清晰明朗。当然这也是自己基础不牢靠所致，共勉吧。如下： 图1：得到两个后验概率的表达式P0和P1，用极大似然估计法找出优化目标L。 图2：再把P0，P1和转化后等价的似然项代入对数似然L,简化后即可得最终优化目标L。 图3：L对2个参数的求导。 最后由牛顿法得到迭代参数公式和优化目标L。 ​ 在之前计算的时候出现了奇异矩阵，即矩阵的行列式值为0，想了下可能是之前的数据之间存在线性相关性，所以导致矩阵的秩不满秩，所以也就不可逆了，即变成了奇异矩阵。若我不做实验，就不会知道奇异矩阵，也就记不起来以前的秩了。想着一些小算法，可以重复造轮子，但是复杂的算法就算了，没必要。人生奇美，怎可废！ ​ 2.python3实现用的python3.5.1+pycharm,将就着看吧。只用了5个样本，准确率不高的，简单的实现了预测，没有深究。下面是伪代码： 上面的终止条件就是人为设定的次数或者是优化目标L达到可接受的范围，否则一直迭代。 具体实现如下,代码在这里排列的不好，但是可以运行： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from numpy import *from math import *import numpy as npfrom numpy.linalg import invdef sigmoid(x): return 1./(1.+np.exp(-x))def logistic_regression(): # 牛顿方法构建Logistic Regression , 对西瓜是否为好瓜或坏进行分类, #Y &lt;&lt;--预测为-- Sigmoid(w1*x1+w2*x2 + bias) X = np.array([[0.697,0.774,0.634,0.666,0.243], [0.46,0.376,0.264,0.091,0.267], [1,1,1,1,1]]) Y =np.array([1,1,1,0,0]) beta = np.array([[0], [0], [1]]) maxCycles = 20 #迭代次数 j = 0 #累积次数 old_L = 0 while j &lt;= maxCycles: #循环20次后者损失函数已经达到合理小的范围就停止迭代 beta_T_x = dot(beta.T[0], X) # 计算β_TX L = 0 # 损失函数 for i in range(5): L = L +(-Y[i]*beta_T_x[i] + log(1 + exp(beta_T_x[i]))) if abs(L-old_L)&lt;= 0.00001: break print('最优化目标L=',L) old_L = L grad = 0 # 一阶导数 H = 0 # 二阶导数 # 计算5个训练样本的优化目标,公式（3.27) for i in range(5): # 梯度: ∂L(β)/∂β，P60公式（3.30）。 grad = grad - dot(array([X[:,i]]).T,(Y[i] - array([[exp(beta_T_x[i])/(1+exp(beta_T_x[i]))]]))) #X[i]是横向量，需要转制，grad结果是1个数 H = H + dot(array([X[:,i]]).T,array([X[:,i]]).T.T)*((exp(beta_T_x[i])/(1+exp(beta_T_x[i]))) * (1-exp(beta_T_x[i])/(1+exp(beta_T_x[i])))) beta = beta - dot(inv(H),grad) #参数迭代公式（3.29） j += 1 #预测部分 print('beta=', beta) x1 = float(input('请输入西瓜的密度:')) x2 = float(input('请输入西瓜的含糖率:')) z = beta[0][0] * x1 + beta[1][0] * x2 + beta[2][0] print('预测值为：',sigmoid(z)) f = sigmoid(z) if(f==1): #P66公式（3.47)正、反例的比值&gt;观测的正反比例即认为正例，否则反例 print('分类器预测此为好瓜。') else: print('分类器预测此为坏瓜。')logistic_regression() 预测结果如下：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Logistic Regression</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bonjour!]]></title>
    <url>%2F2018%2F09%2F13%2FSVM%2F</url>
    <content type="text"><![CDATA[1.SVM总体简单介绍首先，分类的时候我们通常有时候遇到一个分类情况是，多类数据交融，无法通过简单的线性分类器区分。那SVM（Support Vector Machine）就是把数据映射到高维，然后根据支持向量找出一个最佳分类面，使得数据可以分开。这就是支持向量机干的事情。参考下面视频： 2.硬间隔SVM首先清楚四个概念：超平面，函数间隔，几何间隔，支持向量。 （1）超平面在二维空间里面是：Ax+By+C=0 ,是一条直线。在三维空间里面是：Ax+By+Cz+D=0，是一个平面。在三维以上：Ax1+Bx2+Cx3+Dx4+Ex5+Fx6+….+K=0 =&gt; 可看作是2个n维向量W和X的内积：$W^TX+b = 0$,$b$可以认为是截距，是代表了到原点的距离；W是法向量，代表了超平面的方向。这就是它的定义了。 （2）函数间隔定义为：预测函数与实际类标的乘积。我们下面提到的都是简化版的情况，即只讨论在平面情况下的分类。有一些类标$y^i$,只有正例和反例：1或-1，预测的值是：$W^TX^i+b$，函数间隔就是把这2个乘起来： \hat{γ}=y^{i}(W^{T}X^{i}+b)，γ上面有一个hat的就表示函数间隔。发现$y^i$和 $(W^TX^i+b)$是同号的话就是分类正确了。如果真实类标$y^{i}=1$，要令$(W^{T}X^{i}+b)$远大于0，这样才越准确，如果真实类标$y^{i}=-1$,则要$(W^{T}X^{i}+b)$远远小于0才越准确。就是说函数间隔可以看作是描述分类准确与否的一个指标。 （3）几何间隔L几何间隔对比函数间隔是一个更好的指标，因为，若超平面是$W^TX+b = 0$的情况下，同时放大W和b的值为原来的n倍，后发现，函数间隔$y^{i}(W^{T}X^{i}+b)$变大了 n倍，但是超平面$W^TX+b=0$没有变化，这就是函数间隔的缺点。先来看看下面的图片：可以看到，超平面L把数据很好的分开来，其中有某个样本$A(x_i,y_i)$，$B$点在超平面上面，单位向量$W$除以自己的模，得到单位长度为$\frac{W}{||W||}$。有一个$γ^i$与向量AB的长度一样，则$B$点位置的$x$就等于$A$点的$X_i$减去$AB$的长度（$AB$带有方向性），而$AB=γ_i\cdot\frac{W}{||W||}$，即有：$x = x_i - γ_i\cdot\frac{W}{||W||}$,又因为$B$点的$x$在超平面L上，满足L的表达式，于是有： W^{T}(X^{i}-γ^{i}\frac{W}{||w||})+b=0简化提出$γ_i$后得到几何间隔的表达式： γ^{i}=y^{i}[\frac{W}{||w||}X^{i}+\frac{b}{||w||}]可以知道，2点：第一：当$||W||=1$，$\hat{γ^{i}}=γ^{i}$第二：几何间隔就等于函数间隔除以一个$||W||$：$γ^{i}=\frac{\hat{γ^{i}}}{||w||}$ （4）支持向量SVM的目标是在多个分类中找到最佳的超平面，看上面的图，外面的直线上面的点，就是支持向量，这些点与超平面最近且与超平面保持一定的函数距离。中间的虚线就是我们要找的最佳的超平面，这样的超平面可以确保就算是有些点在直线上面或者里面，也可以很好地区分。 3.最优间隔分类器问题从上面图可知，2条直线之间的距离越大越好，即几何间隔越大越好，这样可以使更多的数据很好的分开来。同时，找到唯一的最好超平面，所有样本点与超平面有一定的函数间隔（即分类准确）： \begin{align*} &\max_{w,b,γ^{i}}\quad \frac{\hat{γ}}{||w||}\\ & \begin{array}{r@{\quad}r@{}l@{\quad}l} s.t.&y(W^{T}X+b)>=\hat{γ}\\ \end{array} \end{align*}发现上面的优化问题是非凸优化，所以为了简化问题，思路是先把此问题转化为对偶问题，然后通过求解对偶问题的解，来解出原问题的解。这就是基本思路。为了简化计算，可以把函数间隔$\hat{γ}$令为1，因为函数间隔扩大n倍时，底下的||W||也扩大n倍，所以并不影响优化。上面分母变成常数1后，问题可以转为最小化分子的问题。||W||加了1/2的平方是便于后面求导，无碍： \begin{align*} &\min_{w,b}\quad \frac{1}{2}||w||^{2}\\ & \begin{array}{r@{\quad}r@{}l@{\quad}l} s.t.&y^{i}(W^{T}X^{i}+b)>=1\\ \end{array} \end{align*}根据凸优化理论，该问题可以通过拉格朗日函数将目标函数转化为无约束的目标函数进行求解： L(w,b,α)=\frac{1}{2}||w||^{2}-\sum\limits_{i=1}^{m}α_{i}(y^{i}(W^{T}X^{i}+b)-1)L对αi和b分别求导并令为0后得： W=\sum\limits_{i=1}^{m}α_{i}y^{i}x^{i}\sum\limits_{i=1}^{m}α_{i}y^{i}=0然后把上面的2个式子代入L： L=\sum\limits_{i=1}^{m}α_{i}-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^my^{i}y^{j}α_{i}α_{j}\quad\quad(1)可以令上面L为新的W(α)函数，后面的尖括号里面的就是xi和xj的内积表达式。 4.对偶问题4.1原始问题先来看原始问题，什么是原始问题？假设$f(x)$,$c_i(x)$,$h_j(x)$是定义在$R^n$上的连续可微函数，考虑约束最优化问题时候： \begin{align} ​ \min_{x \in R^n}\quad &f(x) \\ ​ s.t.\quad&c_i(x) \le 0 , i=1,2,\ldots,k\\ ​ &h_j(x) = 0 , j=1,2,\ldots,k ​ \end{align}上面就是一个有约束条件的原始问题。我们一般的思路就是一脑子求导=0，然后解出x代入原式即可(因为极值处的斜率为0 )。可是这里有约束条件，怎么办？于是拉格朗日先生便一跃而出，他说可以干掉这个纸老虎。看看他怎么解决的？ \mathcal{L}(x,\alpha,\beta) = f(x) + \sum_{i=1}^k \alpha_ic_i(x) + \sum_{j=1}^l \beta_jh_j(x)没错，他也是一股脑的把所有式子塞到一起，起个名字叫做广义拉格朗日函数。其中的α和β都是参数，特别要求αi&gt;=0。直接把上面式子求导代入原式即可。就是上面公式(1)的求解结果。发现上面的式子若不满足约束条件，就至无穷大了，若满足约束条件，则令L的最大值为： \theta_P(x) = \max_{\alpha,\beta:\alpha_i \ge 0}\mathcal{L}(x,\alpha,\beta)发现原始问题就等价于最小化上面的式子： \min_{x}max \mathcal{L}(x,\alpha,\beta)=\min_{x} \theta_P(x)=\min_{x} f(x)\quad\quad(2)因为αi&gt;=0,ci(x)&lt;=0,h(x)=0，所以θp的最大值是f(x)。 4.2对偶问题上面先最大化再最小化问题，对偶问题就是先最小化再最大化问题，即对偶问题为： \max_{\alpha,\beta:\alpha_i\ge0}\theta_D(\alpha,\beta)=\max_{\alpha,\beta:\alpha_i\ge0}\min_x\mathcal{L}(x,\alpha,\beta)\quad\quad(3)在一般情况下公式(2)要&gt;=公式(3),例如，对于同一个问题，x∈{0,1},y∈(0,1)，min max(1(x=y))&gt;=max min(1(x=y)),这里的1(x=y)为示性函数:若括号里的为真，则返回1，否则返回0。左边最大的1=1，再取最小值为1，右式最小的0=0，再取最大的值为0,1&gt;0，故可以看出左式&gt;=右式。感兴趣的可以自己搜下证明。那么，什么时候等式才成立呢？这是个好问题，答：满足KKT条件的时候。这样，我们就可以把原始问题转化为对偶问题，通过解出对偶问题的参数，也就解出原始问题的部分参数，这样就简化了计算。 4.3KKT条件对于KKT条件,由定理：对于原始问题和对偶问题，假设函数$f(x)$和$c_i(x)$是凸函数，$h_i(x)$是仿射函数（即由一阶多项式构成的函数，f(x)=Ax + b, A是矩阵，x，b是向量）；并且假设不等式约束$c_i(x)$是严格可行的，即存在x，对所有i有$c_i(x)&lt;0$，则x*,α*,β*分别是原始问题和对偶问题的最优解的充分必要条件是x*,α*,β*满足下面的Karush-Kuhn-Tucker(KKT)条件： \begin{align} & \nabla_xL(x^*,\alpha^*,\beta^*)=0\\ &\nabla_\alpha L(x^*,\alpha^*,\beta^*)=0\\ &\nabla_\beta L(x^*,\alpha^*,\beta^*)=0\\ &\alpha_i^*c_i(x)=0,i=1,2,\ldots,k\quad\quad(a)\\ &\alpha_i^*\ge0,i=1,2,\ldots,k\quad\quad\quad\quad(b)\\ &c_i(x)\le0,i=1,2,\ldots,k\quad\quad\quad (c)\\ &h_j(x^*)=0,j=1,2,\ldots,l\quad\quad\quad(d) \end{align}前三个求偏导是保证驻点的存在，后面的式子(a)、(b)是拉格朗日乘子需要满足的约束，(c)、(d)是原始问题的约束条件。特别注意当αi*&gt;0时，由KKT对偶互补条件可知:ci(x)=0，这个后面会用到。 5.软间隔SVM第3章节主要讲的是硬间隔最大化，那什么是软间隔最大化？硬间隔呢就是支持向量不能超过第二幅图的直线，顶多在线上面，但是软间隔可以容忍少量的支持向量落在直线内，于是我们的约束条件$y_i(W^TX+b)\ge1$就要修改为$y_i(W^TX+b)\ge1-ξ_i$，即允许部分支持向量间隔不足1,$ξ_i&gt;=0$。我们原始的最大化约束问题转为： \begin{align} \min_{w,b}\quad &\frac{1}{2}||w||^{2}+C\sum\limits_{i=1}^{m}ξ_i\\ s.t.\quad&y^{i}(W^{T}X^{i}+b)\ge1-ξ_i\\ &ξ_i\ge0 \end{align}由于加了松弛变量ξi，优化目标后面也加上惩罚项，其中C值大，对误分类的惩罚越大。前面的平方项希望函数间隔越大，后面的惩罚项希望误分类的样本点越少。通过拉格朗日函数将目标函数转化为无约束的目标函数进行求解： \mathcal{L}(w,\alpha,\beta) = \frac{1}{2}||w||^{2} + C\sum\limits_{i=1}^{m}ξ_i-\sum_{i=1}^m \alpha_i[y^{i}(W^{T}X^{i}+b)-1+ξ_i] -\sum_{j=1}^mu_jξ_i\quad\quad(4)把上面L函数分别对W, b, ξi求偏导=0可得： w = \sum_{i=1}^mα_iy_ix_i\quad\quad(5) \sum_{i=1}^mα_iy_i=0 C - α_i - u_i = 0把上面三个式子代入公式（4），可得到我们的对偶问题的优化目标为： \begin{align} \max_\alpha\quad &\sum\limits_{i=1}^{m}α_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^my_iy_jα_iα_j\\ s.t.\quad&\sum_{i=1}^mα_iy_i=0\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad(6)\\ & 0\leα_i\le C,i=1,2,\dots,m \end{align}有上式可求出α，再由α求出W和b。公式（5）可求出W，而b是所有支持向量满足$y_i(W^TX_i+b)=1$的值b的均值。由KKT条件： α_i[y^{i}(W^{T}X^{i}+b)-1+ξ_i]=0\\ u_iξ_i=0上面第一个式子正是由于αi&gt;0,由KKT推断$y_i(W^TX_i+b)-1+ξ_i=0$所得。可知：满足$0\leα_i\le C$的样本点$(x_s,y_s)$，即为支持向量。详细请点击底部的参考1。 5.1核函数对于线性可分数据用硬间隔和软间隔可以区分数据，但对于非线性的数据怎么办？于是，我们的核函数粉墨登场（这里引用原意：演员化妆演戏）。它干的事情直观感受就是上面最开始的视频里干的部分事：把非线性的数据映射到线性可分的维度里面。来，举个栗子八。就是我们有一组非线性的2维模型： f(X_1,X_2) = A_0+A_1X_1+A_2X_2+A_3X_1^2+A_4X_2^2+A_5X_1^3+A_6X_2^3若令： X_1=X_1,X_2=X_2,X_1^2=X_3,X_2^2=X_4,X_1^3=X_5,X_2^3=X_6则可以转化为6维模型： f(X_1,X_2，X_3，X_4，X_5，X_6) = A_0+A_1X_1+A_2X_2+A_3X_3+A_4X_4+A_5X_5+A_6X_6就是说：低维度不可分的数据模型可以映射到高维度后，变成线性可分的模型。下面的核函数就是干的这个事情，就是有核函数k(x,z)把低维度的φ(xi)和φ(zj)之间的内积在低纬度计算掉： k(x,z)=φ(x)φ(z)常见的核函数有： （1）高斯核函数（Gaussian Kernel） \kappa(x, x_i) = exp(-\frac{||x - x_i||^2}{\delta^2})高斯径向基函数是一种局部性强的核函数，其可以将一个样本映射到一个更高维的空间内，该核函数是应用最广的一个，无论大样本还是小样本都有比较好的性能，而且其相对于多项式核函数参数要少。 （2）线性核函数（Exponential Kernel） \kappa(x,x_i) = x \cdot x_i线性核，主要用于线性可分的情况，我们可以看到特征空间到输入空间的维度是一样的，其参数少速度快，对于线性可分数据，其分类效果很理想，因此我们通常首先尝试用线性核函数来做分类，看看效果如何，如果不行再换别的。 （3）多项式核函数（Polynomial kernel function） \kappa(x, x_i) = ((x\cdot x_i) + 1)^d多项式核函数可以实现将低维的输入空间映射到高纬的特征空间，但是多项式核函数的参数多，当多项式的阶数比较高的时候，核矩阵的元素值将趋于无穷大或者无穷小，计算复杂度会大到无法计算。这几个核函数可以优先使用高斯核函数，据说Andrew Ng只用高斯核函数，可见性能确实优越。当然，还是要根据自己的模型找最适合的核函数。还有很多核函数就不介绍了，感兴趣的自行搜索。 6.SMO算法6.1求解$α_1,α_2$了解了核函数后，上面式子（6）对偶问题的内积换成核函数后，提个负号出来整体变为最小化问题就等价于： \begin{align} \min_\alpha\quad &\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^my_iy_jα_iα_jk(x_i,x_j)-\sum\limits_{i=1}^{m}α_i\\ s.t.\quad&\sum_{i=1}^mα_iy_i=0\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad(7)\\ & 0\leα_i\le C,i=1,2,...,m \end{align}SVM的“核“是什么呢？就是SMO（Sequential Minimal Optimization）算法，他是用来求α的，前面说过：求出了α，就可以求出W和b,接着超平面也就找到了。它的主要思想就是：α有m个变量，但是直接根据优化问题来求解很困难。所以，SMO算法每次只对2个变量更新，其他变量看成是常数，而常数项可以在优化目标中省略。 上面的是具体α求解步骤，稍微讲解一下，公式展开来自公式（7）。min后面优化目标这么长，不要吓坏了，只是把α1和α2变量部分展开来，其余看作常数。下面的限制条件原本是$\sum_{i=1}^mα_iy_i=0$，因为$y_i$只能取-1和1，所以就看作是m个正负$α_i$相加=0，拿出2个变量：α1和α2出来玩玩，其余的和为常数-k。需要注意的是2个相同$y_i$的乘积为1，这里都会用到以简化计算。然后得到第1部分的优化目标和约束条件。(图中的$α_2^{new,uncut}$不对，应该是$α_2^{new,unc}$，后面的其实是unclip) 第2部分，是讲解的α的整个迭代过程。过程上面图片有，讲一下L和H的范围是怎么来的。以左边双红线图为例：α本身就有限制[0,C]，故有图中的正方形。坐标轴的横坐标是α1，纵坐标是α2,由α1-α2=K可得2条红色的线，因为已经限制了α1和α2的范围，只能在方框中。可得到线①、线②。可知线①的α2范围是[-K,C]，线②的α2范围是[0,C-K]，再把α1-α2=K代入即可得到L，H的范围。另一种情况也一样可得。 这一步主要是求解未经剪辑的α2，思路是先简化优化目标函数W(α1，α2)，然后代入α1和α2的关系式消去α1，使得表达式全部是关于α2。接着W（α2）对α2求导=0即可。最后可以得到迭代式子： α_2^{new,unc} = α_2^{old} + \frac{y_2(E_1-E_2)}{k_{11}+k_{22}+k_{12}}6.2求解b当$0&lt;α_1^{new}&lt;C$时,根据KKT条件$y_i(W^TX_i+b)=1$: y_1(\sum_{j=1}^{m}α_jy_jk_{1j}+b_1)=1\\ y_1\cdot y_1(\sum_{j=1}^{m}α_jy_jk_{1j}+b_1)=1\cdot y_1 \\ 即为：\sum_{j=1}^{m}α_jy_jk_{1j}+b_1-y_1=0\\ 迭代得：b_1^{new}=y_1-α_1^{new}y_1k_{11}-α_2^{new}y_2k_{12}-\sum_{j=3}^{m}α_jy_jk_{1j}\\ 本轮迭代中E_1: E_1=g(x_1)-y_1\\ y_1代入上面的b_1^{new}可得：\\ b_1^{new}=b^{old}-E_1+y_1k_{11}(α_1^{old}-α_1^{new})+y_2k_{12}(α_2^{old}-α_2^{new})\\ b_2^{new}同理：\\ b_2^{new}=b^{old}-E_2+y_1k_{12}(α_1^{old}-α_1^{new})+y_2k_{22}(α_2^{old}-α_2^{new})\\ 故：b^{new}=\frac{b_1^{new}+b_2^{new}}{2}最后更新Ei为下一轮迭代做准备: E_i = \sum_{S}α_iy_ik_{ij}+b^{new}-y_i最终的b是： b = y_s-\sum_{S}α_iy_ik(x_i,x_s)即所有支持向量的b的均值即为最终的b值。 6.3求分类超平面即为： \sum_{i=1}^mα_iy_ik(x_i,x_s)+b=07.总结SMO算法部分强烈推荐参考：点击此处，我的多数都是来自于此。这篇讲解的非常仔细，比我不知高到哪里去了。最后参考链接第5个是他这个系列的总体，一共5部分，包括代码实现。SVM优点：泛化错误率低，计算开销不大，结果易解释。缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。适用数据类型：数值型和标称型数据。 非常感谢那些乐于分享知识的人。这篇写了整整2天，算上搭建blog的话有一个礼拜了，也算是对最近的一个梳理和总结吧。最后，推荐一款Markdown编辑器：Typora,做到了所谓的“所见即所得”，这是我听过的世界上最好的宣传语(≧∇≦)ﾉ，Au revoir, chère amie。 参考链接： 1.支持向量机Part2—线性支持向量机 2.svm常用核函数 3.从超平面到SVM（三） 4.最优间隔分类器问题 5.支持向量机Part5—Python实现]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>SVM</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
