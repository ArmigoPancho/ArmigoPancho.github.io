<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[矩阵求导简述]]></title>
    <url>%2F2019%2F05%2F05%2FMatrix-derivatives%2F</url>
    <content type="text"><![CDATA[下面是2份矩阵求偏导的pdf，上传以便查看。 上面两份pdf都有矩阵和向量相关的导数和偏导公式总结，按需查找即可。在下面找到的一份资料，也上传，这一份比较全面也复杂，若掌握了基本的ML求导求偏导就不在话下。]]></content>
      <categories>
        <category>矩阵分析</category>
      </categories>
      <tags>
        <tag>矩阵求导</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[skip-gram模型理解二]]></title>
    <url>%2F2019%2F05%2F04%2FNLP2%2F</url>
    <content type="text"><![CDATA[首先说明的是，此为CS224n课程的笔记，有不到之处，望留言提出，thanks。之前也写过word2vec模型，奈何没理解透，这次再理解下 ==。 1.Skip-Gram模型的简单理解1.1 简单理解独热码和分布式表示词向量和skip-gram简单介绍请看： 此链接。skip-gram说讲简单点就是用中心词去预测周围的左右m个词，它是基于神经网络和词向量模型构成，可以处理数十亿级别的词量，效果居然比以往好的出奇。预测周围t个词的概率是p(w-t | wt)，损失函数如下： 下面是skip-gram model的预测演示： 中心词预测周围词后的计算公式如下，就是下文的p(w_t+j | w_t;θ) 计算方式。 说下UoVc表示输出词向量Uo和中心词向量Vc的乘积值，这个值越大表示两个词越相似，相关性越强，也就越可能出现在中心词周围，然后归一化即可。即算出每个词与中心词的相似程度，再后面选一个最相似的即可（概率最大的）。 下面这一张是更加具体的演示： 显然，我们就是要使得周围词预测数来的概率最大化，即周围词p(w_t+j | w_t;θ) ，θ表示我们所有需要优化的参数，假设语料库是极大量的词，有T个单词，那就是把每一个中心词的前t个词和后t个词对应词的分布式表示（这里计算时候是概率）都乘起来最大化即可。第二个公式是为了最小化的同等变换。 1.2 模型求参方式现在就是要最小化上面负的对数似然函数，对于参数可以用梯度下降法来求参数，于是求导。 其中的P(w_t+j | w_t;θ) 就是通过P(O | C)得出来，对Uo和Vc分别求偏如下： 1.3 模型做什么？针对Vc的求偏导，有一种解释： \begin{align*} \frac{∂J}{∂V_c}&=u_0-\sum_{x=1}^{V}\frac{exp(u_x^TV_c)}{\sum_{w=1}^{V}exp(u_w^TV_c)}u_{x}\\ &=u_{0}-\sum_{x=1}^{v} p(x | c) u_{x} \end{align*}$u_0$是实际输出的上下文向量，后面$p(x | c) $是后面$u_{x}$出现的似然概率的加权；$u_{x}$为期望向量，是上下文向量均值；模型做的事就是要使得$u_{0}-\sum_{x=1}^{v} p(x | c) u_{x}$越小越好。 2.参考1.CS224n视频链接，中英文字幕： https://www.bilibili.com/video/av41393758?t=4704&amp;p=2 2.CS224n视频链接，纯英文字幕： https://www.bilibili.com/video/av13383754?t=207&amp;p=2]]></content>
      <categories>
        <category>CS224n笔记</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[名人问题]]></title>
    <url>%2F2019%2F05%2F03%2Ffind-celebrity%2F</url>
    <content type="text"><![CDATA[1.题目描述 Leetcode 277. Find the Celebrity Suppose you are at a party with n people (labeled from 0 to n - 1) and among them, there may exist one celebrity. The definition of a celebrity is that all the other n - 1 people know him/her but he/she does not know any of them. Now you want to find out who the celebrity is or verify that there is not one. The only thing you are allowed to do is to ask questions like: “Hi, A. Do you know B?” to get information of whether A knows B. You need to find out the celebrity (or verify there is not one) by asking as few questions as possible (in the asymptotic sense). You are given a helper function bool knows(a, b) which tells you whether A knows B. Implement a function int findCelebrity(n), your function should minimize the number of calls to knows. Note: There will be exactly one celebrity if he/she is in the party. Return the celebrity’s label if there is a celebrity in the party. If there is no celebrity, return -1. 首先题目定义了：如果一个人A是名人，那么其他的n-1个人都认识A，而且A不认识其他的n-1个人。这题的目标是要从n个人中找出其中的名人，如果没有，则告知其中没有名人。我们可以调用knows(a,b)函数来询问a是否认识b，而我们要尽量少调用这个函数。 （1）n个人中最多可以有几个名人？ 答案：是1个。如果这n个人中有2个名人分别为A和B，那么按照定义，如果一个人A是名人，那么其他的n-1个人都认识A，而且A不认识其他的n-1个人，这也就是说A不认识B。但与此同时我们又定义了如果一个人B是名人，那么其他的n-1个人都认识B，那么A也应该认识B。这就产生了 contradiction了。因此其中不可以有2个或2个以上的名人。 (2) 如果其他n-1个人都认识A，但是A认识了n-1个人中其中一个人，那么A还是名人吗？ 答案：不是的。 (3) 如果A不认识其他的n-1个人，但是n-1个人中有人不认识A，那么A还是名人吗？ 答案：不是的。 这题最直接的想法应该是暴力，但是暴力的复杂度是多少呢？对于每个人我们需要询问: (1) 他是否认识剩下的n-1个人： 最坏的情况需要调用knows(a,b)函数n-1次 (2) 剩下的n-1个人是否认识他：最坏的情况需要调用knows(a,b)函数n-1次 有的可能会重复，但是总体来说需要询问的次数是n(n−1)2n(n−1)2。即时间复杂度为 O(n2n2) 有没有办法优化算法？ 如果我们从n个人中任意挑两个人a,b出来，询问啊是否认识b，那么只有两种情况： （1）a认识b：那么a肯定不是名人。 （2）b认识a：那么b肯定不是名人。 所以任何一种情况，我们都可以排除掉2个人中的1一个人。如果我们不断地重复的这个过程，直到只剩下一个人，那么我们会做n-1次对比。而剩下这个人是唯一可能成为名人的人，那么我们需要询问剩下的n-1个人是否认识他，也需要询问他是否认识剩下的n-1个人。因此我们一共需要询问3(n-1)次——时间复杂度为O(n)。 2.代码描述本题是参考的网友的博客: https://www.cnblogs.com/rgvb178/p/10117404.html 2.1 怎么样才算是名人？第1：假设某个名人Miss.Celebrity存在，则在这n人里面只会有一个人是名人。 第2：Miss.Celebrity 不认识剩下的n-1个人。正向思考：只要有一个人不认识其中某个人，他就有可能是名人，并且需要验证是否不认识剩下的n-1人。反向思考：若A认识B，则A就不是名人。 第3：所有人都认识此名人。反向思考：即只要某一位不被别人认识那此人就不是名人。其实就是名人在此处的定义为：”他/她 不认识其他所有人，但是其他所有人都认识他/她。” 2.2 python描述：123456789101112131415161718192021"""这里已经假设已有一个函数knows(a，b)，a认识b,函数返回True,否则返回False"""class Solution(object): def find_celebrity(self,n): # 考虑极端情况，即n=0，即没有人也就没名人（n也可能是其他怪异值） if n == 0: return -1 curr_stay = 0 #1.待验证的curr_stay认识i,证明curr_stay不是名人，需要验证后面一位大佬 == #2.等到knows(curr_stay,i)返回False,说明curr_stay可能是名人，于是往下验证 for i in range(1,n): if knows(curr_stay,i): curr_stay = i #3.对所有人验证curr_stay是不是名人？check2条：（1）curr_stay不认识所有人成立？（2）curr_stay被所有人认识成立？ for i in range(0,n): #自己都不认识自己了，那名人面子不要哒。&gt;o&lt; if i == curr_stay：continue #check:curr_stay若是名人,curr_stay不认识所有人返回False,不执行返回-1 if knows(curr_stay,i): return -1 #curr_stay若是名人,所有人认识curr_stay，not True不返回 if not knows(i,curr_stay): return -1 return curr_stay]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>名人问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基本的数据清洗]]></title>
    <url>%2F2019%2F04%2F26%2Fdata-cleaning%2F</url>
    <content type="text"><![CDATA[1.简介简单的把参考的2个微信讲解看了遍，贴出来标记一下。主要对数据检查是否有重复的，对缺失值统计和删除，最后利用KNN回归预测Titanic缺失数据年龄，以进行补偿。github地址：请点击这里 ，直径奔向简单的数据清洗即可。当然实际的数据清洗不会这么理想和简单的，数据量也远远少于实际量。 稍微简单说下最后一部分，用KNN来补偿缺失的年龄： 1.先把数据简单的删去无用的特征和特征类型转换 2.再分成2部分数据：一份年龄所在行数据都不缺失的，另一份是年龄是缺失的 3.KNN回归来把不缺失数据拿来训练：非年龄特征作为训练数据，年龄作为回归对象。然后到预测阶段，直接根据缺失数据的其他属性来回归预测实际年龄 2.代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798import pandas as pdfrom sklearn import neighbors#参考： https://www.jb51.net/article/60510.htm# 1. https://mp.weixin.qq.com/s?__biz=MzU0MDQ1NjAzNg==&amp;mid=2247486609&amp;idx=1&amp;sn=2865ab86b1ab0abf3e50e109bbf0ad14&amp;chksm=fb39a99acc4e208c3bbcb410742adca7839408fc4bed9f4aa2446d4f500fcaa360e388e3c6c8&amp;mpshare=1&amp;scene=23&amp;srcid=#rd# 2. https://mp.weixin.qq.com/s?__biz=MzU0MDQ1NjAzNg==&amp;mid=2247486622&amp;idx=1&amp;sn=d85a4866615f8a503151577abb28edcf&amp;chksm=fb39a995cc4e20835fec4a9dcb0ec9b0047265e24bc013e11e69a7b8c259deab0dbbf0e4f303&amp;mpshare=1&amp;scene=23&amp;srcid=#rd# 读入外部数据data3 = pd.read_excel(io=r'data3.xlsx') # pandas 更新到0.24即可# 1.查看数据的情况print(data3.shape)print(data3.dtypes)# 数值型转字符型data3['id'] = data3['id'].astype(str)# 字符型转数值型data3['custom_amt'] = data3['custom_amt'].str[1:].astype(float)# 字符型转日期型data3['order_date'] = pd.to_datetime(data3['order_date'], format = '%Y年%m月%d日')# 重新查看数据集的各变量类型print(data3.dtypes)print(data3.head())# 判断数据中是否存在重复数据print(data3.duplicated().any())'''需要说明的是，在使用duplicated“方法”对数据行作重复性判断时，会返回一个与原数据行数相同的序列（如果数据行没有重复，则对应False，否则对应True），为了得到最终的判断结果，需要再使用any“方法”（即序列中只要存在一个True，则返回True）。duplicated“方法”和drop_duplicates“方法”都有一个非常重要的参数，就是subset。默认情况下不设置该参数时，表示对数据的所有列进行重复性判断；如果需要按指定的变量做数据的重复性判断时，就可以使用该参数指定具体的变量列表。举例如下：'''df = pd.DataFrame(dict(name = ['张三','李四','王二','张三','赵五','丁一','王二'], gender = ['男','男','女','男','女','女','男'], age = [29,25,27,29,21,22,27], income = [15600,14000,18500,15600,10500,18000,13000], edu = ['本科','本科','硕士','本科','大专','本科','硕士']))# 默认情况下，对数据的所有变量进行判断df.drop_duplicates()print(df)df.drop_duplicates(subset=['name','age'],inplace=True) # name、age一样就判为一样的数据print(df,'\n####################################')'''2.缺失值的简单处理有：删除法、替换法和插补法'''# 判断各变量中是否存在缺失值print(data3.isnull().any(axis = 0))# 各变量中缺失值的数量print(data3.isnull().sum(axis = 0)) # axis为1代表统计行，0统计列#缺失值比例print(data3.isnull().sum(axis=0)/data3.shape[0])# 判断数据行中是否存在缺失值data3.isnull().any(axis = 1).any()# 删除字段 -- 如删除缺失率非常高的edu特征print(data3.drop(labels = 'edu', axis = 1, inplace=True))# 数据预览print(data3.head())# 删除观测，-- 如删除age变量中所对应的缺失观测data3_new = data3.drop(labels = data3.index[data3['age'].isnull()], axis = 0)# 查看数据的规模print(data3_new.shape)# 替换法处理缺失值data3.fillna(value = &#123;'gender': data3['gender'].mode()[0], # 使用性别的众数替换缺失性别 'age':data3['age'].mean() # 使用年龄的平均值替换缺失年龄 &#125;, inplace = True # 原地修改数据 )# 再次查看各变量的缺失比例print(data3.isnull().sum(axis = 0))'''3.KNN插补缺失值：以年龄不缺失的其他5特征为训练模型，训练时拟合已有年龄的数据，然后预测（拟合）缺失部分的年龄。'''# 读取数据titanic = pd.read_csv('Titanic.csv')# 删除缺失严重的Cabin变量titanic.drop(labels='Cabin', axis = 1, inplace=True)# 根据Embarked变量，删除对应的缺失行titanic.dropna(subset=['Embarked'], inplace=True)# 删除无关紧要的变量（这些变量对后面预测年龄没有太多的帮助）titanic.drop(labels=['PassengerId','Name','Ticket','Embarked'], axis = 1, inplace=True)# 将字符型的性别变量映射为数值变量titanic.Sex = titanic.Sex.map(&#123;'male':1, 'female':0&#125;)# 将数据拆分为两组，一是年龄缺失组，二是年龄非缺失组，后续基于非缺失值构建KNN模型，再对缺失组做预测nomissing = titanic.loc[~titanic.Age.isnull(),]missing = titanic.loc[titanic.Age.isnull(),]# X是除了age的所有列的index，即属性名字X = nomissing.columns[nomissing.columns != 'Age']# 构建模型knn = neighbors.KNeighborsRegressor()print('\n===========&gt;', nomissing[X].isnull().any(axis=1).any(), nomissing[X].isnull().any,nomissing[X].isnull().sum(axis=0))# 模型拟合knn.fit(nomissing[X], nomissing.Age)# 缺失表的年龄预测pred_age = knn.predict(missing[X])print(pred_age)# Titanic预测kaggle竞赛： https://www.jianshu.com/p/9b6ee1fb7a60?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation 3.参考请点击：参考1 、参考2 $Je\ \ pense\ , \ donc\ \ je\ \ suis .$]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>数据清洗</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夸夸机器人]]></title>
    <url>%2F2019%2F04%2F20%2Fkuakua-robot%2F</url>
    <content type="text"><![CDATA[1.简介​ 按照网上的夸夸机器人进行了稍微的修改，按照微信性别来自动夸人。利用的是itchat API来自动回复的，没有技术含量。还看了一个用TF-IDF来匹配最相似的问题或者语料，然后匹配已经爬下来的夸夸回复，返回即可。这里只展示第一种，第二种不知道哪去了ozz 。这里是github地址，请点击这里 。对了，操作步骤是： 按照文末的参考1安装各种包 修改夸夸群名字 若系统是Mac、Linux选择enableCmdQR=2，运行程序 手机微信扫描run窗口的二维码，字体太大就调小点嘛，不知道？嘿嘿，不告诉你。 然后我被自己的智商拉低了100个高度，因为我发现登录账号怎么发咒语”夸我”都不灵，原来是要群成员发”夸我”，然后扫描的账号才从夸夸语料中抽取一条自动回复群成员 == ，刚开始以为是真的有随机账号来回复任意成员，没想到这个不是的。 然后就没了，尽情享受夸夸的乐趣吧！ 2.代码展示插个代码，以防github打不开或者打开太慢（其实是凑个页面==），没错，github打开贼慢。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345import itchat, refrom itchat.content import *import random""" Constants"""REPLY = &#123;'夸我0': ['你真是太优秀！', '啥也不说了,夸！', '每天看到你心情好呢！', '你真是一位可爱的小天使啊！', '一看你就是美丽与善良的化身 夸！', '你上辈子一定拯救了银河系吧，优秀！', '德才兼备说的就是你这样的社会主义接班人！', '以后你就是夸夸群里的元老，就是夸夸之父，简称夸父！', '你这句话完美的表达了你想被夸的坚定信念，你一定是一个执着追求自己理想的人！', '我不知道该怎样表达你留在我心中最强最深的印象，是你丰满颀长的身材，白皙的皮肤。乌黑幽深的眼睛，小巧红润的嘴唇，但还有一种说不出，捉不到的丰仪在煽动着我的心。', '遇见你之后，再看别的女人，就好象在侮辱自己的眼睛!', '你笑起来的样子最为动人，两片薄薄的嘴唇在笑，长长的眼睛在笑，腮上两个陷得很举动的酒窝也在笑。', '你从小就流露出才华横溢的天资来。', '仲老，真佩服，满腹经纶!这果然是奥妙!', '这番讲话，既有好教训又说得妙趣横生，给我们官兵以极深刻的印象。', '大贤世居大邦，见多识广，而且荣列胶庠，自然才贯二酉，学富五车了。', '庾信，字子山，南阳新野人也。……幼而俊迈，聪敏绝伦，博览群书，尤善《春秋左氏传》。', '人皆言子建出口成章，臣未深信。主上可召入，以才试之。', '俱怀逸兴壮思飞，欲上青天揽明月。', '这学生天资聪颖，文思敏捷，下笔成章，将来未可限量。', '远近书疏，莫不手答，笔翰如流未尝壅滞。', '你像一片轻柔的云在我眼前飘来飘去，你清丽秀雅的脸上荡漾着春天般美丽的笑容。', '你是那样地美，美得象一首抒情诗。', '你那瓜子形的形，那么白净，弯弯的一双眉毛，那么修长;水汪汪的一对眼睛，那么明亮!', '你总是说话得体，举止大方。不要吝啬赞美!因为你的赞美，对他人是一种鼓励，一种信任。', '因为你的赞美，对朋友可以传递友谊，因为你的赞美，对爱人可以体会到温情。不要吝啬赞美!因为你的赞美，对他人是一种鼓励，一种信任。因为你的赞美，对朋友可以传递友谊，因为你的赞美，对爱人可以体会到温情。', '你表现的很勇敢，是一个真正的男子汉!', '你表现这么优秀，和你在一起的时候压力好大啊!', '你的形体真健美!', '你的眼睛真有神!', '你工作的样子很迷人!', '我对你的表现非常满意。', '你就好像是上品的西湖龙井，那种淡淡的苦涩是你的成熟，越品你越有味道。', '尽管你身材纤弱娇小，说话柔声细气，然而却很有力量，这是一种真正的精神美!', '你娉婷婉约的风姿，娇艳俏丽的容貌，妩媚得体的举止，优雅大方的谈吐，一开始就令我刮目相看。', '你全身充溢着少女的纯情和青春的风采。留给我印象最深的是你那双湖水般清澈的眸子，以及长长的、一闪一闪的睫毛。像是探询，像是关切，像是问候。', '你是花丛中的蝴蝶，是百合花中的蓓蕾。无论什么衣服穿到你的身上，总是那么端庄、好看。', '你也许没有一簇樱唇两排贝齿，但你的谈吐也应该高雅脱俗，机智过人。', '你有点像天上的月亮，也像那闪烁的星星，可惜我不是诗人，否则，当写一万首诗来形容你的美丽。', '你看上去真精神。', '你是一个成熟的人!', '你是一个顾家的好男人。', '你是一个很专情的男人!', '你是一个有责任、有担当的男人!', '你是一个有责任心的人!', '瀑布一般的长发，淡雅的连衣裙，标准的瓜子脸，聪明的杏仁眼，那稳重端庄的气质，再调皮的人见了你都会小心翼翼。', '其实，我最先认识你是在照片上。照片上的你托腮凝眸，若有所思。那份温柔、那份美感、那份妩媚，使我久久难以忘怀。', '进步真快!', '你的观察力很强!', '你的想法很有创意!', '你的作品真棒!', '你很有想法哦!', '你真大方!', '你真能干!', '你真幽默!', '你真有思想!', '你很有想象力哦!', '智慧女人是金子，气质女人是钻石，聪明女人是宝藏，可爱女人是名画，据我考证，你是世界上最大的宝藏，里面装满了金子钻石名画!', '只有莲花才能比得上你的圣洁，只有月亮才能比得上你的冰清。', '在人流中，我一眼就发现了你。我不敢说你是她们中最漂亮的一个，可是我敢说，你是她们中最出色的一个。', '在你那双又大又亮的眼睛里，我总能捕捉到你的宁静，你的热烈，你的聪颖，你的敏感。', '你也许没有若隐若现的酒窝，但你的微笑一定是月闭花羞，鱼沉雁落。', '你也许没有水汪汪亮晶晶的眼睛，但你的眼神也应该顾盼多情，勾魂摄魄。', '你很有责任感哦!', '你今天的表现令大家都很开心!', '你今天给了我很多的惊喜!', '你学的真快!', '你真棒!', '你真聪明!', '你真的很能干哦!', '你真可爱!', '你真是个聪明的孩子!', '你做的非常好!', '你做对了', '做得好极了!', '你最近进步很大，继续保持。', '你的学习成绩提升的很快，希望你下面能再接再厉，再创辉煌!', '遇见你之后，再看别的女人，就好象在侮辱自己的眼睛!', '漂亮女孩，天生就漂亮。白皙的皮肤，大大的眼睛，秀气的鼻子，饱满的小嘴，再加上一头可爱的"自来卷"，构成一幅天然的美丽图画。', '清澈明亮的瞳孔，弯弯的柳眉，长长的睫毛微微地颤动着，白皙无瑕的皮肤透出淡淡红粉，薄薄的双唇如玫瑰花瓣娇嫩欲滴。', '若说她年纪轻轻，怎生得如此身段，且有一张勾魂摄魄的俏脸。', '那眼神优雅、娴静，双眼回盼流波，像是俏丽的江南女子;但又挂着一丝倔犟的波纹，又带着北国女儿的神韵。', '清水出芙蓉，天然去雕饰。——李白', '同样是美女，这个女孩给人最深刻的印象是她眉宇之间有种超越了她年龄的惊人的美丽，淡淡的柳眉分明仔细的修饰过，长长的睫毛忽闪忽闪的象两把小刷子，亮得让人觉得刺目的一双漂亮到心悸的大眼睛，异常的灵动有神。', '细致乌黑的长发，常常披于双肩之上，略显柔美，有时松散的数着长发，显出一种别样的风采，突然由成熟变得可爱，让人新生喜爱怜惜之情，洁白的皮肤犹如刚剥壳的鸡蛋，大大的眼睛一闪一闪仿佛会说话，小小的红唇与皮肤的白色，更显分明，一对小酒窝均匀的分布在脸颊两侧，浅浅一笑，酒窝在脸颊若隐若现，可爱如天仙。', '皓腕胜雪，乌发如云，她的眼眸水光潋滟、媚眼如丝，一双勾魂的眼，只一眼，就完全沉溺其中不可自拔。高挺鼻子下的那张玫色小嘴微微张着，如同妖艳的玫瑰。她的面容如娇嫩清雅，犹如杯中之莲，绝色之姿灵气逼人。那美，用怎样的辞藻来形容都是苍白而无力，真是只可意会，不可言传。她静若处子，似不食人间烟火，动若脱兔，眉间微存的稚气带着无比的灵动。', '世上美人众多，肥环燕瘦，无一人有她那样独特的气质。孤傲、无畏、自信、有着一股不羁的野性，她是最璀璨的光华结晶，如同一团烈火，激烈且张狂地燃烧着。她让美丽不再只是容貌上，而是由心真正的散发出来，她紧抓着众人的眼。再加之，一袭冰蚕丝纱裙所衬托出的空灵气质，与在阳光照射下而形成的淡淡光晕，更若天女下凡，绝美无双!', '一张清爽的鹅蛋脸，一头柔顺的直发。高挺的鼻梁，粉玫色的唇瓣，精致的单眼皮搭配着纤长的眼睫，微敛住深褐色的眼眸，闪烁着快乐的光芒，但光芒的深处，是一股浓到化不开的孤寂。', '双眉有如柳叶刀裁，盈盈笑意眉上来，一句“云髻峨峨，修眉联娟”得以道出碧瑶云云细眉。', '双目似有千情万怨，道不尽也诉不完，一句“巧笑倩兮，美目盼兮”可能描述碧瑶盈盈眼波。', '她那双大大的眼睛，闪烁着聪颖的光辉，像两颗朗朗的星星。', '她和她的哥哥一样，生就一副绝顶聪明的头脑，心灵得像窗纸，一点就透。', '聪明的人不是具有广博知识的人，而是掌握了有用知识的人。', '你那双乌黑晶亮的眼睛，骨碌碌地打转，显得很机灵懂事。', '别看你人小，心眼可灵啦，10个大人也比不上你，真是秤砣虽小能吊千斤。', '你是个聪明的孩子，精灵得像个小猴儿。', '别看阿墩胖得肥肉直打颤颤，动作却机警得像只与猎人周旋的豹子。', '她那双圆溜溜的大眼睛，镶了一圈乌黑闪亮的长睫毛，眨动之间，透出一股聪明伶俐劲儿。', '在午后的阳光下，没有丝毫红晕，清秀的脸上只显出了一种病态的苍白，却无时不流露出高贵淡雅的气质，配合你颀长纤细的身材。' '一张坏坏的笑脸，连两道浓浓的眉毛也泛起柔柔的涟漪，好像一直都带着笑意，弯弯的，像是夜空里皎洁的上弦月。白皙的皮肤衬托着淡淡桃红色的嘴唇，俊美突出的五官，完美的脸型，特别是左耳闪着炫目光亮的钻石耳钉，给你的阳光帅气中加入了一丝不羁。', '我和你在一起的时候压力好大啊!谁让你这么优秀啦!', '你乌灵的眼眸，倏地笼上层嗜血的寒意，仿若魔神降世一般，一双冰眸轻易贯穿人心，刺透心底最柔弱，舞衣的角落。', '你那红嘟嘟地脸蛋闪着光亮，像九月里熟透地苹果一样。', '你的耳朵白里透红，耳轮分明，外圈和里圈很匀称，像是一件雕刻出来地艺术品。', '你的眉毛时而紧紧地皱起，眉宇间形成一个问号;时而愉快地舒展，像个感叹号。', '浓密的眉毛叛逆地稍稍向上扬起，长而微卷的睫毛下，有着一双像朝露一样清澈的眼睛，英挺的鼻梁，像玫瑰花瓣一样粉嫩的嘴唇，还有白皙的皮肤。', '你有时候是不是特孤独?世界上这么优秀的人就只有你一个!', '你就好像是上品的西湖龙井那种淡淡的苦涩是你的成熟越品你越有味道。', '那个男子立体的五官刀刻般俊美，整个人发出一种威震天下的王者之气，邪恶而俊美的脸上此时噙着一抹放荡不拘的微笑。'], '夸我1': ['你真是太优秀！', '啥也不说了，夸！', '每天看到你心情好呢！', '你真是一位帅气的man！', '你上辈子一定拯救了银河系吧，优秀！', '德才兼备说的就是你这样的社会主义接班人！', '以后你就是夸夸群里的元老，就是夸夸之父，简称夸父！', '你这句话完美的表达了你想被夸的坚定信念，你一定是一个执着追求自己理想的人！', '你从小就流露出才华横溢的天资来。', '仲老，真佩服，满腹经纶!这果然是奥妙!', '这番讲话，既有好教训又说得妙趣横生，给我们以极深刻的印象。', '大贤世居大邦，见多识广，而且荣列胶庠，自然才贯二酉，学富五车了。', '庾信，字子山，南阳新野人也。……幼而俊迈，聪敏绝伦，博览群书，尤善《春秋左氏传》。', '人皆言子建出口成章，臣未深信。主上可召入，以才试之。', '俱怀逸兴壮思飞，欲上青天揽明月。', '这学生天资聪颖，文思敏捷，下笔成章，将来未可限量。', '远近书疏，莫不手答，笔翰如流未尝壅滞。', '你像一片轻柔的云在我眼前飘来飘去，你清丽秀雅的脸上荡漾着春天般美丽的笑容。', '因为你的赞美，对朋友可以传递友谊，因为你的赞美，对爱人可以体会到温情。不要吝啬赞美!因为你的赞美，对他人是一种鼓励，一种信任。因为你的赞美，对朋友可以传递友谊，因为你的赞美，对爱人可以体会到温情。', '你表现的很勇敢，是一个真正的男子汉!', '你表现这么优秀，和你在一起的时候压力好大啊!', '你的形体真健美!', '你的眼睛真有神!', '你工作的样子很迷人!', '我对你的表现非常满意。', '你就好像是上品的西湖龙井，那种淡淡的苦涩是你的成熟，越品你越有味道。', '你是一个成熟的人!', '你是一个顾家的好男人。', '你是一个很专情的男人!', '你是一个有责任、有担当的男人!', '你是一个有责任心的人!', '你进步真快!', '你的观察力很强!', '你的想法很有创意!', '你的作品真棒!', '你很有想法哦!', '你真大方!', '你真能干!', '你真幽默!', '你真有思想!', '你很有想象力哦!', '你很有责任感哦!', '你今天的表现令大家都很开心!', '你今天给了我很多的惊喜!', '你学的真快!', '你真棒!', '你真聪明!', '你真的很能干哦!', '你真可爱!', '你真是个聪明的孩子!', '你做的非常好!', '你做对了', '做得好极了!', '你最近进步很大，继续保持。', '你的学习成绩提升的很快，希望你下面能再接再厉，再创辉煌!', '你就好像是上品的西湖龙井那种淡淡的苦涩是你的成熟越品你越有味道。', '那个男子立体的五官刀刻般俊美，整个人发出一种威震天下的王者之气，邪恶而俊美的脸上此时噙着一抹放荡不拘的微笑。'], '夸我2': ['你真是太优秀！', '啥也不说了，夸！', '每天看到你心情好呢！', '你真是一位可爱的小天使啊！', '一看你就是美丽与善良的化身 夸！', '你上辈子一定拯救了银河系吧，优秀！', '德才兼备说的就是你这样的社会主义接班人！', '以后你就是夸夸群里的元老，就是夸夸之父，简称夸父！', '你这句话完美的表达了你想被夸的坚定信念，你一定是一个执着追求自己理想的人！', '我不知道该怎样表达你留在我心中最强最深的印象，是你丰满颀长的身材，白皙的皮肤。乌黑幽深的眼睛，小巧红润的嘴唇，但还有一种说不出，捉不到的丰仪在煽动着我的心。', '遇见你之后，眼睛就容不下其他女生了!', '你笑起来的样子最为动人，两片薄薄的嘴唇在笑，长长的眼睛在笑，腮上两个陷得很举动的酒窝也在笑。', '你从小就流露出才华横溢的天资来。', '庾信，字子山，南阳新野人也。……幼而俊迈，聪敏绝伦，博览群书，尤善《春秋左氏传》。', '人皆言子建出口成章，臣未深信。主上可召入，以才试之。', '俱怀逸兴壮思飞，欲上青天揽明月。', '这学生天资聪颖，文思敏捷，下笔成章，将来未可限量。', '远近书疏，莫不手答，笔翰如流未尝壅滞。', '你像一片轻柔的云在我眼前飘来飘去，你清丽秀雅的脸上荡漾着春天般美丽的笑容。', '你是那样地美，美得象一首抒情诗。', '你那瓜子形的形，那么白净，弯弯的一双眉毛，那么修长;水汪汪的一对眼睛，那么明亮!', '你总是说话得体，举止大方。不要吝啬赞美!因为你的赞美，对他人是一种鼓励，一种信任。', '因为你的赞美，对朋友可以传递友谊，因为你的赞美，对爱人可以体会到温情。不要吝啬赞美!因为你的赞美，对他人是一种鼓励，一种信任。因为你的赞美，对朋友可以传递友谊，因为你的赞美，对爱人可以体会到温情。', '你表现这么优秀，和你在一起的时候压力好大啊!', '你的眼睛真有神!', '你工作的样子很迷人!', '我对你的表现非常满意。', '尽管你身材纤弱娇小，说话柔声细气，然而却很有力量，这是一种真正的精神美!', '你娉婷婉约的风姿，娇艳俏丽的容貌，妩媚得体的举止，优雅大方的谈吐，一开始就令我刮目相看。', '你全身充溢着少女的纯情和青春的风采。留给我印象最深的是你那双湖水般清澈的眸子，以及长长的、一闪一闪的睫毛。像是探询，像是关切，像是问候。', '你是花丛中的蝴蝶，是百合花中的蓓蕾。无论什么衣服穿到你的身上，总是那么端庄、好看。', '你也许没有一簇樱唇两排贝齿，但你的谈吐也应该高雅脱俗，机智过人。', '你有点像天上的月亮，也像那闪烁的星星，可惜我不是诗人，否则，当写一万首诗来形容你的美丽。', '你看上去真精神。', '你是一个成熟的人!', '你是一个有责任心的人!', '瀑布一般的长发，淡雅的连衣裙，标准的瓜子脸，聪明的杏仁眼，那稳重端庄的气质，再调皮的人见了你都会小心翼翼。', '其实，我最先认识你是在照片上。照片上的你托腮凝眸，若有所思。那份温柔、那份美感、那份妩媚，使我久久难以忘怀。', '进步真快!', '你的观察力很强!', '你的想法很有创意!', '你的作品真棒!', '你很有想法哦!', '你真大方!', '你真能干!', '你真幽默!', '你真有思想!', '你很有想象力哦!', '智慧女人是金子，气质女人是钻石，聪明女人是宝藏，可爱女人是名画，据我考证，你是世界上最大的宝藏，里面装满了金子钻石名画!', '只有莲花才能比得上你的圣洁，只有月亮才能比得上你的冰清。', '在人流中，我一眼就发现了你。我不敢说你是她们中最漂亮的一个，可是我敢说，你是她们中最出色的一个。', '在你那双又大又亮的眼睛里，我总能捕捉到你的宁静，你的热烈，你的聪颖，你的敏感。', '你也许没有若隐若现的酒窝，但你的微笑一定是月闭花羞，鱼沉雁落。', '你也许没有水汪汪亮晶晶的眼睛，但你的眼神也应该顾盼多情，勾魂摄魄。', '你很有责任感哦!', '你今天的表现令大家都很开心!', '你今天给了我很多的惊喜!', '你学的真快!', '你真棒!', '你真聪明!', '你真的很能干哦!', '你真可爱!', '你真是个聪明的孩子!', '你做的非常好!', '你做对了', '做得好极了!', '你最近进步很大，继续保持。', '你的学习成绩提升的很快，希望你下面能再接再厉，再创辉煌!', '遇见你之后，再看别的女人，就好象在侮辱自己的眼睛!', '漂亮女孩，天生就漂亮。白皙的皮肤，大大的眼睛，秀气的鼻子，饱满的小嘴，再加上一头可爱的"自来卷"，构成一幅天然的美丽图画。', '清澈明亮的瞳孔，弯弯的柳眉，长长的睫毛微微地颤动着，白皙无瑕的皮肤透出淡淡红粉，薄薄的双唇如玫瑰花瓣娇嫩欲滴。', '那眼神优雅、娴静，双眼回盼流波，像是俏丽的江南女子;但又挂着一丝倔犟的波纹，又带着北国女儿的神韵。', '清水出芙蓉，天然去雕饰。', '同样是美女，这个女孩给人最深刻的印象是她眉宇之间有种超越了她年龄的惊人的美丽，淡淡的柳眉分明仔细的修饰过，长长的睫毛忽闪忽闪的象两把小刷子，亮得让人觉得刺目的一双漂亮到心悸的大眼睛，异常的灵动有神。', '细致乌黑的长发，常常披于双肩之上，略显柔美，有时松散的数着长发，显出一种别样的风采，突然由成熟变得可爱，让人新生喜爱怜惜之情，洁白的皮肤犹如刚剥壳的鸡蛋，大大的眼睛一闪一闪仿佛会说话，小小的红唇与皮肤的白色，更显分明，一对小酒窝均匀的分布在脸颊两侧，浅浅一笑，酒窝在脸颊若隐若现，可爱如天仙。', '世上美人众多，肥环燕瘦，无一人有她那样独特的气质。孤傲、无畏、自信、有着一股不羁的野性，她是最璀璨的光华结晶，如同一团烈火，激烈且张狂地燃烧着。她让美丽不再只是容貌上，而是由心真正的散发出来，她紧抓着众人的眼。再加之，一袭冰蚕丝纱裙所衬托出的空灵气质，与在阳光照射下而形成的淡淡光晕，更若天女下凡，绝美无双!', '一张清爽的鹅蛋脸，一头柔顺的直发。高挺的鼻梁，粉玫色的唇瓣，精致的单眼皮搭配着纤长的眼睫，微敛住深褐色的眼眸，闪烁着快乐的光芒，但光芒的深处，是一股浓到化不开的孤寂。', '双眉有如柳叶刀裁，盈盈笑意眉上来，一句“云髻峨峨，修眉联娟”得以道出碧瑶云云细眉。', '双目似有千情万怨，道不尽也诉不完，一句“巧笑倩兮，美目盼兮”可能描述碧瑶盈盈眼波。', '她那双大大的眼睛，闪烁着聪颖的光辉，像两颗朗朗的星星。', '聪明的人不是具有广博知识的人，而是掌握了有用知识的人。', '你那双乌黑晶亮的眼睛，骨碌碌地打转，显得很机灵懂事。', '她那双圆溜溜的大眼睛，镶了一圈乌黑闪亮的长睫毛，眨动之间，透出一股聪明伶俐劲儿。', '一张坏坏的笑脸，连两道浓浓的眉毛也泛起柔柔的涟漪，好像一直都带着笑意，弯弯的，像是夜空里皎洁的上弦月。白皙的皮肤衬托着淡淡桃红色的嘴唇，俊美突出的五官，完美的脸型，特别是左耳闪着炫目光亮的钻石耳钉，给你的阳光帅气中加入了一丝不羁。', '我和你在一起的时候压力好大啊!谁让你这么优秀啦!', '你乌灵的眼眸，倏地笼上层嗜血的寒意，仿若魔神降世一般，一双冰眸轻易贯穿人心，刺透心底最柔弱，舞衣的角落。', '你那红嘟嘟地脸蛋闪着光亮，像九月里熟透地苹果一样。', '你的耳朵白里透红，耳轮分明，外圈和里圈很匀称，像是一件雕刻出来地艺术品。'], 'default': ['太棒了！', '真不错！', '好开心！', '嗯哪！', '没什么好说的了，我送你一道彩虹吧！']&#125;@itchat.msg_register([TEXT], isGroupChat=True)def text_reply(msg): # group_name的值修改成你要夸的weixin群名 group_name = '桔子次苹果' if msg['User']['NickName'] == group_name: print('Message from: %s' % msg['User']['NickName']) # 发送者的昵称 username = msg['ActualNickName'] print('Who sent it: %s' % username) match = re.search('夸我', msg['Text']) or re.search('求夸', msg['Text']) user_info = itchat.search_friends(name='&#123;&#125;'.format(username)) print('user_info type是：', type(user_info)) user_info_str = str(user_info) print('user_info变更后type是：', type(user_info)) print('用户信息====', user_info) print('len(REPLY[\'夸我2\'])=', len(REPLY['夸我2'])) print('匹配到性别为：', re.search('\'Sex\': 1', user_info_str)) if match: # 获取发消息人的性别，然后依据性别来夸人。未标注的就随机夸。 if re.search('\'Sex\': 1', user_info_str) is not None: print('-+-+' * 5) print('Message content:%s' % msg['Content']) print('夸我 is: %s' % (match is not None)) randomIdx = random.randint(0, len(REPLY['夸我1']) - 1) itchat.send('%s' % (REPLY['夸我1'][randomIdx]), msg['FromUserName']) elif re.search('\'Sex\': 2', user_info_str) is not None: print('-+-+' * 5) print('Message content:%s' % msg['Content']) print('夸我 is: %s' % (match is not None)) randomIdx = random.randint(0, len(REPLY['夸我2']) - 1) itchat.send( '%s' % (REPLY['夸我2'][randomIdx]), msg['FromUserName']) else: print('-+-+' * 5) print('Message content:%s' % msg['Content']) print('夸我 is: %s' % (match is not None)) randomIdx = random.randint(0, len(REPLY['夸我0']) - 1) itchat.send('%s' % (REPLY['夸我0'][randomIdx]), msg['FromUserName']) print('isAt is:%s' % msg['isAt']) if msg['isAt']: randomIdx = random.randint(0, len(REPLY['default']) - 1) itchat.send('@' + '%s\n%s' % (username, REPLY['default'][randomIdx]), msg['FromUserName']) print('-+-+'*5)# Windows系统，enableCmdQR=Trueitchat.auto_login(enableCmdQR=True, hotReload=True)# 若操作系统是Mac、Linux，enableCmdQR=2# itchat.auto_login(enableCmdQR=2, hotReload=True)itchat.run()'''# 获取好友列表friends = itchat.get_friends()[0:]# 男性male = 0# 女性female = 0# 未知性别other = 0for i in friends[1:]: sex = i['Sex'] if sex == 1: male += 1 elif sex == 2: female += 1 else: other += 1# 微信好友数量total = len(friends[1:])print("微信好友总数:%d" % (total))print("男性好友:%.2f%%" % (float(male) / total * 100))print("女性好友:%.2f%%" % (float(female) / total * 100))print("未知性别好友:%.2f%%" % (float(other) / total * 100))''' 3.参考1.参考blog： https://www.cnblogs.com/geeksongs/p/10581302.html 2.itchart说明文档: https://itchat.readthedocs.io/zh/latest/]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>夸夸机器人</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bagging简介]]></title>
    <url>%2F2019%2F03%2F25%2Fbagging%2F</url>
    <content type="text"><![CDATA[1.Bagging算法介绍1.1 思想介绍介绍完AdaBoost，来学习下Bagging算法，这是集成算法里面另一个集大成者，代表有随机森林等算法。先看看Bagging是什么。它是并行式集成学习最著名的代表作，基于自主采样法（bootstrap sampling）。给定m个样本的数据集，随机抽取一个样本放入采样集，再把样本放回初始数据集，使得下次再有可能抽到此样本。这样经过m次抽样得到含m个样本的采样集；但是原始样本集里面有的样本被多次抽到，有的则一次也没有抽到。这样，我们可以得到T个包含m个样本的训练集，然后每一个训练集训练一个基学习器，再将这些基学习器进行结合即可。一般做分类的话就是最后基学习器的投票多数类即为最后预测类标，若做回归任务，通常使用简单平均法，对T个弱学习器得到的回归结果进行算术平均得到最终的模型输出。 1.2 Bagging算法流程输入为样本集$D={(x_1,y_1),(x_2,y_2),…(x_m,y_m)}$，弱学习器算法, 弱分类器迭代次数T。输出为最终的强分类器$f(x)$ 1）对于$t=1,2…,T$： a)对训练集进行第$m$次随机采样，共采集$t$轮，得到包含$m$个样本的采样集$D_t$ b)用采样集$D_t$训练第$t$个弱学习器$G_t(x)$ 2) 如果是分类算法预测，则$T$个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，$T$个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。 1.3 自助采样法的采样极限因为Bagging的子采样是放回采样，所以有部分是一直采不到的。我们简单计算一下这个”踩不到”的部分有多少。因为是有放回的采样，所以每次从m个样本中采样一个样本的概率是1/m，那踩不到的概率就是1-1/m，有这样m次采样，所以所有踩不到的概率是$(1-\frac{1}{m})^m$，由第二重要极限$\lim\limits_{m \rightarrow \infty} (1+\frac{1}{m})^m=e$ 知： \lim\limits_{m \rightarrow \infty} (1-\frac{1}{m})^m=\lim\limits_{m \rightarrow \infty} [(1+\frac{1}{-m})^{（-m）}]^{-1}=\frac{1}{e}≈0.368就是说有36.8%的样本会没有抽到，即没有用于训练。 顺便一提，假定基学习器的计算复杂度是$O(m)$，则Bagging的复杂度是$T(O(m)+O(s))$，但是参与投票/平均的过程$O(s)$和T都不是很大，所以训练一个Bagging集成和直接使用基学习器算法训练一个学习器的复杂度是同阶的。故Bagging是一个高效的集成学习算法。 2.随机森林随机森林（Random Forest）是Bagging一个开展体。传统决策树在选择划分属性时是在当前节点的属性集合里面选择最优的一个属性；而在RF中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集里面选择一个最优的属性用于划分。这里的k控制了随机性的引入程度。若k=d，则基决策树的构建与传统决策树一致，若令k=1,则随机选择一个属性用于划分；一般地，推荐使用$k=log_2d$。 随机森林简单、容易实现、计算开销小、性能也是优的。 3.决策树和随机森林代码可耻的掉包了，嗯，按照最后参考中的第三个书籍，需要对照书本Robert Layton著作，杜春晓翻译的《Python数据碗挖掘入门与实践》P31：第三章节来看。这里贴上代码github地址： 请点击这里 ，下面是代码展示，数据集请去GitHub下载。主要是提取特定的特征然后进行NBA球赛的分类预测，分类器用到了决策树、随机森林。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145import pandas as pdfrom collections import defaultdictfrom sklearn.grid_search import GridSearchCVfrom sklearn.model_selection import cross_val_scorefrom sklearn.tree import DecisionTreeClassifierimport numpy as npfrom sklearn.preprocessing import LabelEncoderfrom sklearn.preprocessing import OneHotEncoderfrom sklearn.ensemble import RandomForestClassifier"""用决策树预测获胜球队"""dataset = pd.read_csv('sportsref_download (1).csv', parse_dates=["Date"], usecols=[0,1,2,3,4,5,6,9]) #read_csv()参数介绍：https://www.cnblogs.com/datablog/p/6127000.htmldataset.columns = ['Date', 'Score Type', 'Visitor Team', 'VisitorPts', 'Home Team', 'HomePts', 'OT?', 'Notes'] # 修改列名字# # Note：筛选某一列名的所在行# for i in range(len(dataset)):# if dataset['Score Type'][i] == '8:00p':# print(dataset[i:i+1])# 主场胜就是1，否则客场胜为0dataset['HomeWin'] = dataset["VisitorPts"] &lt; dataset["HomePts"]y_true = dataset["HomeWin"].values# 新增2列值dataset["HomeLastWin"] = Falsedataset["VisitorLastWin"] = False# print(dataset.ix[:5])print("Home Win percentage: &#123;0:.1f&#125;%".format(100 * dataset["HomeWin"].sum() / dataset["HomeWin"].count()))# 遍历所有数据，并设置主、客场上一场的胜负情况数据won_last = defaultdict(int)for index, row in dataset.iterrows(): # Note that this is not efficient home_team_name = row["Home Team"] visitor_team_name = row["Visitor Team"] row["HomeLastWin"] = won_last[home_team_name] # 初始化都是False row["VisitorLastWin"] = won_last[visitor_team_name] dataset.ix[index] = row # Set current win won_last[home_team_name] = row["HomeWin"] # 每一次上次某行的主、客队伍胜负情况都会被当前的覆盖 won_last[visitor_team_name] = not row["HomeWin"]# print(dataset.ix[:5])# ########### No1. 2列特征做训练# 其取值不变时，用相同的训练集建树得到的结果一模一样，对测试集的预测结果也是一样的；其值改变时，得到的结果不同；若不设置此参数，则函数会自动选择一种随机模式，每次得到的结果也就不同。clf = DecisionTreeClassifier(random_state=12)X_previouswins = dataset[['HomeLastWin', 'VisitorLastWin']].values # 把数据集中某几列凑成list单独拿出来scores = cross_val_score(clf, X_previouswins, y_true, cv=10, scoring='accuracy')print('主客队上场比赛结果为特征的Accuracy:&#123;0:.1f&#125;%'.format(np.mean(scores) * 100))# ######## No2. 用2013年战绩来添加一个“主场对是否比对手水平高”的属性,3列特征做训练standing = pd.read_csv("expend_standings.csv", skiprows=[0]) # 表名字不要# print("取出某球队名字的行的所在'Team'列的值：", standing[standing['Team'] == 'Denver Nuggets']["Rk"].values[0])dataset['HomeTeamRanksHigher'] = 0for index, row in dataset.iterrows(): home_team_name = row["Home Team"] visitor_team_name = row["Visitor Team"] if home_team_name == "New Orleans Pelicans": # 检查名字是否更名 home_team_name = "New Orleans Hornets" elif visitor_team_name == "New Orleans Pelicans": visitor_team_name = "New Orleans Hornets" home_rank = standing[standing['Team'] == home_team_name]["Rk"].values[0] visitor_rank = standing[standing['Team'] == visitor_team_name]["Rk"].values[0] row['HomeTeamRanksHigher'] = int(home_rank &gt; visitor_rank) dataset.ix[index] = row# print(dataset.ix[:5])# No3.再添加一个新的特征：上一场比赛情况X_homehigher = dataset[["HomeLastWin", "VisitorLastWin", "HomeTeamRanksHigher"]].valuesclf = DecisionTreeClassifier(random_state=14)scores = cross_val_score(clf, X_homehigher, y_true, scoring="accuracy")print("添加比赛等级特征后的Accuracy:&#123;0:.1f&#125;".format(np.mean(scores) * 100))last_math_winer = defaultdict(int)dataset["HomeTeamWonLast"] = 0for index, row in dataset.iterrows(): home_team = row["Home Team"] visitor_team = row["Visitor Team"] teams = tuple(sorted([home_team, visitor_team])) # 球队名字按照字母排序，上一场比赛的赢得为键 row["HomeTeamWonLast"] = 1 if last_math_winer[teams] == row["Home Team"] else 0 dataset.ix[index] = row winner = row["Home Team"] if row["HomeWin"] else row["Visitor Team"] last_math_winer[teams] = winnerX_lastwinner = dataset[["HomeTeamRanksHigher", "HomeTeamWonLast"]].valuesclf = DecisionTreeClassifier(random_state=14)scores = cross_val_score(clf, X_lastwinner, y_true, scoring="accuracy")print("根据上次比赛胜负和主队以往排名为特征的Accuracy:&#123;0:.1f&#125;%".format(np.mean(scores) * 100))# No4.球队名字用独热码表示encoding = LabelEncoder()encoding.fit(dataset["Home Team"].values)home_teams = encoding.transform(dataset["Home Team"].values)visitor_teams = encoding.transform(dataset["Visitor Team"].values)x_teams = np.vstack([home_teams, visitor_teams]).T# print(x_teams)onehot = OneHotEncoder()x_teams_expand = onehot.fit_transform(x_teams).todense()clf = DecisionTreeClassifier(random_state=14)scores = cross_val_score(clf, x_teams_expand, y_true, scoring="accuracy")print("球队名字用独热码表示后：Accuracy:&#123;0:.1f&#125;%".format(np.mean(scores) * 100))# ######################## 随机森林 ####################################clf = RandomForestClassifier(random_state=14)score = cross_val_score(clf, x_teams_expand, y_true, scoring="accuracy")print("随机森林做分类器后的Accuracy：&#123;0:.1f&#125;%".format(np.mean(score)*100))# 多加入几个特征试试效果x_all = np.hstack([X_homehigher, x_teams_expand])print('测试特征有：', np.shape(x_all))clf = RandomForestClassifier(random_state=14)score = cross_val_score(clf, x_all, y_true, scoring="accuracy")print("随机森林后的Accuracy：&#123;0:.1f&#125;%".format(np.mean(score)*100))parameter_space = &#123; "max_features": [2, 10, 'auto'], "n_estimators": [100], "criterion": ["gini", "entropy"], "min_samples_leaf": [2, 4, 6] &#125;clf = RandomForestClassifier(random_state=14)grid = GridSearchCV(clf, parameter_space)grid.fit(x_all, y_true)print("随机森林+网格搜索后的Accuracy：&#123;0:.1f&#125;%".format(grid.best_score_ * 100))print(grid.best_params_)# 参数最好的设置clf = RandomForestClassifier(bootstrap=True, random_state=14, criterion="entropy", max_depth=None, max_features=2, max_leaf_nodes=None, min_samples_leaf=6, min_samples_split=2, n_estimators=100, n_jobs=2, oob_score=False, verbose=0, )score = cross_val_score(clf, x_all, y_true, scoring="accuracy")print("随机森林参数设置后的Accuracy：&#123;0:.1f&#125;%".format(np.mean(score)*100)) 4.参考 老刘的博客： https://www.cnblogs.com/pinard/p/6156009.html 周志华《机器学习》 Robert Layton著作，杜春晓翻译的《Python数据碗挖掘入门与实践》，人民邮电出版社 总有一天，这世界会再次焕发光彩，我相信，一定会到来。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Bagging</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性判别分析]]></title>
    <url>%2F2019%2F03%2F21%2FLDA%2F</url>
    <content type="text"><![CDATA[1.线性判别分析介绍线性判别分析（Linear Discriminant Analysis，LDA）是一种经典的线性学习方法，可以分类和预测。还有一个也叫LDA是主题模型里面的，这里不提。简单的说此LDA就是如下图所示，把同一类的点都尽可能地集中某条线段上，不同类的尽可能分开。总结：同类的投影点越集中，异类的投影点要越分开。 2.LDA二分类证明主要参考周志华的西瓜书和这个链接就直接上笔记，打公式太麻烦，嗯。 上图中因为分子分母都是关于$w$的二次项，所以解与$w$的长度无关，只与它的方向有关，所以令$w^Ts_ww=1$以简化计算。具体参考《机器学习》P61. 3.代码参考代码请参考： 此链接 ，已有数据集。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>线性判别分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AdaBoost简介]]></title>
    <url>%2F2019%2F03%2F14%2FAdaBoost%2F</url>
    <content type="text"><![CDATA[1.基本概念 我们可以对集成学习的思想做一个概括。对于训练集数据，我们通过训练若干个个体学习器，通过一定的结合策略，就可以最终形成一个强学习器。 个体学习器 由一些异质集成的个体学习器由不同算法构成，通常叫做“组件学习器”，有时候直接叫做个体学习器。 基学习器 就是同质的（同一个算法或者相似算法）学习算法集成在一起个体学习器叫做“基学习器”。 比如下图三个基学习器通过一定的结合策略，组合成一个强学习器。 弱分类器 就是通常是指的是泛化性能略优于随机猜测的学习器，基学习器有时候也被称为弱学习器，集成后，通常来讲组合的弱学习器有一个很好的提升。 对基学习器的期望 我们希望每个基学习器好而不同，就是性能很好，并且保持多样性。比如下图（a）中的三个基学习器在不同样本间分类正确（打勾表示分类正确，否则错误），集成后子分类器的准确率是66%，但是集成后是100%。效果提升。（b）并没有提升最终效果。（c）效果从33%降到了0，起负作用了。 2.AdaBoost介绍2.1 思想集成学习大致分为2类，一个是个体学习器之间存在强的依赖关系、必须串行生成的序列方法；另一种是个体学习器之间不存在强依赖关系、可同时生成的并行化方法。前者代表是AdaBoost算法，后者代表是Bagging和随即森林。 Boosting是一族可以将弱学习器提升为强学习器的算法。这族算法的工作机制类似：先从初始训练集训练出一个基学习器，使得先前的基学习器做错的训练样本在后面受到更加多的关注，然后基于调整过后的样本分布来训练下一个基学习器；如此往复进行，直到基学习器数目达到事先预定的数量T值，最终我们将这些T个基学习器进行加权结合。 注意2点：AdaBoost有两种权值的调整，一种是针对分类错误的样本，会把分错的样本在下一个基分类器训练前提高权重；另一个是针对基分类ht，对分类错误率较高的基分类器也会降低权重。参考下图：显然这是个串行的方式，每一次对样本有一个权重调整后再放入下一个弱学习器训练，权重是根据分类的误差率来调整的；最后把所有带权重的弱分类器线性总和即为我们集成的强分类器。有点三个臭裨将顶个诸葛亮的意思。 2.2 算法框架 上面是周志华《机器学习》P174页的算法框架，大致和前面说的差不多但是会有疑惑的点是错误率如何来的？还有最后的αt和数据集Dt分布如何更新？针对这3个问题，有了第三小节。 2.3 公式推导参考自机器学习P175页，错误率就是每次样本分错权重的归一化值。对于第6行怎么来的，参考下图：由于是二分类问题，yi的预测值不是1就是-1，所以只分为2种情况，即预测准确的期望值和预测错误的期望值。 对于第7行怎么来的，看下图：这里参考了索引1，点击这里访问。但是他里面有2个错误，一个是红色虚线部分，一个最后求L的最后第2步，杠掉Wm,i部分。图中已更正! 字丑，见谅。 上图是对基分类器权重αm用另一种方式的推导。 2.4 代码参考这里是网上的一个代码，只包含训练模型，没有预测部分，建议先看博客： https://www.cnblogs.com/davidwang456/articles/8927029.html 里面的例子，再看代码更好理解。写的蛮好的。代码也可以在我的github上面克隆或下载： https://github.com/ArmigoPancho/mechine-learning-codes/tree/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0python%E4%BB%A3%E7%A0%81 。 One more thing：热爱生活，并为之砥砺前行。 3.参考资料1.AdaBoost原理详：https://www.cnblogs.com/ScorpioLu/p/8295990.html 2.老刘博客：请点击这里 3.周志华《机器学习》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>AdabBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Word2vec]]></title>
    <url>%2F2019%2F03%2F04%2Fword2vec%2F</url>
    <content type="text"><![CDATA[1.Word2vec向量是什么？1.1独热编码表示（One Hot Representation）Word2vec，由 Google 于 2013 年发表，是一种神经网络实现，可以学习单词的分布式表示。在此之前已经提出了用于学习单词表示的其他深度或循环神经网络架构，但是这些的主要问题是训练模型所需时长间。 Word2vec 相对于其他模型学习得快。Word2Vec 不需要标签来创建有意义的表示。这很有用，因为现实世界中的大多数数据都是未标记的。如果给网络足够的训练数据（数百亿个单词），它会产生特征极好的单词向量。具有相似含义的词出现在簇中，并且簇具有间隔，使得可以使用向量数学来再现诸如类比的一些词关系。着名的例子是，通过训练好的单词向量，“国王 - 男人 + 女人 = 女王”。上面kaggle对word2vec的介绍，看完还是不了解具体它做了什么。理解Wordvec之前先了解One hot representation模式：向量中每一个元素都关联着词库中的一个单词，指定词的向量表示为：其在向量中对应的元素设置为1，其他的元素设置为0。比如下面图所示：只有1处表示当前单词queen，其他单词的位置都是0，one-hot就是这样子表示每一个单词。所以每个单词相互正交，所以也不会有单词之间的联系了。 再如考虑下面的三个特征： [“male”, “female”] [“from Europe”, “from US”, “from Asia”] [“uses Firefox”, “uses Chrome”, “uses Safari”, “uses Internet Explorer”] 将它换成独热编码后，应该是： feature1=[01,10] feature2=[001,010,100] feature3=[0001,0010,0100,1000] 优点：一是解决了分类器不好处理离散数据的问题，二是在一定程度上也起到了扩充特征的作用。 缺点：在文本特征表示上有些缺点就非常突出了。首先，它是一个词袋模型，不考虑词与词之间的顺序（文本中词的顺序信息也是很重要的）；其次，它假设词与词相互独立（在大多数情况下，词与词是相互影响的）；最后，它得到的特征是离散稀疏的。 1.2分布式表示（Distributed Representation）Distributed representation可以解决One hot representation的维度灾难和稀疏问题，它的思路是通过训练，将每个词都映射到一个较短的词向量上来。比如下图我们将词汇表里的词用”Royalty”,”Masculinity”, “Femininity”和”Age”4个维度来表示，King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)(0.99,0.99,0.05,0.7)。当然在实际情况中，我们并不能对词向量的每个维度做一个很好的解释。 有了上面的表示方式，我们就可以用向量来分析词语词之间的关系。比如下面一个有趣的公式： \vec {King} - \vec {Man} + \vec {Woman} = \vec {Queen}怎么训练得到合适的词向量呢？一个很常见的方法是使用神经网络语言模型。 2.神经网络语言模型神经网络语言模型采用的方法一般是一个三层的神经网络结构（当然也可以多层），分为输入层，隐藏层和输出层(softmax层)。这个模型是如何定义数据的输入和输出呢？Word2vec模式中一般分为CBOW(Continuous Bag-of-Words 与Skip-Gram两种模型。CBOW模型根据中心词W(t)的周围词来预测中心词，Skip-gram模型则根据中心词W(t)来预测周围词。两者类似相反的过程。 2.1CBOW模型（Continuous Bag-of-Words ）先看一个网上的例子： 假设我们现在的Corpus是这一个简单的只有四个单词的document：{I drink coffee everyday}我们选coffee作为中心词，window size设为2也就是说，我们要根据”I”,”drink”和”everyday”三个周围词来预测一个中心单词，并且我们希望这个单词是coffee。 （1）首先，初始化周围词的独热码。”I”,”drink”和”everyday”三个周围词的独热码是作为输入层的3个向量（单词向量空间dim为1 x V），目标是预测”coffee”的独热码。上图中的window size：表示当前词与预测词在一个句子中的最大距离是多少，其他参数请参考：https://blog.csdn.net/szlcw1/article/details/52751314 （2）初始化输入权重矩阵W（单词向量空间dim为1 x V，W的维度是V x N，N为自己设定的数），再把所有one hot分别乘以共享的输入权重矩阵W得到向量Vi，图中的hi就是这里的Vi。 （3）然后把所有每个分量Vi相加求平均值得到一个隐层向量V，维度是Nx1，行维度与W的行维度一致。 （4）初始化输出权重W’ （维度N x V），由W’ x V = U可得到我们的中间矩阵U，为后面的输出矩阵做准备。 （5）最后一步就是中间矩阵U输入到softmax分类函数里面去得到最后的输出向量y。其中的每一维斗代表着一个单词，概率最大的index所指示的单词为预测出的中间词（target word）。 （6）预测出的中间词与true label的onehot做比较，误差越小越好。 2.2Skip-Gram模型2.2.1 Skip-Gram介绍以下翻译自CS 224D: Deep Learning for NLP。Skip-Gram Model就是与CBOW相反的情况，根据某个中心词来预测周围c/2个词，就是向左c/2个词，向右c/2个词。总的框架如下图所示： 符号标记：一个中心词$x$；输出向量是$y^j$；V是输入单词矩阵（维度n x V）；$v_i$是$V$的第$i$列，表示单词$w_i$的输入向量；$U$代表输出矩阵（维度n x V）；$u_i$表示$U$的第$i$行，表示单词$w_i$的输出向量。 步骤1：生成输入向量的独热码x 步骤2：我们由上下文$v_c = V x$得到嵌入词单向量 步骤3：由于没有平均值，只需设置$\hat{v}= vc$ 步骤4：用$ u = U v_c=$来计算$u_{c-m}, . . . , u_{c-1}, u_{c+1}, . . . , u_{c+m}$，以此生成2 x m个分数向量 步骤5：用公式$y = softmax(u)$把每一个分数转为概率 步骤6：用我们预测的概率向量$y^{(c-m)}, . . . , y^{(c-1)}, y^{(c+1)}, . . . , y^{(c+m)}$来和真实的独热码进行比较，越接近越好 2.2.2 隐层细节训练模型的真正目的是获得模型基于训练数据学得的隐层权重。为了得到这些权重，我们首先要构建一个完整的神经网络作为我们的“Fake Task”。模型的输入如果为一个10000维的向量，那么输出也是一个10000维度（词汇表的大小）的向量，它包含了10000个概率，每一个概率代表着当前词是输入样本中output word的概率大小。由于输入向量X是one-hot编码，那么只有输入权重向量W中的非零元素才能对隐藏层产生输入。比如下面公式： h = x^TW=W_{k,.}:=v_{wI}\tag{$1$}看不懂的话，看下图，由于左边的独热码矩阵在维度很高时候做相乘的操作时，非常消耗资源，我们这里只要根据独热码不为0的列号取输入矩阵W所对应的行，所以模型中的隐层权重矩阵便成了一个”查找表“（lookup table）。 2.2.3模型优化目标在CBOW中，我们需要设定一个目标函数来估计我们的模型。但是这里不同的是，因为这里假设是用朴素贝叶斯来断开概率之间的联系，就是说，这里假设中心词一旦给定，那所有输出的单词都是相互独立的。故有如下优化目标函数J： 要是得周围词是最佳的词，就是所有预测出来单词的概率是最大，P(Wc-m,…,Wc+m | Wc)是概率在0-1之间，所以底数&gt;1的情况下，log(P(Wc-m,…,Wc+m | Wc))&lt;0，加上负号表示概率越小，损失越大，周围词就越不好。相反概率越大越好。由于所有输出单词都是假设独立的，所以每个单词概率简单的相乘就是总体的概率。第四个等号后面是用softmax函数对概率进行处理了。最后一步拆开来即可。在上面的优化目标下，这里采用随机梯度下降（Stochastic Gradient Descent）来计算未知参数的梯度。 简单的总体讲一下，就是三层网络模型，第一层是输入层，中间一层是隐藏结点，最后一层是输出层。模型需要训练的参数是输入权重和输出权重。建议看论文原文，skip-gram写的不好，没理解透。其实还有针对模型提出的3个训练方法，有兴趣的可以在刘建平的这3篇博客了解。 3.参考1.CBOW：https://www.zhihu.com/question/44832436/answer/266068967 2.CS224D笔记，百度网盘链接：https://pan.baidu.com/s/1UZygZEVFbPO5fcdncJDvow提取码：kxu1 3.刘建平的CBOW与Skip-Gram模型基础：https://www.cnblogs.com/pinard/p/7160330.html) 4.skip介绍： http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/ 5.skip-gram视频，需要翻墙： https://www.youtube.com/watch?v=EHqXB6P1PzA]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[情感分析案例]]></title>
    <url>%2F2019%2F02%2F27%2FSentiment%20Analysis%2F</url>
    <content type="text"><![CDATA[１．数据集介绍本案例是在来自Kaggle网站，所有详细信息均可以参考网址： https://www.kaggle.com/c/word2vec-nlp-tutorial ，本次案例是Bag of Words Meets Bags of Popcorn，我翻译成词带模型爱上爆米花，anyway，数据集有四个文件一个有类标的训练集labeledTrainData如下图所示，就是三列：id、sentiment、review。每条评论是对某电影的评论，只是简单的二分类问题，1表示IMDB评分&gt;=7，若某评论的IMDB评级&lt;5会导致情绪评分为0，这里已经打好了类标记。 testData是一个没有类标的测试集，预测提交到kaggle自然会打分，sampleSubmission是以正确格式的逗号分隔的示例提交文件，还有unlabeledTrainData**是另外50,000个IMDB评论没有提供任何评级标签。 2.TF-IDF和Word2vec在贝叶斯、逻辑回归方法中的比较下面两段代码都是来自kaggle网上和其他参考资料，已经过不动脑子的誊写 :）+运行，把数据放在一个py文件同目录下即可： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110import reimport pandas as pdfrom bs4 import BeautifulSoupimport numpy as npfrom sklearn.model_selection import cross_val_predictfrom sklearn.feature_extraction.text import TfidfVectorizer as TFIDFfrom sklearn.naive_bayes import MultinomialNB as MNBfrom sklearn.linear_model import LogisticRegression as LRfrom sklearn.grid_search import GridSearchCVfrom sklearn.model_selection import cross_val_score# 参考：http://blog.csdn.net/longxinchen_ml/article/details/50629613def review_to_wordlist(review): ''' 把IMDB的评论转成词序列 参考：http://blog.csdn.net/longxinchen_ml/article/details/50629613 ''' # 去掉HTML标签，拿到内容 review_text = BeautifulSoup(review, "html.parser").get_text() # 用正则表达式取出符合规范的部分 review_text = re.sub("[^a-zA-Z]", " ", review_text) # 小写化所有的词，并转成词list words = review_text.lower().split() # 返回words return wordsroot_dir = "../word2vec-nlp-tutorial"train = pd.read_csv('%s/%s' % (root_dir, 'labeledTrainData.tsv'), header=0, delimiter="\t", quoting=3)test = pd.read_csv('%s/%s' % (root_dir, 'testData.tsv'), header=0, delimiter="\t", quoting=3)# 预处理数据label = train['sentiment']train_data = []for i in range(len(train['review'])): train_data.append(' '.join(review_to_wordlist(train['review'][i])))test_data = []for i in range(len(test['review'])): test_data.append(' '.join(review_to_wordlist(test['review'][i])))"""特征处理直接丢给计算机这些词文本，计算机是无法计算的，因此我们需要把文本转换为向量，有几种常见的文本向量处理方法，比如：1)单词计数2)TF-IDF向量3)Word2vec向量我们先使用TF-IDF来试一下min_df: 最小支持度为2（词汇出现的最小次数）max_features: 默认为None，可设为int，对所有关键词的term frequency进行降序排序，只取前max_features个作为关键词集strip_accents: 将使用ascii或unicode编码在预处理步骤去除raw document中的重音符号analyzer: 设置返回类型token_pattern: 表示token的正则表达式，需要设置analyzer == 'word'，默认的正则表达式选择2个及以上的字母或数字作为token，标点符号默认当作token分隔符，而不会被当作tokenngram_range: 词组切分的长度范围use_idf: 启用逆文档频率重新加权use_idf：默认为True，权值是tf*idf，如果设为False，将不使用idf，就是只使用tf，相当于CountVectorizer了。smooth_idf: idf平滑参数，默认为True，idf=ln((文档总数+1)/(包含该词的文档数+1))+1，如果设为False，idf=ln(文档总数/包含该词的文档数)+1sublinear_tf: 默认为False，如果设为True，则替换tf为1 + log(tf)stop_words: 设置停用词，设为english将使用内置的英语停用词，设为一个list可自定义停用词，设为None不使用停用词，设为None且max_df∈[0.7, 1.0)将自动根据当前的语料库建立停用词表"""tfidf = TFIDF(min_df=2, max_features=None, strip_accents='unicode', analyzer='word', token_pattern=r'\w&#123;1,&#125;', ngram_range=(1, 3), # 二元文法模型 use_idf=1, smooth_idf=1, sublinear_tf=1, stop_words = 'english') # 去掉英文停用词# 合并训练和测试集以便进行TFIDF向量化操作data_all = train_data + test_datalen_train = len(train_data)tfidf.fit(data_all)data_all = tfidf.transform(data_all)# 恢复成训练集和测试集部分train_x = data_all[:len_train]test_x = data_all[len_train:]print('TF-IDF处理结束.')print("train: \n", np.shape(train_x[0]))print("test: \n", np.shape(test_x[0]))"""1.朴素贝叶斯训练模型"""model_NB = MNB()MNB(alpha = 1.0,class_prior=None,fit_prior=True)model_NB.fit(train_x,label)print("多项式贝叶斯模型10折交叉验证得分：",np.mean( cross_val_score(model_NB, train_x, label, cv = 10, scoring ='roc_auc')))"""2.使用逻辑回归模型来测试一遍"""#设定网格搜索参数grid_values = &#123;'C':[30]&#125;#设定打分为roc_aucmodel_LR = GridSearchCV(LR(penalty='l2',dual=True,random_state =0),grid_values,scoring = 'roc_auc',cv=2)#放入数据model_LR.fit(train_x,label)print("训练结果：",model_LR.grid_scores_, '\n', model_LR.best_params_, model_LR.best_score_)#预测结果test_results = np.array(model_LR.predict(test_x))print("预测结果是")submission_df = pd.DataFrame(data=&#123;'id':test['id'],'sentiment':test_results&#125;)print(submission_df.head(10))submission_df.to_csv('../submission_br.csv',columns = ['id','sentiment'], index = False) 下面是Word2vec在GNB中的运用： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140"""3.高斯贝叶斯+Word2vec训练"""import gensimimport nltkimport reimport numpy as npimport pandas as pdfrom nltk.corpus import stopwordsfrom bs4 import BeautifulSoupimport timefrom gensim.models import Word2Vecfrom sklearn.naive_bayes import GaussianNB as GNBfrom sklearn.cross_validation import cross_val_scoreimport warningswarnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')import gensim#文本预处理root_dir = "../word2vec-nlp-tutorial"train = pd.read_csv('%s/%s' % (root_dir, 'labeledTrainData.tsv'), header=0, delimiter="\t", quoting=3)test = pd.read_csv('%s/%s' % (root_dir, 'testData.tsv'), header=0, delimiter="\t", quoting=3)tokenizer = nltk.data.load('file:D:/nltk_data/tokenizers/punkt/english.pickle') #若没有file：开头，误以为是http协议开头，自然找不到或格式错误了 def review_to_wordlist(review, remove_stopwords=False): # review = BeautifulSoup(review, "html.parser").get_text() review_text = re.sub("[^a-zA-Z]"," ", review) words = review_text.lower().split() if remove_stopwords: stops = set(stopwords.words("english")) words = [w for w in words if not w in stops] return(words)def review_to_sentences( review, tokenizer, remove_stopwords=False ): ''' 将评论段落转换为句子，返回句子列表，每个句子由一堆词组成 ''' raw_sentences = tokenizer.tokenize(review.strip()) sentences = [] for raw_sentence in raw_sentences: if len(raw_sentence) &gt; 0: # 获取句子中的词列表 sentences.append( review_to_wordlist( raw_sentence, remove_stopwords )) return sentences# 预处理数据label = train['sentiment']train_data = []for i in range(len(train['review'])): train_data.append(' '.join(review_to_wordlist(train['review'][i])))test_data = []for i in range(len(test['review'])): test_data.append(' '.join(review_to_wordlist(test['review'][i])))sentences = []for i, review in enumerate(train["review"]): # print(i, review) sentences += review_to_sentences(review, tokenizer, True)print(np.shape(train["review"]))print(np.shape(sentences)) # 模型参数num_features = 300 # Word vector dimensionality min_word_count = 40 # 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5 num_workers = 4 # Number of threads to run in parallelcontext = 10 # Context window size downsampling = 1e-3 # Downsample setting for frequent words# 训练模型#word2vec参数解释： https://blog.csdn.net/laobai1015/article/details/86540813print("训练模型中...")model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count=min_word_count, window=context, sample=downsampling)print("训练完成")print('保存模型...')model.init_sims(replace=True)model_name = "%s/%s" % (root_dir, "300features_40minwords_10context.txt")model.save(model_name)print('保存结束')def makeFeatureVec(words, model, num_features): ''' 对段落中的所有词向量进行取平均操作 ''' featureVec = np.zeros((num_features,), dtype="float32") nwords = 0. # Index2word包含了词表中的所有词，为了检索速度，保存到set中 index2word_set = set(model.wv.index2word) for word in words: if word in index2word_set: nwords = nwords + 1. featureVec = np.add(featureVec, model[word]) # 取平均 featureVec = np.divide(featureVec, nwords) return featureVecdef getAvgFeatureVecs(reviews, model, num_features): ''' 给定一个文本列表，每个文本由一个词列表组成，返回每个文本的词向量平均值 ''' counter = 0 reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype="float32") for review in reviews: if counter % 5000 == 0: print("Review %d of %d" % (counter, len(reviews))) reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features) counter = counter + 1 return reviewFeatureVecstrainDataVecs = getAvgFeatureVecs(train_data, model, num_features)print(np.shape(trainDataVecs))testDataVecs = getAvgFeatureVecs(test_data, model, num_features)print(np.shape(testDataVecs))model_GNB = GNB()model_GNB.fit(trainDataVecs,label)print("高斯贝叶斯分类器10折交叉验证得分: ", np.mean(cross_val_score(model_GNB, trainDataVecs, label, cv=10, scoring='roc_auc')))print("保存结果。。。")result = model_GNB.predict(testDataVecs)submission_df = pd.DataFrame(data=&#123;'id':test['id'],'sentiment':result&#125;)print(submission_df.head(10))submission_df.to_csv('C:/Users/asus1/Desktop/gnb_word2vec.csv',columns = ['id','sentiment'], index = False)]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP，情感分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《亡徵》]]></title>
    <url>%2F2018%2F12%2F24%2Fperdition%2F</url>
    <content type="text"><![CDATA[1.《韩非子》战国 (公元前475年 - 公元前221年)时期，百家争鸣，百花齐放，实在是难得。韩非（约公元前280年—公元前233年），战国时期韩国都城新郑（今河南省郑州市新郑市）人 [1] ，法家代表人物，杰出的思想家、哲学家和散文家。韩王之子，荀子学生，李斯同门师兄。下面是《四部丛刊初编》中第350～352册。景上海涵芬楼藏景宋钞校本，本书二十卷，完整版见此链接： https://ctext.org/library.pl?if=gb&amp;file=77707&amp;page=4&amp;remap=gb 。 2.《亡徵》原文​ 凡人主之国小而家大，权轻而臣重者，可亡也。简法禁而务谋虑，荒封内而恃交援者，可亡也。群臣为学，门子好辩，商贾外积，小民右仗者，可亡也。好宫室台榭陂池，事车服器玩好，罢露百姓，煎靡货财者，可亡也。用时日，事鬼神，信卜筮，而好祭祀者，可亡也。听以爵不待参验，用一人为门户者，可亡也。官职可以重求，爵禄可以货得者，可亡也。缓心而无成，柔茹而寡断，好恶无决，而无所定立者，可亡也。饕贪而无餍，近利而好得者，可亡也。喜淫而不周于法，好辩说而不求其用，滥于文丽而不顾其功者，可亡也，浅薄而易见，漏泄而无藏，不能周密，而通群臣之语者，可亡也。很刚而不和，愎谏而好胜，不顾社稷而轻为自信者，可亡也。恃交援而简近邻，怙强大之救，而侮所迫之国者，可亡也。羁旅侨士，重帑在外，上闲谋计，下与民事者，可亡也。民信其相，下不能其上，主爱信之而弗能废者，可亡也。境内之杰不事，而求封外之士，不以功伐课试，而好以名问举错，羁旅起贵以陵故常者，可亡也。轻其适正，庶子称衡，太子未定而主即世者，可亡也。大心而无悔，国乱而自多，不料境内之资而易其邻敌者，可亡也。国小而不处卑，力少而不畏强，无礼而侮大邻，贪愎而拙交者，可亡也。太子已置，而娶于强敌以为后妻，则太子危，如是，则群臣易虑，群臣易虑者，可亡也。怯慑而弱守，蚤见而心柔懦，知有谓可，断而弗敢行者，可亡也。出君在外而国更置，质太子未反而君易子，如是则国携，国携者，可亡也，挫辱大臣而狎其身，刑戮小民而逆其使，怀怒思耻而专习则贼生，贼生者，可亡也。大臣两重，父兄众强，内党外援以争事势者，可亡也。婢妾之言听，爱玩之智用，外内悲惋而数行不法者，可亡也。简侮大臣，无礼父兄，劳苦百姓，杀戮不辜者，可亡也。好以智矫法，时以行集公，法禁变易，号令数下者，可亡也。无地固，城郭恶，无畜积，财物寡，无守战之备而轻攻伐者，可亡也。种类不寿，主数即世，婴儿为君，大臣专制，树羁旅以为党，数割地以待交者，可亡也。太子尊显，徒属众强，多大国之交，而威势蚤具者，可亡也。变褊（biǎn，狭隘）而心急，轻疾而易动发，心悁忿（yuān fèn，怨怒）而不訾（zī，估量、思考）前后者，可亡也。主多怒而好用兵，简本教而轻战攻者，可亡也。贵臣相妒，大臣隆盛，外藉敌国，内困百姓，以攻怨雠，而人主弗诛者，可亡也。君不肖而侧室贤，太子轻而庶子伉，官吏弱而人民桀，如此则国躁，国躁者，可亡也。藏怒而弗发，悬罪而弗诛，使群臣阴憎而愈忧惧，而久未可知者，可亡也。出军命将太重，边地任守太尊，专制擅命，径为而无所请者，可亡也。后妻淫乱，主母畜秽，外内混通，男女无别，是谓两主，两主者，可亡也。后妻贱而婢妾贵，太子卑而庶子尊，相室轻而典谒重，如此则内外乖，内外乖者，可亡也。大臣甚贵，偏党众强，壅塞主断而重擅国者，可亡也。私门之官用，马府之世，乡曲之善举，官职之劳废，贵私行而贱公功者，可亡也。公家虚而大臣实，正户贫而寄寓富，耕战之士困，末作之民利者，可亡也。见大利而不趋，闻祸端而不备，浅薄于争守之事，而务以仁义自饰者，可亡也。不为人主之孝，而慕匹夫之孝，不顾社稷之利，而听主母之令，女子用国，刑馀用事者，可亡也。辞辩而不法，心智而无术，主多能而不以法度从事者，可亡也。亲臣进而故人退，不肖用事而贤良伏，无功贵而劳苦贱，如是则下怨，下怨者，可亡也。父兄大臣禄秩过功，章服侵等，宫室供养太侈，而人主弗禁，则臣心无穷，臣心无穷者，可亡也。公婿公孙与民同门，暴傲其邻者，可亡也。亡徵者，非曰必亡，言其可亡也。夫两尧不能相王，两桀不能相亡，亡王之机，必其治乱、其强弱相踦者也。木之折也必通蠹，墙之坏也必通隙。然木虽蠹，无疾风不折；墙虽隙，无大雨不坏。万乘之主，有能服术行法以为亡徵之君风雨者，其兼天下不难矣。 3.《亡徵》译文​ 凡属君主国家弱小而臣下强大的，君主权轻而臣下权重的，可能灭亡。轻视法令而好用计谋，荒废内政而依赖外援的，可能灭亡。群臣喜欢私学，贵族子弟喜欢辩术，商人在外囤积财富，百姓崇尚私斗的，可能灭亡。嗜好宫殿楼阁池塘，爱好车马服饰玩物，喜欢让百姓疲劳困顿，压榨挥霍钱财的，可能灭亡。选吉日，信鬼神，迷信占卜而讲究祭祀的，国家就可能灭亡。听凭有爵位的人的意见而不考核验证，只听一个人的话而且由他去办的，国家就可能灭亡。官职可以依靠重臣求得，爵禄可以用财宝换取的，国家就可能灭亡。办事拖拉而无成效，优柔寡断，好坏不分又无主见、行动不定的，国家就可能灭亡。贪得无厌，唯利是图的，国家就可能灭亡。喜欢玩弄词藻但不合乎法规，讲究巧辩而不求实用，沉溺于华美的文采而不顾它的功效的，国家就可能灭亡。君主不学无术而且轻易表露情态，泄露机密而不知隐藏，不能严密防范而又把臣下的进言透露出去的，国家就可能灭亡。凶狠暴虐而不平和，不听别人的忠谏而逞强好胜，不顾国家安危而轻率自信的，国家就可能灭亡。仰仗外国的支援而怠慢近邻，依靠强国的救援而侮辱邻国的，国家就可能灭亡。寄居国外的游客，把大批财宝存放在国外，对上刺探国家机密，对下干预民事的，国家就可能灭亡。百姓相信相国，臣下轻视君主，君主宠爱相国，相信他又不能废黜的，国家就可能灭亡。国内的杰出人才闲置不用，反而去寻求国外的人，不按功劳大小去考核，而喜欢用虚名，把寄寓作客的人破格起用而凌驾故旧的，国家就可能灭亡。轻视嫡子而庶出的掌权，太子还没有确定而君主过世的，国家就可能灭亡。君主狂妄自大而不悔悟，国家混乱而自以为美，不估量本国的实力而看轻邻近敌国的，国家就可能灭亡。国家弱小而又不卑躬谦下，国力不足而又不服强国，没有礼貌而又侮辱强大的邻国，贪婪任性而不善于外交的，国家就可能灭亡。已经有了太子，而又从强大的敌国娶来正妻，那太子就有危险了，这样群臣也会变心；群臣变心的，国家就可能灭亡。胆小怕事而又不敢坚持己见，问题早已发现而因内心懦弱，也知道可以解决，决定了却又不敢执行的，国家就可能灭亡。出国的君主还在国外而国内已另立新君，在国外做人质的太子还没有回国而君主又另立太子，这样群臣就会有二心，群臣有二心的，国家就可能灭亡。打击侮辱大臣而又戏弄他们，杀戮小民而又一反常规地役使他们，小民心怀怨恨牢记耻辱，而君主又专心一意地宠幸而又戏辱他们，于是劫杀的事就会发生；发生劫杀之事的，国家就可能灭亡。两个大臣同时当权，君主的叔伯兄弟又多又强，国内结党谋取外援而争夺权势的，国家就可能灭亡。 ​ 只听侍从妃妾的话，只听宠幸近臣的计策，朝野内外悲愁怨恨而又屡行不法的，国家就可能灭亡。怠慢侮辱大臣，对叔伯兄弟又没有礼貌，使百姓劳乏困苦，杀戮无辜的，国家就可能灭亡。喜欢用自己的小聪明来改变法规，时常把自己的私利混杂在公务之中，法制、禁令常常改动，号令下达无数的，国家就可能灭亡。没有险要的地势可守，城墙不坚固，无粮食储备，财力物力贫乏，没有防守和攻战的准备，又轻举妄动去进攻的，国家就可能灭亡。君主的族人寿命不长，君主又接连死去，婴儿做了君主，奸臣专权，拉拢国外的游勇结为私党，一再割地以求得敌国青睐的，国家就可能灭亡。太子受到尊重而名声显赫，侍从、属下及其党羽众多而且强盛，又与许多大国友好交往，而且很早就有威势的，国家就可能灭亡。心情偏邪狭小而又急躁，轻浮而又极易冲动，当他愤怒时从不慎重思考的，国家就可能灭亡。君主多动怒而好用兵打仗，放松农业生产和平时练兵的，国家就可能灭亡。贵臣互相妒嫉，重臣权势又大，在外借重敌国的势力，对内压榨百姓，用以攻击和自己有怨仇的人，而君主不加诛戮的，国家就可能灭亡。君主不贤而庶出贤，太子轻薄而庶子高尚，官吏懦弱而人民豪横，这样的国家就会动荡不安；动荡不安的国家就可能灭亡。心藏怨恨不敢发作，对罪臣迟迟不予惩办，使群臣在暗中怀恨而更加忧虑畏惧，过了很长时间也不知会有什么结果的，国家就可能灭亡。发兵任命的将领权势太重，驻守边疆的官员地位太高，他们独断专行，直接处理问题而不向君主请示的，国家就可能灭亡。太后淫乱，主母养奸，内外不分，男女无别，这就形成后党一方和君主一方的两种势力、两个主子；有了两个主子的，国家就可能灭亡。王后卑微而婢妾尊贵，太子卑微而庶子尊贵，丞相权轻而掌宾客的官吏权重，这样就会里外不分而轻重颠倒；里外不分、轻重颠倒的，国家就可能灭亡。大臣异常显贵，私党人多势众，阻闭君主的视听，阻碍君主决断而独揽大权的，国家就可能灭亡。豪门贵族私人的属下可以被任用，立过军功的后代可以被排挤，偏僻乡村里的有善名的被举荐，在职官吏的功劳被埋没，重视谋私利的人而看不起为国立功的，国家就可能灭亡。国家的府库空虚而大臣却很富厚，本地居民贫苦而客居之人却很富足，耕作出征之家困乏而工商之家得利的，国家就可能灭亡。看到国家的利益而不赶紧去办，听到有祸乱的苗头而不去及早设防，征战守备之事流于轻浮，只是想着用仁义装扮自己的，国家就可能灭亡。不是想着祖先的社稷，只是羡慕小民对父母的孝敬，不顾国家的利益，而听从王后的意旨，妇女掌政，宦官弄权的，国家就可能灭亡。能言善辩而不合乎法度，聪明伶俐而没有法术，君主多才多艺却不按法度行事的，国家就可能灭亡。新任之臣晋升，原有之臣被辞退，无德无才的人管事，有德有才的靠边，没有功劳的人地位显贵，劳苦为国的人地位卑贱，这样臣下就会怨恨；臣下怨恨的，国家就可能灭亡。君主的叔伯兄弟以及大臣的俸禄品位高过他们的功劳，服饰高过他们的等级，宫室华丽以及供养、消费太奢侈，而君主并不禁止，臣下的贪求就无止境；臣下贪求无止境的，国家就可能灭亡。驸马或公子王孙与百姓同住在一条小巷里，对邻里强横残暴的，国家就可能灭亡。 ​ 说它有灭亡的征兆，并不是说它必然就会灭亡，只是说它有灭亡的可能。两个唐尧不可能相互统一天下，两个夏桀不可能相互灭亡；灭亡与统一天下的机遇，就要看国家的治与乱、强与弱的两端哪一头重了。大树的折断，必定是由于蛀虫蛀蚀的结果；大墙的倒塌，必定是由于缝隙裂开的缘故。然而大树虽说被蛀蚀，没有大风也不会折断；大墙虽然有了裂缝，没有大雨也不会倒塌。万乘大国君主，如果有推行法术的人协助，一定会像暴风骤雨那样，很容易就能摧毁有灭亡的征兆的国家，而兼并天下也就不是什么困难的事了! ​ 一共48种”可亡也”的征兆，古人思想之深远，志虑之广袤，非常人所比。]]></content>
      <categories>
        <category>火花</category>
      </categories>
      <tags>
        <tag>亡徵，韩非子</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法代码]]></title>
    <url>%2F2018%2F12%2F13%2FAlgorithm%2F</url>
    <content type="text"><![CDATA[1.冒泡排序对一些常见的排序刷一下,可以参考 https://blog.csdn.net/sgn132/article/details/47279511 ，https://blog.csdn.net/wfq784967698/article/details/79551476 等博客。冒泡排序就是每一趟都通过两两比较相邻的值，逆序就调整顺序，遍历整个list找到最大值放到最后一个，然后list长度减1；再次遍历直到最后一个值是最大值，如此循环。计算复杂度最好时为$O（n）$,最差是$O（n^2）$，故计算复杂度为$O(n^2)​$。 123456789101112# 冒泡排序def bubble_sort(l): # 外层循环 len(l)遍，内层循环少一遍 for i in range(len(l)-1): for j in range(len(l) - 1): # 找出最大值，然后交换位置到最后 if l[j] &gt; l[j+1]: l[j], l[j+ 1] = l[j+1], l[j]if __name__ == "__main__": l = [5, 1, 91, 3, 2, 7] bubble_sort(l) print(l)]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>冒泡排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[D3.JS]]></title>
    <url>%2F2018%2F12%2F08%2FD3.JS%2F</url>
    <content type="text"><![CDATA[1.可缩放矢量图形（SVG） SVG（Scalable Vector Graphics） 可被非常多的工具读取和修改（比如记事本）,由于使用xml格式定义，所以可以直接被当作文本文件打开，看里面的数据 SVG 与 JPEG 和 GIF 图像比起来，尺寸更小，且可压缩性更强，SVG 图就相当于保存了关键的数据点，比如要显示一个圆，需要知道圆心和半径，那么SVG 就只保存圆心坐标和半径数据，而平常我们用的位图都是以像素点的形式根据图片大小保存对应个数的像素点，因而SVG尺寸更小 SVG 是可伸缩的，平常使用的位图拉伸会发虚，压缩会变形，而SVG格式图片保存数据进行运算展示，不管多大多少，可以不失真显示。作图例子如下： （1）平面上一个圈圈：包涵圈和圈内、圈外。发现这个可缩放的矢量图是在SVG模块里面再添加一个circle模块的：’’svgCircleTutorial’’是svg的id名字、xmlns是XML命名空间，区别开在不同文件中的相同的标志、height=”250”是SVG的高度；cx=”55” cy=”55”表示坐标（55,55）、r是半径、fill是填充圈内颜色、stroke是轮廓的颜色、stroke-width是轮廓宽度。把下面代码保存到txt文件，修改文件名字为circle.html 用任意浏览器打开即可显示下图。 123&lt;svg id="svgCircleTutorial" height="250" xmlns="http://www.w3.org/2000/svg"&gt; &lt;circle id="myCircle" cx="100" cy="100" r="60" fill="#219E3E" stroke="#17301D" stroke-width="10" /&gt;&lt;/svg&gt; （2）rect 元素的 width 和 height 属性可定义矩形的高度和宽度、style属性用来定义CSS属性、CSS 的 fill 属性定义矩形的填充颜色（rgb 值、颜色名或者十六进制值）、CSS 的 stroke-width 属性定义矩形边框的宽度、CSS 的 stroke 属性定义矩形边框的颜色，如下： 1234&lt;svg&gt; &lt;rect width="150" height="150" style="fill:rgb(255,255,0);stroke-width:12;stroke:rgb(0,150,0)"/&gt;&lt;/svg&gt; （3）路径：设置线段坐标和图像填充，fill为white时在背景也是白色的情况下只有楼梯线段了。 123456&lt;svg&gt; &lt;polyline points="0 40,40 40,40 80,80 80,80 120,120 120,120 160" style="fill:blue; stroke:gray; stroke-width:3" /&gt;&lt;/svg&gt; 2.HTML例子由于html也是前端的核心，暂时了解下，直接例子理解可能好些。 （1）title和内容的位置不同： 123456789101112&lt;html&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;我是标题，我在title标签内，我显示在浏览器窗口最顶部&lt;/title&gt;&lt;/head&gt;&lt;body&gt;我是内容，后面是我的博客地址：&lt;a href="http://www.armigo.fun"&gt;这是一个链接&lt;/a&gt;下面是一张图片：&lt;br&gt;&lt;img src="http://pic.qiantucdn.com/58pic/18/06/80/21N58PICBfS_1024.jpg" width="787.2" height="492" /&gt;&lt;/body&gt;&lt;/html&gt; （2）绘制表格： 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt;&lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;每个表格从一个 table 标签开始。每个表格行从 tr 标签开始。每个表格的数据从 td 标签开始。&lt;/p&gt;&lt;h4&gt;一列:&lt;/h4&gt;&lt;table border="2"&gt;&lt;tr&gt; &lt;td&gt;100&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;h4&gt;一行三列:&lt;/h4&gt;&lt;table border="1"&gt;&lt;tr&gt; &lt;td&gt;100&lt;/td&gt; &lt;td&gt;200&lt;/td&gt; &lt;td&gt;300&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;h4&gt;2行3列&lt;/h4&gt;&lt;table border="1"&gt;&lt;tr&gt; &lt;td&gt;11&lt;/td&gt; &lt;td&gt;22&lt;/td&gt; &lt;td&gt;33&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td&gt;14&lt;/td&gt; &lt;td&gt;25&lt;/td&gt; &lt;td&gt;36&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt; （3）页面布局： 12345678910111213141516171819202122&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;title&gt;Armigo&lt;/title&gt; &lt;/head&gt;&lt;body&gt;&lt;div id="container" style="width:500px"&gt;&lt;div id="header" style="background-color:#FFA500;"&gt;&lt;h1 style="margin-bottom:0;text-align:center"&gt;网页标题:(*´▽｀)ノノ&lt;/h1&gt;&lt;/div&gt;&lt;div id="menu" style="background-color:#FFD700;height:200px;width:100px;float:right;"&gt;&lt;b&gt;菜单&lt;/b&gt;&lt;br&gt;HTML&lt;br&gt;SVG&lt;br&gt;JavaScript&lt;/div&gt;&lt;div id="content" style="background-color:#EEEEEE;height:200px;width:400px;float:left;"&gt;内容：你好帅！&lt;/div&gt;&lt;div id="footer" style="background-color:#FFA500;clear:both;text-align:center;"&gt;版权 © ARMIGO.FUN&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3.CSS样式 CSS 指层叠样式表 (Cascading Style Sheets) 样式定义如何显示 HTML 元素 样式通常存储在样式表中 把样式添加到 HTML 4.0 中，是为了解决内容与表现分离的问题 外部样式表可以极大提高工作效率 外部样式表通常存储在 CSS 文件中 多个样式定义可层叠为一 4.D3.JSD3 的全称是（Data-Driven Documents），顾名思义可以知道是一个被数据驱动的文档。说简单一点，其实就是一个 JavaScript 的函数库，使用它主要是用来做数据可视化的。 （1）文字展示方式： 12345678910111213141516&lt;body&gt;&lt;p1&gt;Hello World 1&lt;/p1&gt;&lt;br&gt;&lt;p2&gt;Hello World 2&lt;/p2&gt;&lt;script src=&quot;http://d3js.org/d3.v3.min.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;script&gt;d3.select(&apos;body&apos;).selectAll(&apos;p&apos;);//选择&lt;body&gt;中所有的&lt;p&gt;，其文本内容为practise，选择集保存在变量 p 中var p=d3.select(&apos;body&apos;) .select(&apos;p1&apos;) .text(&apos;practise&apos;);//修改段落颜色和字体大小p.style(&quot;color&quot;,&quot;red&quot;) .style(&quot;font-size&quot;,&quot;36px&quot;)&lt;/script&gt;&lt;/body&gt;]]></content>
      <categories>
        <category>可视化工具</category>
      </categories>
      <tags>
        <tag>D3.JS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯模型]]></title>
    <url>%2F2018%2F12%2F03%2FNB%2F</url>
    <content type="text"><![CDATA[1.朴素贝叶斯公式1.1概念简述简介下三个重要的条件概率的公式： \begin{align*} &(1)条件概率公式: P(A|B)=P(AB)/P(B)\ ，其中P(B)>0 \\ &(2)乘法公式：P(A∩B)=P(AB)=P(A|B)P(B)=P(B|A)P(A)\\ &\quad\ 推广：P(A1A2...An)=P(A1)P(A2|A1)P(A3|A1A2)...P(An|A1...An-1)\\ &(3)全概率公式：P(A)=\sum_{i=1}^{n}P(A|B_{i})P(B_{i}) \end{align*} （1）条件概率公式设A,B是两个事件，且P(B)&gt;0,则在事件B发生的条件下，计算事件A发生的条件概率。在条件概率中，最本质的变化是样本空间缩小了——由原来的整个样本空间缩小到了给定条件的样本空间。 （2）乘法公式表示A、B同时发生的概率可以看作两个概率的乘积。推广时候，就是多个事件交集的概率可以拆成单个事件Ai的条件概率的乘积形式。 （3）全概率公式就是下图中把整个样本空间S给划分为n块B区域，其中有一块彩色A事件，那么A事件可由每个事件Bi条件下A的部分概率求和而成。 1.2贝叶斯公式（Naive Bayes）废话不多说，先上朴素贝叶斯公式： \begin{align*} P(c_i|x) &= \frac{P(x|c_i)P(c_i)}{P(c_i)}\\ &= \frac{P(x|c_i)P(c_i)}{\sum_i P(x|c_i)P(c_i)} \end{align*}其中，第一个没有代入全概率公式。$P(c_i)$是先验概率（Prior probability），就是在条件x发生之前对类c有一个经验上的估计值，这个估计值是主观的、人为的，但是也是依据一定实际观察或者经验所得。由于主观的猜测值没有令人信服的证明，所以被频率学派所疑虑。比如我们可以猜想对抛1次硬币得到正面的概率为0.5，因为不是正面就是反面（假设没有竖立情况），这就是根据的经验值，因为你并没有去做大量（无穷）的实验来证明。 $P(c_i|x)$是后验概率（Posterior probability）。比如有十个不透明盒子，只有一个盒子里有球，则猜中有球的概率为1/10。当事件 x：打开一个盒子发现没有球，发生后，此时若再打开一个盒子有球的概率就变成了1/9。后验概率是建立在先验概率的基础上，通过贝叶斯公式建立起来的。它是一种“知果求因”的思路，这里打开后的“结果”发现没有球，此时才知道“原因”的概率是1/9。 $P(x|c_i)$是似然函数（Likelyhood），在机器学习中更多的时候它表示了第i个类C中出现属性x的概率，我们希望是使得类c中出现属性x的概率越大越好，因为反过来$P(c_i|x)$概率也越大。统计学中似然函数用符号$f(x|\theta)$，其中θ是需要估计的参数θ(事先假设已经确定有唯一θ)，x是随机变量x，是假设参数θ已知后我们观察到的样本应该是什么样子的，即在此参数θ下样本似然真实样本的程度，旨在最大化下面的似然函数以求需要估计的参数θ： L(\theta) = \prod_{i=1}^{n} f(x_i|\theta)上面就是大名鼎鼎的最大似然估计（MLE），是用来估计参数的方法，还有一种叫矩估计。最大似然估计和矩估计都是频率学派的经典方法。最大似然估计就是每个变量对应似然函数$f(x_i|\theta)$的乘积结果最大化即可停止，可获估计的参数θ。关于参数估计深入一点，有一个置信区间和可信区间区别，假设我们以一批样本可能服从高斯分布，通过多个样本X估计得到参数μ、σ后，置信区间表示有90%的置信程度是当前样本下的高斯分布是以这组参数μ、σ而构造，这里的置信区间只与样本相关，与参数θ无关，频率学派 认为模型的参数是固定的,不会随着样本的变化而变化，只是当前样本产生的某个参数可信度或大或小，抽样越多，频率越接近于真实概率，估计得到的参数越可信，即假定参数是唯一的，只是我们不知道，通过不断的抽样去估计这个参数罢了。而可信区间表示我们有很多个这样的参数，这些参数组成一个可信区间，从中抽取一个概率大一点的使用，比如90%下的参数。置信区间只有一组或一个参数，而可信区间有多组或多个参数。再提一下贝叶斯学派 观点：贝叶斯学派则认为参数不是唯一的，是一个随机变量，服从一个先验分布，而样本是固定的，通过先验分布、后验分布来获取参数。 分母部分$\sum_i{P(x|c_i)P(c_i)}$叫做”证据因子“（Evidence )，以保证各类别的后验概率总和为1从而满足概率条件，表示各个类别的出现属性x的概率总和。发现贝叶斯公式后验概率与分子部分成正比： P(c_i|x)∝P(x|c_i)P(c_i)1.2贝叶斯例子例子1： 症状 职业 疾病 打喷嚏 护士 感冒 打喷嚏 农夫 过敏 头疼 建筑工人 脑震荡 头疼 建筑工人 感冒 打喷嚏 教师 感冒 头疼 教师 脑震荡 某个医院早上收了六个门诊病人，如上表，现在又来了第七个病人，是一个打喷嚏的建筑工人。请问他患上感冒的概率有多大？ 根据公式$P(c_i|x) = \frac{P(x|c_i)P(c_i)}{P(c_i)}$ 可知： P(感冒|打喷嚏，建筑工人)=\frac{P(打喷嚏，建筑工人|感冒)\ast P(感冒)}{P(打喷嚏，建筑工人)}假设打喷嚏和建筑工人是相互独立的，则公式变为： P(感冒|打喷嚏，建筑工人)=\frac{P(打喷嚏|感冒)\ast P(建筑工人|感冒)\ast P(感冒)}{P(打喷嚏)\ast P(建筑工人)}其中：$P(感冒)=\frac{1}{2}$，$P(打喷嚏|感冒)=\frac{2}{3}$，$P(建筑工人|感冒)=\frac{1}{3}$，$P(打喷嚏) \ast P(建筑工人)=\frac{1}{2}\ast\frac{1}{3}=\frac{1}{6}$ ，代入上式子即可得： P(感冒|打喷嚏，建筑工人)=\frac{\frac{2}{3}*\frac{1}{3}*{\frac{1}{2}}}{\frac{1}{6}}=\frac{2}{3}\approx0.666说明此打喷嚏的建筑工人患感冒的概率为60.6%，依次计算打喷嚏、建筑工人的条件下患过敏、脑震荡的概率，然后有三个类别的后验概率，取最大的值作为该情况的预测值即可，这种模型常用于文本分类等。 例子2： 一种癌症，得了这个癌症的人被检测出为阳性的几率为90%，未得这种癌症的人被检测出阴性的几率为90%，而人群中得这种癌症的几率为1%，一个人被检测出阳性，问这个人得癌症的几率为多少？猛地一看，被检查出阳性，而且得癌症的话阳性的概率是90%，概率蛮大的，算下来看看。 我们用$A$表示事件 “测出为阳性”, 用$B_1$表示“得癌症”, $B_2$表示“未得癌症”。根据题目，我们知道如下信息: P(A|B_{1}) = 0.9, P(A|B_{2}) = 0.1, P(B_{1}) = 0.01, P(B_{2}) = 0.99那么我们现在想得到的是已知为阳性的情况下，得癌症的几率$P(B_{1},A)：$ P(B_{1},A) = P(B_{1}) \cdot P(A|B_{1}) = 0.01 \times 0.9 = 0.009表示1000人中只有9个人检测为阳性的是真的得癌症的。再来看看检测出阳性但是没有得癌症的概率： P(B_{2},A) = P(B_{2}) \cdot P(A|B_{2}) = 0.99 \times 0.1 = 0.099即1000人中间只有99个人检测为阳性但是没有得癌症的。那么在检测出阳性的前提下得癌症的概率，即把上面得值归一化即可： P(B_{1}|A)=\frac{0.009}{0.099 + 0.009} \approx 0.083显然，检测为阳性不得癌症的概率约是0.917，那么我们检测为阳性后，也不必万念俱灰，至少Thomas Bayes告诉你，你还有救。在这点上，他果然是上帝。 通过例子知道，贝叶斯的分母可以有2种计算方式，一种是以每个事件都是独立的，连乘积的形式。另一种是全概率公式展开的形式，依据不同情况活用。具体的算法流程请参考： https://blog.csdn.net/dataningwei/article/details/54140537 ，注意到此链接中因为分母对每类后验概率一样，所以后验概率与分子成正比，所以直接最大化分子部分即可。 2.多项式朴素贝叶斯朴素贝叶斯模型最大的特点是基于贝叶斯公式和特征的条件概率是独立的，这就给我们很大的简化空间和计算的方便。多项式朴素贝叶斯（Multinomial Naive Bayes） 比如对于文本分类来说，我们的目标就是获得最大的某一类的后验概率，这里我们假设每个特征之间是独立的： f(x)=argmax_{c_{k}}\ P(c_{k}|x)=argmax_{c_{k}} \frac{P(c_{k})\prod_{i=1}^{n}P(x_{i}|c_{k})}{\sum_{k}P(c_{k})\prod_{i=1}^{n}P(x_{i}|c_{k})}由于所有类的分子一样，所以直接做大化分母即可： f(x)=argmax \ P(c_{k})\ast\prod_{i=1}^{n}P(x_{i}|c_{k})对于$P(c_k),P(x_{i}|c_{k})$计算方法如下： P(c_{k})=\frac{N_{c_{k}}+\alpha}{N+k\alpha}N是总的样本个数，k是总的类别个数，$N_{c_k}$是类别为$c_k$的样本个数，$α$是平滑值。 P(x_{i}|c_{k})=\frac{N_{c_{k},x_{i}}+\alpha}{N_{c_{k}}+n\alpha}$N_{c_{k}}$是第k类$c_{k}$的样本个数，n是特征x的取值个数，$N_{c_{k},x_{i}}$是类别为$c_k$的样本中，第i维特征的值是$x_i$的样本个数，$α$是平滑值。因为若在预测时候，若预测中有特征没有出现在训练集中，那么$P(x_{i}|c_{k})$是0，所以整个乘积都是0,这样就没有意义了。为了防止概率为0，加上一个平滑，让它较好的反映原始概率。 当α=1时，称作Laplace平滑，当0&lt;α&lt;1时，称作Lidstone平滑，α=0时不做平滑。 例子可以参考这篇文章： https://blog.csdn.net/u012162613/article/details/48323777 ，里面还有其他2种模式的解释，不错哦。 3.NB和其他NB的区别（1）相同点：都是以贝叶斯模型为基础。 （2）朴素贝叶斯和多项式贝叶斯的区别： ​ 先看完此链接，有一个基本的认识： https://blog.csdn.net/gaotihong/article/details/78803197 ，主要对NB和MNB进行了概括，可以知道不同点有词向量的表示方式不同，NB是用0或1表示词在字典中出现与否，并且每个垃圾邮件的词向量一样长；但是MNB词向量是以每篇词长度为维度，词在字典中的序号为标记表示向量。另一个不同点我自己看的，在于似然函数的表示不同，我认为这才是真正的本质区别： NB算法图： MNB算法图： 可以看到上面第一张图NB算法中最下面似然函数的表示，垃圾邮件中词的概率：P（某词 j|垃圾邮件=1）=词j在几个垃圾邮件中出现/垃圾邮件的总个数 第二张图中MNB算法的似然函数：P（词 j |1=垃圾）= 所有垃圾邮件中词j出现的次数/所有垃圾邮件的词总量 可以发现似然函数不同。总结下：个别论文说是朴素贝叶斯只关心特征（词）出现还是不出现在一个类别中，但是多项式贝叶斯以每个特征（词）在样本集里的频次来决定类标。这样对NB和MNB有一个更加清晰的认识。 （3）朴素贝叶斯和各个贝叶斯之间的区别： （1）朴素贝叶斯的假设前提有两个第一个为：各特征彼此独立；第二个：对被解释变量的影响一致，不能进行变量筛选。贝叶斯显然是没有独立的前提，即各个特征之间可以有依赖的，也可以独立。 （2）GaussianNB、MultinomialNB和BernoulliNB的似然函数的计算方式不同。GaussianNB就是似然函数$P( x_{i} | c_{k})$为高斯分布的朴素贝叶斯，MultinomialNB就是似然函数为多项式分布的朴素贝叶斯，而BernoulliNB就是似然函数为伯努利分布的朴素贝叶斯。 （3）一般来说，如果样本特征的分布大部分是连续值，使用GaussianNB会比较好。如果如果样本特征的分大部分是多元离散值，使用MultinomialNB比较合适。而如果样本特征是二元离散值或者很稀疏的多元离散值，应该使用BernoulliNB。 4.参考（1）贝叶斯的三种模型 （2）例子1来源 （3）贝叶斯和MCMC博客 （4）贝叶斯网络与朴素贝叶斯的区别]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Naive Bayes</tag>
        <tag>多项式朴素贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[向量空间模型（VSM）]]></title>
    <url>%2F2018%2F11%2F28%2F%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[1.词袋模型(BOW)​ 简单理解下词带模型，顾名思义就是一个袋里装了很多单词，可知特点：（1）假设词语无序 （2）假设词与词之间独立。Bag-of-words模型是信息检索领域常用的文档表示方法。忽略了它的单词顺序和语法、句法等要素，将其仅仅看作是若干个词汇的集合。 它具体是如何表示的？如下有一个句子： I have a pear and a strawberry. 于是可以构建一个字典，如下： {“i”:0,”have”:1,”a”:2,”pear”:3,”and”:4,”strawberry”:5} ，这个字典里面的字符可表示出所有的词，且唯一。但是字典里面词的顺序和原句子词序并不相关，这里只是特例。于是可以建立一个词向量如下： [1,1,2,1,1,1] ，这里是字典里面的词在原句子里出现的次数的统计，一一对应。于是若有若干句子组成一篇文档，则可以构建一个对应的字典，然后有一个N维的向量来表示这篇文档（词频的统计）。若要比较2篇文档相似度，也可以比较2个向量之间的余弦相似度。 2.向量空间模型(VSM)2.1VSM概念​ 向量空间模型（Vector Space Model）和词袋模型很像（个别说是一样的），都是把一个文档表示成向量的模式，而向量空间模型运用较为广泛的是TF-IDF方法。 第1：把一篇文档表示成词向量：D=[W_1,W_2,W_3,…,W_m],某D文档中有m个项。这里可以是的单个词，可以是词组等等。 第2：计算每个项的贡献度：Q =[Q_1,Q_2,Q_3,…,Q_m] ,这里的Q_i是每个项的贡献度，名字不一，也可以理解为每一项（每个词）的比重，权重。计算贡献度的方法不一。 于是每个文档便由一组词向量和对应的贡献度向量表示而成。 2.2TF-IDF方法其实就是第2步的算法不同，这里的TF-IDF分为TF（每一项的频率：Term Frequency）和IDF（逆文档率：Inverse Document Frequency），这里的TF，就是每个词在文档中的频次比例。如下： TF =\frac{N_i}{M},N_i是第i个词在某一篇文档的频次，M是该文档的总词量。IDF就是这个词在哪些文档出现了，因为语料库由K篇文档组成，每篇文档M个词。计算如下： IDF =log(\frac{K}{L+1}),K表示语料库有k篇文档，L表示包含该词的文档数量，L+1防止分母为0。那TF-IDF计算公式如下： TF-IDF =\frac{N_i}{M}*log(\frac{K}{L+1})=\frac{N_i}{M}*log(\frac{1}{D_i}),D_i是某个词出现在所有文档的中的频次比例。发现：$log_a^b​$（底数a&gt;1）函数是单调增函数，且定义域在(0,1]时值域为负值，大于1时为单调正数。由于概率是在[0,1]的，倒数就是大于等于1，这里分母取不到0，倒数就不会是正无穷 ，log(b)值就是一个正数，并且b越大IDF就越大。这里其实K/L是某个词的在语料库中的文档率的倒数。所以当K和L的比值越大，IDF就越大，说明K和L比例差的越大，对我们越有利。比如有20篇文档，若一个词在所有文档中出现了1次，K/(L+1)=20/(1+1)=10,log(10)=3.32，相对其他来说是一个蛮大的比重。那这个词对我们来说也确实是重要的，在做文档相似度的时候作用就很大，因为只要这个词出现说明该词有很好的区分性。相反，若K/L的比值很小，比如K/(L+1)=1时，log1=0,比重就很小。为什么该词比重需要设置的小？因为K/(L+1)==1，说明K和L相近，即这个词在每个文档都出现，那这个词很可能是介词、连词等没有区分性的词，比如’the’，’for’,’的’，’在’这些词很大概率出现在所有文档中，所以不重要，权重就小。当然，也可以通过设置停用词的方法来筛掉这些词，这个是数据预处理的部分了。 比如语料库中共有10篇文档，每篇800词。某一篇文档中的”战斗机” 和 “的”都出现了20次，但是”战斗机” 在2篇文档中出现，而”的”在9篇文档中出现，”的”字符的IDF=TF*log1=0，这样算下来”战斗机” 的TF-IDF的值肯定比0大，故”战斗机” 这个词更加重要。然后排序后，可以取前n个重要的词放入分类器做分类也可，计算相似度也可。其实，TF-IDF计算方法类似于交叉熵，贡献度的计算法子还有很多，按下不表。 补充一下我在weka包中遇到的问题：weka里面的TF和IDF可以分开来设置，若两者都是True,则表示TF-IDF=log（1+某文档中某个词的频次） x log（逆文档率），按照这种计算出来得值可以大于1，之前一直没想明白，因为我把TF的频次算成了频率所以一直小于1。 BOW和VSM两者的区别其实很小，我自己看来，就在于贡献度不一。 2.3熵的计算方法这里其实也算是特征选择，本文只给出了特征选择的计算方法。特征选择是一个系统的工程，有自己的方法和技术。本文特指章节2.1里面第2步骤中贡献度的计算方法。 （1）信息熵 前面提到了交叉熵，先理解什么是熵。熵是由香农提出的，记得有位科学家说：若自己余生也发明一个类似“熵”这样的概念，这一生也值了。可见其重要程度，后来证实熵在计算机领域，通信领域，信息论等等都是举足轻重的。到底什么是熵？直接给出离散变量x的熵公式： H(X)=-\sum\limits_{x\in\mathcal{X}}p(x)\log p(x)作用： 用来度量信息的不确定程度。举个栗子：若你在3个盒子中抽一个奖，设每个盒子抽到奖的概率=1/3，代入熵公式=- 1/3 x log(1/3) - 1/3 x log(1/3) - 1/3 x log(1/3) =0.47712。但是第二个人由于商家作弊，使得第2个盒子抽到的概率上升为0.8，其他2个都是0.1,H(X) = -0.1 x log(0.1) x 2 - 0.8 x log(0.8) = 0.277528。显然这个第二个熵变小了，但是确实符合它反映的信息量。因为第二个事件的信息量更加大。信息熵的特点是熵越小，表示信息越纯，即信息量越大。所以，这个值和我们的事实是符合的。 （2）条件熵 定义：在一个条件下，随机变量的不确定性。 ​ 条件熵公式：$H(X|Y)=-\sum\limits_{x,y}{P(X,Y)*log(P(X|Y))}$ 证明如下图： 下面证明 条件熵=联合熵-单独的熵：$H(Y|X)=H(X,Y)-H(X)$： 举个栗子吧： https://zhuanlan.zhihu.com/p/26551798 ,这篇便有。 （3）信息增益（Information Gain） IG=熵 - 条件熵即为： Grain(Y,X)=H(Y)-H(Y|X)例子请参考周志华《机器学习》P75。 （4）交叉熵 交叉熵又叫KL散度，是用来衡量2个概率分布的差异程度。交叉熵即KL散度的计算公式： D_{KL}(p||q)=\sum_{i=1}^np(x_i)log(\frac{p(x_i)}{q(x_i)}) \tag{1}这里的p是真实分布，比如我们遇到一个复杂的p分布，要做的事就是用一个简单的分布q来代替复杂的、不好计算的真实分布p，KL散度就是用来衡量这2个分布的差异。发现log后面p(xi)和q(xi)越接近时候，KL散度值近乎为0。就是说KL散度值越小，两个分布越接近，即可以用后面的q分布来代替真实分布p。在变分推断里会遇到这个概念，我等只能敬而远之。交叉熵在部分问题时候，也可以做为loss函数。比如：https://blog.csdn.net/tsyccnh/article/details/79163834 大致把熵过了一遍，说的好不如做得好，还是多实践吧。$Vouloir\ \ c’est\ \ pouvoir$（有志者，事竟成）。 3.参考1.词袋模型简介 2.逆文档频率法]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>向量空间模型</tag>
        <tag>词袋模型</tag>
        <tag>信息熵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性空间的定义]]></title>
    <url>%2F2018%2F11%2F19%2F%E7%BA%BF%E6%80%A7%E7%A9%BA%E9%97%B4%E5%AE%9A%E4%B9%89%2F</url>
    <content type="text"><![CDATA[做了一个关于线性空间的思维导图，纯粹练练思维导图的作图技术。希望有启发。 Bonne chance.]]></content>
      <categories>
        <category>矩阵分析</category>
      </categories>
      <tags>
        <tag>线性空间定义</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归和python3实现]]></title>
    <url>%2F2018%2F11%2F10%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[1.逻辑回归笔记​ 逻辑回归就是在线性回归的基础上套一个分类函数，当然也可以不用线性回归。下面是我根据周志华的逻辑回归写的笔记。具体请参考周志华的《机器学习》P57-60。第1张图是说明了逻辑回归的预测模型，第2张图是优化目标L的由来，第3张图是详细的求出一阶导数和二阶导数。我觉得这里一阶导数和二阶导数周志华这里写的不好，矩阵计算写的不清晰明朗。当然这也是自己基础不牢靠所致，共勉吧。如下： 图1：得到两个后验概率的表达式P0和P1，用极大似然估计法找出优化目标L。 图2：再把P0，P1和转化后等价的似然项代入对数似然L,简化后即可得最终优化目标L。 图3：L对2个参数的求导。 最后由牛顿法得到迭代参数公式和优化目标L。 ​ 在之前计算的时候出现了奇异矩阵，即矩阵的行列式值为0，想了下可能是之前的数据之间存在线性相关性，所以导致矩阵的秩不满秩，所以也就不可逆了，即变成了奇异矩阵。若我不做实验，就不会知道奇异矩阵，也就记不起来以前的秩了。想着一些小算法，可以重复造轮子，但是复杂的算法就算了，没必要。人生奇美，怎可废！ ​ 2.python3实现用的python3.5.1+pycharm,将就着看吧。只用了5个样本，准确率不高的，简单的实现了预测，没有深究。下面是伪代码： 上面的终止条件就是人为设定的次数或者是优化目标L达到可接受的范围，否则一直迭代。 具体实现如下,代码在这里排列的不好，但是可以运行： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from numpy import *from math import *import numpy as npfrom numpy.linalg import invdef sigmoid(x): return 1./(1.+np.exp(-x))def logistic_regression(): # 牛顿方法构建Logistic Regression , 对西瓜是否为好瓜或坏进行分类, #Y &lt;&lt;--预测为-- Sigmoid(w1*x1+w2*x2 + bias) X = np.array([[0.697,0.774,0.634,0.666,0.243], [0.46,0.376,0.264,0.091,0.267], [1,1,1,1,1]]) Y =np.array([1,1,1,0,0]) beta = np.array([[0], [0], [1]]) maxCycles = 20 #迭代次数 j = 0 #累积次数 old_L = 0 while j &lt;= maxCycles: #循环20次后者损失函数已经达到合理小的范围就停止迭代 beta_T_x = dot(beta.T[0], X) # 计算β_TX L = 0 # 损失函数 for i in range(5): L = L +(-Y[i]*beta_T_x[i] + log(1 + exp(beta_T_x[i]))) if abs(L-old_L)&lt;= 0.00001: break print('最优化目标L=',L) old_L = L grad = 0 # 一阶导数 H = 0 # 二阶导数 # 计算5个训练样本的优化目标,公式（3.27) for i in range(5): # 梯度: ∂L(β)/∂β，P60公式（3.30）。 grad = grad - dot(array([X[:,i]]).T,(Y[i] - array([[exp(beta_T_x[i])/(1+exp(beta_T_x[i]))]]))) #X[i]是横向量，需要转制，grad结果是1个数 H = H + dot(array([X[:,i]]).T,array([X[:,i]]).T.T)*((exp(beta_T_x[i])/(1+exp(beta_T_x[i]))) * (1-exp(beta_T_x[i])/(1+exp(beta_T_x[i])))) beta = beta - dot(inv(H),grad) #参数迭代公式（3.29） j += 1 #预测部分 print('beta=', beta) x1 = float(input('请输入西瓜的密度:')) x2 = float(input('请输入西瓜的含糖率:')) z = beta[0][0] * x1 + beta[1][0] * x2 + beta[2][0] print('预测值为：',sigmoid(z)) f = sigmoid(z) if(f==1): #P66公式（3.47)正、反例的比值&gt;观测的正反比例即认为正例，否则反例 print('分类器预测此为好瓜。') else: print('分类器预测此为坏瓜。')logistic_regression() 预测结果如下：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Logistic Regression</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bonjour!]]></title>
    <url>%2F2018%2F09%2F13%2FSVM%2F</url>
    <content type="text"><![CDATA[1.SVM总体简单介绍首先，分类的时候我们通常有时候遇到一个分类情况是，多类数据交融，无法通过简单的线性分类器区分。那SVM（Support Vector Machine）就是把数据映射到高维，然后根据支持向量找出一个最佳分类面，使得数据可以分开。这就是支持向量机干的事情。参考下面视频： 2.硬间隔SVM首先清楚四个概念：超平面，函数间隔，几何间隔，支持向量。 （1）超平面在二维空间里面是：Ax+By+C=0 ,是一条直线。在三维空间里面是：Ax+By+Cz+D=0，是一个平面。在三维以上：Ax1+Bx2+Cx3+Dx4+Ex5+Fx6+….+K=0 =&gt; 可看作是2个n维向量W和X的内积：$W^TX+b = 0$,$b$可以认为是截距，是代表了到原点的距离；W是法向量，代表了超平面的方向。这就是它的定义了。 （2）函数间隔定义为：预测函数与实际类标的乘积。我们下面提到的都是简化版的情况，即只讨论在平面情况下的分类。有一些类标$y^i$,只有正例和反例：1或-1，预测的值是：$W^TX^i+b$，函数间隔就是把这2个乘起来： \hat{γ}=y^{i}(W^{T}X^{i}+b)，γ上面有一个hat的就表示函数间隔。发现$y^i$和 $(W^TX^i+b)$是同号的话就是分类正确了。如果真实类标$y^{i}=1$，要令$(W^{T}X^{i}+b)$远大于0，这样才越准确，如果真实类标$y^{i}=-1$,则要$(W^{T}X^{i}+b)$远远小于0才越准确。就是说函数间隔可以看作是描述分类准确与否的一个指标。 （3）几何间隔L几何间隔对比函数间隔是一个更好的指标，因为，若超平面是$W^TX+b = 0$的情况下，同时放大W和b的值为原来的n倍，后发现，函数间隔$y^{i}(W^{T}X^{i}+b)$变大了 n倍，但是超平面$W^TX+b=0$没有变化，这就是函数间隔的缺点。先来看看下面的图片：可以看到，超平面L把数据很好的分开来，其中有某个样本$A(x_i,y_i)$，$B$点在超平面上面，单位向量$W$除以自己的模，得到单位长度为$\frac{W}{||W||}$。有一个$γ^i$与向量AB的长度一样，则$B$点位置的$x$就等于$A$点的$X_i$减去$AB$的长度（$AB$带有方向性），而$AB=γ_i\cdot\frac{W}{||W||}$，即有：$x = x_i - γ_i\cdot\frac{W}{||W||}$,又因为$B$点的$x$在超平面L上，满足L的表达式，于是有： W^{T}(X^{i}-γ^{i}\frac{W}{||w||})+b=0简化提出$γ_i$后得到几何间隔的表达式： γ^{i}=y^{i}[\frac{W}{||w||}X^{i}+\frac{b}{||w||}]可以知道，2点：第一：当$||W||=1$，$\hat{γ^{i}}=γ^{i}$第二：几何间隔就等于函数间隔除以一个$||W||$：$γ^{i}=\frac{\hat{γ^{i}}}{||w||}$ （4）支持向量SVM的目标是在多个分类中找到最佳的超平面，看上面的图，外面的直线上面的点，就是支持向量，这些点与超平面最近且与超平面保持一定的函数距离。中间的虚线就是我们要找的最佳的超平面，这样的超平面可以确保就算是有些点在直线上面或者里面，也可以很好地区分。 3.最优间隔分类器问题从上面图可知，2条直线之间的距离越大越好，即几何间隔越大越好，这样可以使更多的数据很好的分开来。同时，找到唯一的最好超平面，所有样本点与超平面有一定的函数间隔（即分类准确）： \begin{align*} &\max_{w,b,γ^{i}}\quad \frac{\hat{γ}}{||w||}\\ & \begin{array}{r@{\quad}r@{}l@{\quad}l} s.t.&y(W^{T}X+b)>=\hat{γ}\\ \end{array} \end{align*}发现上面的优化问题是非凸优化，所以为了简化问题，思路是先把此问题转化为对偶问题，然后通过求解对偶问题的解，来解出原问题的解。这就是基本思路。为了简化计算，可以把函数间隔$\hat{γ}$令为1，因为函数间隔扩大n倍时，底下的||W||也扩大n倍，所以并不影响优化。上面分母变成常数1后，问题可以转为最小化分子的问题。||W||加了1/2的平方是便于后面求导，无碍： \begin{align*} &\min_{w,b}\quad \frac{1}{2}||w||^{2}\\ & \begin{array}{r@{\quad}r@{}l@{\quad}l} s.t.&y^{i}(W^{T}X^{i}+b)>=1\\ \end{array} \end{align*}根据凸优化理论，该问题可以通过拉格朗日函数将目标函数转化为无约束的目标函数进行求解： L(w,b,α)=\frac{1}{2}||w||^{2}-\sum\limits_{i=1}^{m}α_{i}(y^{i}(W^{T}X^{i}+b)-1)L对αi和b分别求导并令为0后得： W=\sum\limits_{i=1}^{m}α_{i}y^{i}x^{i}\sum\limits_{i=1}^{m}α_{i}y^{i}=0然后把上面的2个式子代入L： L=\sum\limits_{i=1}^{m}α_{i}-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^my^{i}y^{j}α_{i}α_{j}\quad\quad(1)可以令上面L为新的W(α)函数，后面的尖括号里面的就是xi和xj的内积表达式。 4.对偶问题4.1原始问题先来看原始问题，什么是原始问题？假设$f(x)$,$c_i(x)$,$h_j(x)$是定义在$R^n$上的连续可微函数，考虑约束最优化问题时候： \begin{align} ​ \min_{x \in R^n}\quad &f(x) \\ ​ s.t.\quad&c_i(x) \le 0 , i=1,2,\ldots,k\\ ​ &h_j(x) = 0 , j=1,2,\ldots,k ​ \end{align}上面就是一个有约束条件的原始问题。我们一般的思路就是一脑子求导=0，然后解出x代入原式即可(因为极值处的斜率为0 )。可是这里有约束条件，怎么办？于是拉格朗日先生便一跃而出，他说可以干掉这个纸老虎。看看他怎么解决的？ \mathcal{L}(x,\alpha,\beta) = f(x) + \sum_{i=1}^k \alpha_ic_i(x) + \sum_{j=1}^l \beta_jh_j(x)没错，他也是一股脑的把所有式子塞到一起，起个名字叫做广义拉格朗日函数。其中的α和β都是参数，特别要求αi&gt;=0。直接把上面式子求导代入原式即可。就是上面公式(1)的求解结果。发现上面的式子若不满足约束条件，就至无穷大了，若满足约束条件，则令L的最大值为： \theta_P(x) = \max_{\alpha,\beta:\alpha_i \ge 0}\mathcal{L}(x,\alpha,\beta)发现原始问题就等价于最小化上面的式子： \min_{x}max \mathcal{L}(x,\alpha,\beta)=\min_{x} \theta_P(x)=\min_{x} f(x)\quad\quad(2)因为αi&gt;=0,ci(x)&lt;=0,h(x)=0，所以θp的最大值是f(x)。 4.2对偶问题上面先最大化再最小化问题，对偶问题就是先最小化再最大化问题，即对偶问题为： \max_{\alpha,\beta:\alpha_i\ge0}\theta_D(\alpha,\beta)=\max_{\alpha,\beta:\alpha_i\ge0}\min_x\mathcal{L}(x,\alpha,\beta)\quad\quad(3)在一般情况下公式(2)要&gt;=公式(3),例如，对于同一个问题，x∈{0,1},y∈(0,1)，min max(1(x=y))&gt;=max min(1(x=y)),这里的1(x=y)为示性函数:若括号里的为真，则返回1，否则返回0。左边最大的1=1，再取最小值为1，右式最小的0=0，再取最大的值为0,1&gt;0，故可以看出左式&gt;=右式。感兴趣的可以自己搜下证明。那么，什么时候等式才成立呢？这是个好问题，答：满足KKT条件的时候。这样，我们就可以把原始问题转化为对偶问题，通过解出对偶问题的参数，也就解出原始问题的部分参数，这样就简化了计算。 4.3KKT条件对于KKT条件,由定理：对于原始问题和对偶问题，假设函数$f(x)$和$c_i(x)$是凸函数，$h_i(x)$是仿射函数（即由一阶多项式构成的函数，f(x)=Ax + b, A是矩阵，x，b是向量）；并且假设不等式约束$c_i(x)$是严格可行的，即存在x，对所有i有$c_i(x)&lt;0$，则x*,α*,β*分别是原始问题和对偶问题的最优解的充分必要条件是x*,α*,β*满足下面的Karush-Kuhn-Tucker(KKT)条件： \begin{align} & \nabla_xL(x^*,\alpha^*,\beta^*)=0\\ &\nabla_\alpha L(x^*,\alpha^*,\beta^*)=0\\ &\nabla_\beta L(x^*,\alpha^*,\beta^*)=0\\ &\alpha_i^*c_i(x)=0,i=1,2,\ldots,k\quad\quad(a)\\ &\alpha_i^*\ge0,i=1,2,\ldots,k\quad\quad\quad\quad(b)\\ &c_i(x)\le0,i=1,2,\ldots,k\quad\quad\quad (c)\\ &h_j(x^*)=0,j=1,2,\ldots,l\quad\quad\quad(d) \end{align}前三个求偏导是保证驻点的存在，后面的式子(a)、(b)是拉格朗日乘子需要满足的约束，(c)、(d)是原始问题的约束条件。特别注意当αi*&gt;0时，由KKT对偶互补条件可知:ci(x)=0，这个后面会用到。 5.软间隔SVM第3章节主要讲的是硬间隔最大化，那什么是软间隔最大化？硬间隔呢就是支持向量不能超过第二幅图的直线，顶多在线上面，但是软间隔可以容忍少量的支持向量落在直线内，于是我们的约束条件$y_i(W^TX+b)\ge1$就要修改为$y_i(W^TX+b)\ge1-ξ_i$，即允许部分支持向量间隔不足1,$ξ_i&gt;=0$。我们原始的最大化约束问题转为： \begin{align} \min_{w,b}\quad &\frac{1}{2}||w||^{2}+C\sum\limits_{i=1}^{m}ξ_i\\ s.t.\quad&y^{i}(W^{T}X^{i}+b)\ge1-ξ_i\\ &ξ_i\ge0 \end{align}由于加了松弛变量ξi，优化目标后面也加上惩罚项，其中C值大，对误分类的惩罚越大。前面的平方项希望函数间隔越大，后面的惩罚项希望误分类的样本点越少。通过拉格朗日函数将目标函数转化为无约束的目标函数进行求解： \mathcal{L}(w,\alpha,\beta) = \frac{1}{2}||w||^{2} + C\sum\limits_{i=1}^{m}ξ_i-\sum_{i=1}^m \alpha_i[y^{i}(W^{T}X^{i}+b)-1+ξ_i] -\sum_{j=1}^mu_jξ_i\quad\quad(4)把上面L函数分别对W, b, ξi求偏导=0可得： w = \sum_{i=1}^mα_iy_ix_i\quad\quad(5) \sum_{i=1}^mα_iy_i=0 C - α_i - u_i = 0把上面三个式子代入公式（4），可得到我们的对偶问题的优化目标为： \begin{align} \max_\alpha\quad &\sum\limits_{i=1}^{m}α_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^my_iy_jα_iα_j\\ s.t.\quad&\sum_{i=1}^mα_iy_i=0\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad(6)\\ & 0\leα_i\le C,i=1,2,\dots,m \end{align}有上式可求出α，再由α求出W和b。公式（5）可求出W，而b是所有支持向量满足$y_i(W^TX_i+b)=1$的值b的均值。由KKT条件： α_i[y^{i}(W^{T}X^{i}+b)-1+ξ_i]=0\\ u_iξ_i=0上面第一个式子正是由于αi&gt;0,由KKT推断$y_i(W^TX_i+b)-1+ξ_i=0$所得。可知：满足$0\leα_i\le C$的样本点$(x_s,y_s)$，即为支持向量。详细请点击底部的参考1。 5.1核函数对于线性可分数据用硬间隔和软间隔可以区分数据，但对于非线性的数据怎么办？于是，我们的核函数粉墨登场（这里引用原意：演员化妆演戏）。它干的事情直观感受就是上面最开始的视频里干的部分事：把非线性的数据映射到线性可分的维度里面。来，举个栗子八。就是我们有一组非线性的2维模型： f(X_1,X_2) = A_0+A_1X_1+A_2X_2+A_3X_1^2+A_4X_2^2+A_5X_1^3+A_6X_2^3若令： X_1=X_1,X_2=X_2,X_1^2=X_3,X_2^2=X_4,X_1^3=X_5,X_2^3=X_6则可以转化为6维模型： f(X_1,X_2，X_3，X_4，X_5，X_6) = A_0+A_1X_1+A_2X_2+A_3X_3+A_4X_4+A_5X_5+A_6X_6就是说：低维度不可分的数据模型可以映射到高维度后，变成线性可分的模型。下面的核函数就是干的这个事情，就是有核函数k(x,z)把低维度的φ(xi)和φ(zj)之间的内积在低纬度计算掉： k(x,z)=φ(x)φ(z)常见的核函数有： （1）高斯核函数（Gaussian Kernel） \kappa(x, x_i) = exp(-\frac{||x - x_i||^2}{\delta^2})高斯径向基函数是一种局部性强的核函数，其可以将一个样本映射到一个更高维的空间内，该核函数是应用最广的一个，无论大样本还是小样本都有比较好的性能，而且其相对于多项式核函数参数要少。 （2）线性核函数（Exponential Kernel） \kappa(x,x_i) = x \cdot x_i线性核，主要用于线性可分的情况，我们可以看到特征空间到输入空间的维度是一样的，其参数少速度快，对于线性可分数据，其分类效果很理想，因此我们通常首先尝试用线性核函数来做分类，看看效果如何，如果不行再换别的。 （3）多项式核函数（Polynomial kernel function） \kappa(x, x_i) = ((x\cdot x_i) + 1)^d多项式核函数可以实现将低维的输入空间映射到高纬的特征空间，但是多项式核函数的参数多，当多项式的阶数比较高的时候，核矩阵的元素值将趋于无穷大或者无穷小，计算复杂度会大到无法计算。这几个核函数可以优先使用高斯核函数，据说Andrew Ng只用高斯核函数，可见性能确实优越。当然，还是要根据自己的模型找最适合的核函数。还有很多核函数就不介绍了，感兴趣的自行搜索。 6.SMO算法6.1求解$α_1,α_2$了解了核函数后，上面式子（6）对偶问题的内积换成核函数后，提个负号出来整体变为最小化问题就等价于： \begin{align} \min_\alpha\quad &\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^my_iy_jα_iα_jk(x_i,x_j)-\sum\limits_{i=1}^{m}α_i\\ s.t.\quad&\sum_{i=1}^mα_iy_i=0\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad(7)\\ & 0\leα_i\le C,i=1,2,...,m \end{align}SVM的“核“是什么呢？就是SMO（Sequential Minimal Optimization）算法，他是用来求α的，前面说过：求出了α，就可以求出W和b,接着超平面也就找到了。它的主要思想就是：α有m个变量，但是直接根据优化问题来求解很困难。所以，SMO算法每次只对2个变量更新，其他变量看成是常数，而常数项可以在优化目标中省略。 上面的是具体α求解步骤，稍微讲解一下，公式展开来自公式（7）。min后面优化目标这么长，不要吓坏了，只是把α1和α2变量部分展开来，其余看作常数。下面的限制条件原本是$\sum_{i=1}^mα_iy_i=0$，因为$y_i$只能取-1和1，所以就看作是m个正负$α_i$相加=0，拿出2个变量：α1和α2出来玩玩，其余的和为常数-k。需要注意的是2个相同$y_i$的乘积为1，这里都会用到以简化计算。然后得到第1部分的优化目标和约束条件。(图中的$α_2^{new,uncut}$不对，应该是$α_2^{new,unc}$，后面的其实是unclip) 第2部分，是讲解的α的整个迭代过程。过程上面图片有，讲一下L和H的范围是怎么来的。以左边双红线图为例：α本身就有限制[0,C]，故有图中的正方形。坐标轴的横坐标是α1，纵坐标是α2,由α1-α2=K可得2条红色的线，因为已经限制了α1和α2的范围，只能在方框中。可得到线①、线②。可知线①的α2范围是[-K,C]，线②的α2范围是[0,C-K]，再把α1-α2=K代入即可得到L，H的范围。另一种情况也一样可得。 这一步主要是求解未经剪辑的α2，思路是先简化优化目标函数W(α1，α2)，然后代入α1和α2的关系式消去α1，使得表达式全部是关于α2。接着W（α2）对α2求导=0即可。最后可以得到迭代式子： α_2^{new,unc} = α_2^{old} + \frac{y_2(E_1-E_2)}{k_{11}+k_{22}+k_{12}}6.2求解b当$0&lt;α_1^{new}&lt;C$时,根据KKT条件$y_i(W^TX_i+b)=1$: y_1(\sum_{j=1}^{m}α_jy_jk_{1j}+b_1)=1\\ y_1\cdot y_1(\sum_{j=1}^{m}α_jy_jk_{1j}+b_1)=1\cdot y_1 \\ 即为：\sum_{j=1}^{m}α_jy_jk_{1j}+b_1-y_1=0\\ 迭代得：b_1^{new}=y_1-α_1^{new}y_1k_{11}-α_2^{new}y_2k_{12}-\sum_{j=3}^{m}α_jy_jk_{1j}\\ 本轮迭代中E_1: E_1=g(x_1)-y_1\\ y_1代入上面的b_1^{new}可得：\\ b_1^{new}=b^{old}-E_1+y_1k_{11}(α_1^{old}-α_1^{new})+y_2k_{12}(α_2^{old}-α_2^{new})\\ b_2^{new}同理：\\ b_2^{new}=b^{old}-E_2+y_1k_{12}(α_1^{old}-α_1^{new})+y_2k_{22}(α_2^{old}-α_2^{new})\\ 故：b^{new}=\frac{b_1^{new}+b_2^{new}}{2}最后更新Ei为下一轮迭代做准备: E_i = \sum_{S}α_iy_ik_{ij}+b^{new}-y_i最终的b是： b = y_s-\sum_{S}α_iy_ik(x_i,x_s)即所有支持向量的b的均值即为最终的b值。 6.3求分类超平面即为： \sum_{i=1}^mα_iy_ik(x_i,x_s)+b=07.总结SMO算法部分强烈推荐参考：点击此处，我的多数都是来自于此。这篇讲解的非常仔细，比我不知高到哪里去了。最后参考链接第5个是他这个系列的总体，一共5部分，包括代码实现。SVM优点：泛化错误率低，计算开销不大，结果易解释。缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。适用数据类型：数值型和标称型数据。 非常感谢那些乐于分享知识的人。这篇写了整整2天，算上搭建blog的话有一个礼拜了，也算是对最近的一个梳理和总结吧。最后，推荐一款Markdown编辑器：Typora,做到了所谓的“所见即所得”，这是我听过的世界上最好的宣传语(≧∇≦)ﾉ，Au revoir, chère amie。 参考链接： 1.支持向量机Part2—线性支持向量机 2.svm常用核函数 3.从超平面到SVM（三） 4.最优间隔分类器问题 5.支持向量机Part5—Python实现]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>SVM</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
