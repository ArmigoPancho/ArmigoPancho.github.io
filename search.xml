<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[向量空间模型（VSM）]]></title>
    <url>%2F2018%2F11%2F28%2F%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[1.词袋模型(bag of words)​ 简单理解下词带模型，顾名思义就是一个袋里装了很多单词，可知特点：（1）假设词语无序 （2）假设词与词之间独立。Bag-of-words模型是信息检索领域常用的文档表示方法。忽略了它的单词顺序和语法、句法等要素，将其仅仅看作是若干个词汇的集合。 它具体是如何表示的？如下有一个句子： I have a pear and a strawberry. 于是可以构建一个字典，如下： {“i”:0,”have”:1,”a”:2,”pear”:3,”and”:4,”strawberry”:5} ，这个字典里面的字符可表示出所有的词，且唯一。但是字典里面词的顺序和原句子词序并不相关，这里只是特例。于是可以建立一个词向量如下： [1,1,2,1,1,1] ，这里是字典里面的词在原句子里出现的次数的统计，一一对应。于是若有若干句子组成一篇文档，则可以构建一个对应的字典，然后有一个N维的向量来表示这篇文档（词频的统计）。若要比较2篇文档相似度，也可以比较2个向量之间的余弦相似度。 2.向量空间模型2.1VSM概念​ 向量空间模型（Vector Space Model）和词袋模型很像（个别说是一样的），都是把一个文档表示成向量的模式，而向量空间模型运用较为广泛的是TF-IDF方法。 第1：把一篇文档表示成词向量：D=[W_1,W_2,W_3,…,W_m],某D文档中的m个词。这里可以是的单个词，可以是词组等等。 第2：计算每个词的贡献度：Q =[Q_1,Q_2,Q_3,…,Q_m] ,这里的贡献度，名字不一，也可以理解为每一项（每个词）的比重，权重。计算的方法不一。 于是每个文档便由一组词向量和对应的贡献度向量表示而成。 2.2TF-IDF方法其实就是第2步的算法不同，这里的TF-IDF分为TF（每一项的频率：Term Frequency）和IDF（逆文档率：Inverse Document Frequency），这里的TF，就是每个词在文档中的频次比例。如下： TF =\frac{N_i}{M},N_i是第i个词在某一篇文档的频次，M是该文档的总词量。IDF就是这个词在哪些文档出现了，因为语料库由K篇文档组成，每篇文档M个词。计算如下： IDF =log(\frac{K}{L+1}),K表示语料库有k篇文档，L表示包含该词的文档数量，L+1防止分母为0。发现：$log_a^b$（底数a&gt;1）函数是单调增函数，且定义域在(0,1]时值域为负值。由于概率是在[0,1]的，倒数就是大于等于1，这里分母取不到0，倒数就不会是正无穷 ，log(b)值就是一个正数，并且b越大IDF就越大。这里其实K/L是某个词的在语料库中的文档率的倒数。所以当K和L的比值越大，IDF就越大，说明K和L比例差的越大，对我们越有利。比如有20篇文档，若一个词在所有文档中出现了1次，K/(L+1)=20/(1+1)=10,log(10)=3.32，相对其他来说是一个蛮大的比重。那这个词对我们来说也确实是重要的，在做文档相似度的时候作用就很大，因为只要这个词出现说明该词有很好的区分性。相反，若K/L的比值很小，比如K/(L+1)=1时，log1=0,比重就很小。为什么该词比重需要设置的小？因为K/(L+1)==1，说明K和L相近，即这个词在每个文档都出现，那这个词很可能是介词、连词等没有区分性的词，比如’the’，’for’,’的’，’在’这些词很大概率出现在所有文档中，所以不重要，权重就小。当然，也可以通过设置停用词的方法来筛掉这些词，这个是数据预处理的部分了。 那TF-IDF计算公式如下： TF-IDF =\frac{N_i}{M}*log(\frac{K}{L+1})=\frac{N_i}{M}*log(\frac{1}{D_i}),D_i是某个词出现在所有文档的中的频次比例。比如语料库中共有10篇文档，每篇800词。某一篇文档中的”战斗机” 和 “的”都出现了20次，但是”战斗机” 在2篇文档中出现，而”的”在9篇文档中出现，”的”字符的IDF=TF*log1=0，这样算下来”战斗机” 的TF-IDF的值肯定比0大，故”战斗机” 这个词更加重要。然后排序后，可以取前n个重要的词放入分类器做分类也可，计算相似度也可。 参考：]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>向量空间模型 - 词袋模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性空间的定义]]></title>
    <url>%2F2018%2F11%2F19%2F%E7%BA%BF%E6%80%A7%E7%A9%BA%E9%97%B4%E5%AE%9A%E4%B9%89%2F</url>
    <content type="text"><![CDATA[做了一个关于线性空间的思维导图，纯粹练练思维导图的作图技术。希望有启发。 Bonne chance.]]></content>
      <categories>
        <category>矩阵分析</category>
      </categories>
      <tags>
        <tag>线性空间定义</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归和python3实现]]></title>
    <url>%2F2018%2F11%2F10%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[1.逻辑回归笔记​ 逻辑回归就是在线性回归的基础上套一个分类函数，当然也可以不用线性回归。下面是我根据周志华的逻辑回归写的笔记。具体请参考周志华的《机器学习》P57-60。第1张图是说明了逻辑回归的预测模型，第2张图是优化目标L的由来，第3张图是详细的求出一阶导数和二阶导数。我觉得这里一阶导数和二阶导数周志华这里写的不好，矩阵计算写的不清晰明朗。当然这也是自己基础不牢靠所致，共勉吧。如下： 图1：得到两个后验概率的表达式P0和P1，用极大似然估计法找出优化目标L。 图2：再把P0，P1和转化后等价的似然项代入对数似然L,简化后即可得最终优化目标L。 图3：L对2个参数的求导。 最后由牛顿法得到迭代参数公式和优化目标L。 ​ 在之前计算的时候出现了奇异矩阵，即矩阵的行列式值为0，想了下可能是之前的数据之间存在线性相关性，所以导致矩阵的秩不满秩，所以也就不可逆了，即变成了奇异矩阵。若我不做实验，就不会知道奇异矩阵，也就记不起来以前的秩了。想着一些小算法，可以重复造轮子，但是复杂的算法就算了，没必要。人生奇美，怎可废！ ​ 2.python3实现用的python3.5.1+pycharm,将就着看吧。只用了5个样本，准确率不高的，简单的实现了预测，没有深究。下面是伪代码： 上面的终止条件就是人为设定的次数或者是优化目标L达到可接受的范围，否则一直迭代。 具体实现如下,代码在这里排列的不好，但是可以运行： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from numpy import *from math import *import numpy as npfrom numpy.linalg import invdef sigmoid(x): return 1./(1.+np.exp(-x))def logistic_regression(): # 牛顿方法构建Logistic Regression , 对西瓜是否为好瓜或坏进行分类, #Y &lt;&lt;--预测为-- Sigmoid(w1*x1+w2*x2 + bias) X = np.array([[0.697,0.774,0.634,0.666,0.243], [0.46,0.376,0.264,0.091,0.267], [1,1,1,1,1]]) Y =np.array([1,1,1,0,0]) beta = np.array([[0], [0], [1]]) maxCycles = 20 #迭代次数 j = 0 #累积次数 old_L = 0 while j &lt;= maxCycles: #循环20次后者损失函数已经达到合理小的范围就停止迭代 beta_T_x = dot(beta.T[0], X) # 计算β_TX L = 0 # 损失函数 for i in range(5): L = L +(-Y[i]*beta_T_x[i] + log(1 + exp(beta_T_x[i]))) if abs(L-old_L)&lt;= 0.00001: break print('最优化目标L=',L) old_L = L grad = 0 # 一阶导数 H = 0 # 二阶导数 # 计算5个训练样本的优化目标,公式（3.27) for i in range(5): # 梯度: ∂L(β)/∂β，P60公式（3.30）。 grad = grad - dot(array([X[:,i]]).T,(Y[i] - array([[exp(beta_T_x[i])/(1+exp(beta_T_x[i]))]]))) #X[i]是横向量，需要转制，grad结果是1个数 H = H + dot(array([X[:,i]]).T,array([X[:,i]]).T.T)*((exp(beta_T_x[i])/(1+exp(beta_T_x[i]))) * (1-exp(beta_T_x[i])/(1+exp(beta_T_x[i])))) beta = beta - dot(inv(H),grad) #参数迭代公式（3.29） j += 1 #预测部分 print('beta=', beta) x1 = float(input('请输入西瓜的密度:')) x2 = float(input('请输入西瓜的含糖率:')) z = beta[0][0] * x1 + beta[1][0] * x2 + beta[2][0] print('预测值为：',sigmoid(z)) f = sigmoid(z) if(f==1): #P66公式（3.47)正、反例的比值&gt;观测的正反比例即认为正例，否则反例 print('分类器预测此为好瓜。') else: print('分类器预测此为坏瓜。')logistic_regression() 预测结果如下：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Logistic Regression</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bonjour!]]></title>
    <url>%2F2018%2F09%2F13%2FSVM%2F</url>
    <content type="text"><![CDATA[1.SVM总体简单介绍首先，分类的时候我们通常有时候遇到一个分类情况是，多类数据交融，无法通过简单的线性分类器区分。那SVM（Support Vector Machine）就是把数据映射到高维，然后根据支持向量找出一个最佳分类面，使得数据可以分开。这就是支持向量机干的事情。参考下面视频： 2.硬间隔SVM首先清楚四个概念：超平面，函数间隔，几何间隔，支持向量。 （1）超平面在二维空间里面是：Ax+By+C=0 ,是一条直线。在三维空间里面是：Ax+By+Cz+D=0，是一个平面。在三维以上：Ax1+Bx2+Cx3+Dx4+Ex5+Fx6+….+K=0 =&gt; 可看作是2个n维向量W和X的内积：$W^TX+b = 0$,$b$可以认为是截距，是代表了到原点的距离；W是法向量，代表了超平面的方向。这就是它的定义了。 （2）函数间隔定义为：预测函数与实际类标的乘积。我们下面提到的都是简化版的情况，即只讨论在平面情况下的分类。有一些类标$y^i$,只有正例和反例：1或-1，预测的值是：$W^TX^i+b$，函数间隔就是把这2个乘起来： \hat{γ}=y^{i}(W^{T}X^{i}+b)，γ上面有一个hat的就表示函数间隔。发现$y^i$和 $(W^TX^i+b)$是同号的话就是分类正确了。如果真实类标$y^{i}=1$，要令$(W^{T}X^{i}+b)$远大于0，这样才越准确，如果真实类标$y^{i}=-1$,则要$(W^{T}X^{i}+b)$远远小于0才越准确。就是说函数间隔可以看作是描述分类准确与否的一个指标。 （3）几何间隔L几何间隔对比函数间隔是一个更好的指标，因为，若超平面是$W^TX+b = 0$的情况下，同时放大W和b的值为原来的n倍，后发现，函数间隔$y^{i}(W^{T}X^{i}+b)$变大了 n倍，但是超平面$W^TX+b=0$没有变化，这就是函数间隔的缺点。先来看看下面的图片：可以看到，超平面L把数据很好的分开来，其中有某个样本$A(x_i,y_i)$，$B$点在超平面上面，单位向量$W$除以自己的模，得到单位长度为$\frac{W}{||W||}$。有一个$γ^i$与向量AB的长度一样，则$B$点位置的$x$就等于$A$点的$X_i$减去$AB$的长度（$AB$带有方向性），而$AB=γ_i\cdot\frac{W}{||W||}$，即有：$x = x_i - γ_i\cdot\frac{W}{||W||}$,又因为$B$点的$x$在超平面L上，满足L的表达式，于是有： W^{T}(X^{i}-γ^{i}\frac{W}{||w||})+b=0简化提出$γ_i$后得到几何间隔的表达式： γ^{i}=y^{i}[\frac{W}{||w||}X^{i}+\frac{b}{||w||}]可以知道，2点：第一：当$||W||=1$，$\hat{γ^{i}}=γ^{i}$第二：几何间隔就等于函数间隔除以一个$||W||$：$γ^{i}=\frac{\hat{γ^{i}}}{||w||}$ （4）支持向量SVM的目标是在多个分类中找到最佳的超平面，看上面的图，外面的直线上面的点，就是支持向量，这些点与超平面最近且与超平面保持一定的函数距离。中间的虚线就是我们要找的最佳的超平面，这样的超平面可以确保就算是有些点在直线上面或者里面，也可以很好地区分。 3.最优间隔分类器问题从上面图可知，2条直线之间的距离越大越好，即几何间隔越大越好，这样可以使更多的数据很好的分开来。同时，找到唯一的最好超平面，所有样本点与超平面有一定的函数间隔（即分类准确）： \begin{align*} &\max_{w,b,γ^{i}}\quad \frac{\hat{γ}}{||w||}\\ & \begin{array}{r@{\quad}r@{}l@{\quad}l} s.t.&y(W^{T}X+b)>=\hat{γ}\\ \end{array} \end{align*}发现上面的优化问题是非凸优化，所以为了简化问题，思路是先把此问题转化为对偶问题，然后通过求解对偶问题的解，来解出原问题的解。这就是基本思路。为了简化计算，可以把函数间隔$\hat{γ}$令为1，因为函数间隔扩大n倍时，底下的||W||也扩大n倍，所以并不影响优化。上面分母变成常数1后，问题可以转为最小化分子的问题。||W||加了1/2的平方是便于后面求导，无碍： \begin{align*} &\min_{w,b}\quad \frac{1}{2}||w||^{2}\\ & \begin{array}{r@{\quad}r@{}l@{\quad}l} s.t.&y^{i}(W^{T}X^{i}+b)>=1\\ \end{array} \end{align*}根据凸优化理论，该问题可以通过拉格朗日函数将目标函数转化为无约束的目标函数进行求解： L(w,b,α)=\frac{1}{2}||w||^{2}-\sum\limits_{i=1}^{m}α_{i}(y^{i}(W^{T}X^{i}+b)-1)L对αi和b分别求导并令为0后得： W=\sum\limits_{i=1}^{m}α_{i}y^{i}x^{i}\sum\limits_{i=1}^{m}α_{i}y^{i}=0然后把上面的2个式子代入L： L=\sum\limits_{i=1}^{m}α_{i}-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^my^{i}y^{j}α_{i}α_{j}\quad\quad(1)可以令上面L为新的W(α)函数，后面的尖括号里面的就是xi和xj的内积表达式。 4.对偶问题4.1原始问题先来看原始问题，什么是原始问题？假设$f(x)$,$c_i(x)$,$h_j(x)$是定义在$R^n$上的连续可微函数，考虑约束最优化问题时候： \begin{align} ​ \min_{x \in R^n}\quad &f(x) \\ ​ s.t.\quad&c_i(x) \le 0 , i=1,2,\ldots,k\\ ​ &h_j(x) = 0 , j=1,2,\ldots,k ​ \end{align}上面就是一个有约束条件的原始问题。我们一般的思路就是一脑子求导=0，然后解出x代入原式即可(因为极值处的斜率为0 )。可是这里有约束条件，怎么办？于是拉格朗日先生便一跃而出，他说可以干掉这个纸老虎。看看他怎么解决的？ \mathcal{L}(x,\alpha,\beta) = f(x) + \sum_{i=1}^k \alpha_ic_i(x) + \sum_{j=1}^l \beta_jh_j(x)没错，他也是一股脑的把所有式子塞到一起，起个名字叫做广义拉格朗日函数。其中的α和β都是参数，特别要求αi&gt;=0。直接把上面式子求导代入原式即可。就是上面公式(1)的求解结果。发现上面的式子若不满足约束条件，就至无穷大了，若满足约束条件，则令L的最大值为： \theta_P(x) = \max_{\alpha,\beta:\alpha_i \ge 0}\mathcal{L}(x,\alpha,\beta)发现原始问题就等价于最小化上面的式子： \min_{x}max \mathcal{L}(x,\alpha,\beta)=\min_{x} \theta_P(x)=\min_{x} f(x)\quad\quad(2)因为αi&gt;=0,ci(x)&lt;=0,h(x)=0，所以θp的最大值是f(x)。 4.2对偶问题上面先最大化再最小化问题，对偶问题就是先最小化再最大化问题，即对偶问题为： \max_{\alpha,\beta:\alpha_i\ge0}\theta_D(\alpha,\beta)=\max_{\alpha,\beta:\alpha_i\ge0}\min_x\mathcal{L}(x,\alpha,\beta)\quad\quad(3)在一般情况下公式(2)要&gt;=公式(3),例如，对于同一个问题，x∈{0,1},y∈(0,1)，min max(1(x=y))&gt;=max min(1(x=y)),这里的1(x=y)为示性函数:若括号里的为真，则返回1，否则返回0。左边最大的1=1，再取最小值为1，右式最小的0=0，再取最大的值为0,1&gt;0，故可以看出左式&gt;=右式。感兴趣的可以自己搜下证明。那么，什么时候等式才成立呢？这是个好问题，答：满足KKT条件的时候。这样，我们就可以把原始问题转化为对偶问题，通过解出对偶问题的参数，也就解出原始问题的部分参数，这样就简化了计算。 4.3KKT条件对于KKT条件,由定理：对于原始问题和对偶问题，假设函数$f(x)$和$c_i(x)$是凸函数，$h_i(x)$是仿射函数（即由一阶多项式构成的函数，f(x)=Ax + b, A是矩阵，x，b是向量）；并且假设不等式约束$c_i(x)$是严格可行的，即存在x，对所有i有$c_i(x)&lt;0$，则x*,α*,β*分别是原始问题和对偶问题的最优解的充分必要条件是x*,α*,β*满足下面的Karush-Kuhn-Tucker(KKT)条件： \begin{align} & \nabla_xL(x^*,\alpha^*,\beta^*)=0\\ &\nabla_\alpha L(x^*,\alpha^*,\beta^*)=0\\ &\nabla_\beta L(x^*,\alpha^*,\beta^*)=0\\ &\alpha_i^*c_i(x)=0,i=1,2,\ldots,k\quad\quad(a)\\ &\alpha_i^*\ge0,i=1,2,\ldots,k\quad\quad\quad\quad(b)\\ &c_i(x)\le0,i=1,2,\ldots,k\quad\quad\quad (c)\\ &h_j(x^*)=0,j=1,2,\ldots,l\quad\quad\quad(d) \end{align}前三个求偏导是保证驻点的存在，后面的式子(a)、(b)是拉格朗日乘子需要满足的约束，(c)、(d)是原始问题的约束条件。特别注意当αi*&gt;0时，由KKT对偶互补条件可知:ci(x)=0，这个后面会用到。 5.软间隔SVM第3章节主要讲的是硬间隔最大化，那什么是软间隔最大化？硬间隔呢就是支持向量不能超过第二幅图的直线，顶多在线上面，但是软间隔可以容忍少量的支持向量落在直线内，于是我们的约束条件$y_i(W^TX+b)\ge1$就要修改为$y_i(W^TX+b)\ge1-ξ_i$，即允许部分支持向量间隔不足1,$ξ_i&gt;=0$。我们原始的最大化约束问题转为： \begin{align} \min_{w,b}\quad &\frac{1}{2}||w||^{2}+C\sum\limits_{i=1}^{m}ξ_i\\ s.t.\quad&y^{i}(W^{T}X^{i}+b)\ge1-ξ_i\\ &ξ_i\ge0 \end{align}由于加了松弛变量ξi，优化目标后面也加上惩罚项，其中C值大，对误分类的惩罚越大。前面的平方项希望函数间隔越大，后面的惩罚项希望误分类的样本点越少。通过拉格朗日函数将目标函数转化为无约束的目标函数进行求解： \mathcal{L}(w,\alpha,\beta) = \frac{1}{2}||w||^{2} + C\sum\limits_{i=1}^{m}ξ_i-\sum_{i=1}^m \alpha_i[y^{i}(W^{T}X^{i}+b)-1+ξ_i] -\sum_{j=1}^mu_jξ_i\quad\quad(4)把上面L函数分别对W, b, ξi求偏导=0可得： w = \sum_{i=1}^mα_iy_ix_i\quad\quad(5) \sum_{i=1}^mα_iy_i=0 C - α_i - u_i = 0把上面三个式子代入公式（4），可得到我们的对偶问题的优化目标为： \begin{align} \max_\alpha\quad &\sum\limits_{i=1}^{m}α_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^my_iy_jα_iα_j\\ s.t.\quad&\sum_{i=1}^mα_iy_i=0\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad(6)\\ & 0\leα_i\le C,i=1,2,\dots,m \end{align}有上式可求出α，再由α求出W和b。公式（5）可求出W，而b是所有支持向量满足$y_i(W^TX_i+b)=1$的值b的均值。由KKT条件： α_i[y^{i}(W^{T}X^{i}+b)-1+ξ_i]=0\\ u_iξ_i=0上面第一个式子正是由于αi&gt;0,由KKT推断$y_i(W^TX_i+b)-1+ξ_i=0$所得。可知：满足$0\leα_i\le C$的样本点$(x_s,y_s)$，即为支持向量。详细请点击底部的参考1。 5.1核函数对于线性可分数据用硬间隔和软间隔可以区分数据，但对于非线性的数据怎么办？于是，我们的核函数粉墨登场（这里引用原意：演员化妆演戏）。它干的事情直观感受就是上面最开始的视频里干的部分事：把非线性的数据映射到线性可分的维度里面。来，举个栗子八。就是我们有一组非线性的2维模型： f(X_1,X_2) = A_0+A_1X_1+A_2X_2+A_3X_1^2+A_4X_2^2+A_5X_1^3+A_6X_2^3若令： X_1=X_1,X_2=X_2,X_1^2=X_3,X_2^2=X_4,X_1^3=X_5,X_2^3=X_6则可以转化为6维模型： f(X_1,X_2，X_3，X_4，X_5，X_6) = A_0+A_1X_1+A_2X_2+A_3X_3+A_4X_4+A_5X_5+A_6X_6就是说：低维度不可分的数据模型可以映射到高维度后，变成线性可分的模型。下面的核函数就是干的这个事情，就是有核函数k(x,z)把低维度的φ(xi)和φ(zj)之间的内积在低纬度计算掉： k(x,z)=φ(x)φ(z)常见的核函数有： （1）高斯核函数（Gaussian Kernel） \kappa(x, x_i) = exp(-\frac{||x - x_i||^2}{\delta^2})高斯径向基函数是一种局部性强的核函数，其可以将一个样本映射到一个更高维的空间内，该核函数是应用最广的一个，无论大样本还是小样本都有比较好的性能，而且其相对于多项式核函数参数要少。 （2）线性核函数（Exponential Kernel） \kappa(x,x_i) = x \cdot x_i线性核，主要用于线性可分的情况，我们可以看到特征空间到输入空间的维度是一样的，其参数少速度快，对于线性可分数据，其分类效果很理想，因此我们通常首先尝试用线性核函数来做分类，看看效果如何，如果不行再换别的。 （3）多项式核函数（Polynomial kernel function） \kappa(x, x_i) = ((x\cdot x_i) + 1)^d多项式核函数可以实现将低维的输入空间映射到高纬的特征空间，但是多项式核函数的参数多，当多项式的阶数比较高的时候，核矩阵的元素值将趋于无穷大或者无穷小，计算复杂度会大到无法计算。这几个核函数可以优先使用高斯核函数，据说Andrew Ng只用高斯核函数，可见性能确实优越。当然，还是要根据自己的模型找最适合的核函数。还有很多核函数就不介绍了，感兴趣的自行搜索。 6.SMO算法6.1求解$α_1,α_2$了解了核函数后，上面式子（6）对偶问题的内积换成核函数后，提个负号出来整体变为最小化问题就等价于： \begin{align} \min_\alpha\quad &\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^my_iy_jα_iα_jk(x_i,x_j)-\sum\limits_{i=1}^{m}α_i\\ s.t.\quad&\sum_{i=1}^mα_iy_i=0\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad(7)\\ & 0\leα_i\le C,i=1,2,...,m \end{align}SVM的“核“是什么呢？就是SMO（Sequential Minimal Optimization）算法，他是用来求α的，前面说过：求出了α，就可以求出W和b,接着超平面也就找到了。它的主要思想就是：α有m个变量，但是直接根据优化问题来求解很困难。所以，SMO算法每次只对2个变量更新，其他变量看成是常数，而常数项可以在优化目标中省略。 上面的是具体α求解步骤，稍微讲解一下，公式展开来自公式（7）。min后面优化目标这么长，不要吓坏了，只是把α1和α2变量部分展开来，其余看作常数。下面的限制条件原本是$\sum_{i=1}^mα_iy_i=0$，因为$y_i$只能取-1和1，所以就看作是m个正负$α_i$相加=0，拿出2个变量：α1和α2出来玩玩，其余的和为常数-k。需要注意的是2个相同$y_i$的乘积为1，这里都会用到以简化计算。然后得到第1部分的优化目标和约束条件。(图中的$α_2^{new,uncut}$不对，应该是$α_2^{new,unc}$，后面的其实是unclip) 第2部分，是讲解的α的整个迭代过程。过程上面图片有，讲一下L和H的范围是怎么来的。以左边双红线图为例：α本身就有限制[0,C]，故有图中的正方形。坐标轴的横坐标是α1，纵坐标是α2,由α1-α2=K可得2条红色的线，因为已经限制了α1和α2的范围，只能在方框中。可得到线①、线②。可知线①的α2范围是[-K,C]，线②的α2范围是[0,C-K]，再把α1-α2=K代入即可得到L，H的范围。另一种情况也一样可得。 这一步主要是求解未经剪辑的α2，思路是先简化优化目标函数W(α1，α2)，然后代入α1和α2的关系式消去α1，使得表达式全部是关于α2。接着W（α2）对α2求导=0即可。最后可以得到迭代式子： α_2^{new,unc} = α_2^{old} + \frac{y_2(E_1-E_2)}{k_{11}+k_{22}+k_{12}}6.2求解b当$0&lt;α_1^{new}&lt;C$时,根据KKT条件$y_i(W^TX_i+b)=1$: y_1(\sum_{j=1}^{m}α_jy_jk_{1j}+b_1)=1\\ y_1\cdot y_1(\sum_{j=1}^{m}α_jy_jk_{1j}+b_1)=1\cdot y_1 \\ 即为：\sum_{j=1}^{m}α_jy_jk_{1j}+b_1-y_1=0\\ 迭代得：b_1^{new}=y_1-α_1^{new}y_1k_{11}-α_2^{new}y_2k_{12}-\sum_{j=3}^{m}α_jy_jk_{1j}\\ 本轮迭代中E_1: E_1=g(x_1)-y_1\\ y_1代入上面的b_1^{new}可得：\\ b_1^{new}=b^{old}-E_1+y_1k_{11}(α_1^{old}-α_1^{new})+y_2k_{12}(α_2^{old}-α_2^{new})\\ b_2^{new}同理：\\ b_2^{new}=b^{old}-E_2+y_1k_{12}(α_1^{old}-α_1^{new})+y_2k_{22}(α_2^{old}-α_2^{new})\\ 故：b^{new}=\frac{b_1^{new}+b_2^{new}}{2}最后更新Ei为下一轮迭代做准备: E_i = \sum_{S}α_iy_ik_{ij}+b^{new}-y_i最终的b是： b = y_s-\sum_{S}α_iy_ik(x_i,x_s)即所有支持向量的b的均值即为最终的b值。 6.3求分类超平面即为： \sum_{i=1}^mα_iy_ik(x_i,x_s)+b=07.总结SMO算法部分强烈推荐参考：点击此处，我的多数都是来自于此。这篇讲解的非常仔细，比我不知高到哪里去了。最后参考链接第5个是他这个系列的总体，一共5部分，包括代码实现。SVM优点：泛化错误率低，计算开销不大，结果易解释。缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。适用数据类型：数值型和标称型数据。 非常感谢那些乐于分享知识的人。这篇写了整整2天，算上搭建blog的话有一个礼拜了，也算是对最近的一个梳理和总结吧。最后，推荐一款Markdown编辑器：Typora,做到了所谓的“所见即所得”，这是我听过的世界上最好的宣传语(≧∇≦)ﾉ，Au revoir, chère amie。 参考链接： 1.支持向量机Part2—线性支持向量机 2.svm常用核函数 3.从超平面到SVM（三） 4.最优间隔分类器问题 5.支持向量机Part5—Python实现]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>SVM</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
