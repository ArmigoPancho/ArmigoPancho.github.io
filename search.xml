<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Seq2Seq简介]]></title>
    <url>%2F2019%2F09%2F08%2FSeq2Seq%2F</url>
    <content type="text"><![CDATA[1.Seq2Seq（1）简介sequence to sequence模型是一类End-to-End的算法框架，也就是从序列到序列的转换模型框架，应用在机器翻译，自动应答等场景。 基本思想就是利用两个RNN，一个RNN作为encoder，另一个RNN作为decoder。encoder负责将输入序列压缩成指定长度的向量，这个向量就可以看成是这个序列的语义，这个过程称为编码，如下图，获取语义向量最简单的方式就是直接将最后一个输入的隐状态作为语义向量C。也可以对最后一个隐含状态做一个变换得到语义向量，还可以将输入序列的所有隐含状态做一个变换得到语义变量。 而decoder则负责根据语义向量生成指定的序列，这个过程也称为解码，如下图，最简单的方式是将encoder得到的语义变量作为初始状态输入到decoder的RNN中，得到输出序列。可以看到上一时刻的输出会作为当前时刻的输入，而且其中语义向量C只作为初始状态参与运算，后面的运算都与语义向量C无关。这里的C指的是中间那个箭头输出的向量C。 比如将法语翻译为英语句子，这里它使用一个左边的编码网络来输入法语句子并处理特征，然后用一个RNN网络作为解码网络来输出对应的英语句子序列。 （2）集束搜索（Bean Search）对于机器翻译来说，给定输入，比如法语句子，你不会想要输出一个随机的英语翻译结果，你想要一个最好的，最可能的英语翻译结果。肯能你会用贪心算法，找到与之最配的单词，但是英语中词和词之间的搭配实在太多所以每次都选出最优的词反而不好。那定向搜索或者叫集束搜索就是解决此问题的一种解决方案，它会每一次搜索产生多个词的搭配结果。其中有个参数B，叫做集束宽（beam width），网络就是每次产生的预测结果的个数。 绿色是编码部分（下图），紫色是解码部分，来评估第一个单词的概率值y1，给定输入序列X，即法语作为输入，第一个输出y的概率值是多少。执行集束搜索的第一步，你需要输入法语句子到编码网络，然后会解码这个网络，这个softmax层会输出10,000个概率值，得到这10,000个输出的概率值，取前三个存起来。即下图中10000个词中选出概率最大的三个词，就是我们的第一步结果。这里假设是”in、jane、september”三个词。 然后我们的目标是选出这三个词分别后面接的下一个概率最大的词是哪些？这里是已经有第一个词了，所以在解码器里面会把它作为输入，放到预测第二个词的RNN单元中，而词P（A+B）的概率简单分为P(A)、P（B）概率的乘积，以简化计算： p\left(y^{(1)}, y^{(2)} | x\right)=p\left(y^{(1)} | x\right) p\left(y^{(2)} | x, y^{(1)}\right)\tag1 这样最大化公式（1）即可，对于每一个词都这样做，就有3个RNN网络分别预测第二个词，对于维度，第二步有3*10000个单词，就是3万个概率值如图左显示，从这3万个组合中选出最大的3个概率值，假设分别是”in september”、”jane is”、”jane visits”，那”september”就会删除，只保留最大的三个词；接着就第三个词的预测，如下图所示，把前面已经确定的2个词作为输入，并且预测第三个词，就这样一直遇到终止符号”EOS”，就不再预测词。 当B即集束宽度为1时，算法性能约等于贪心算法。 （3）改进的集束搜索由于原始优化目标是每个词的连乘积，这样乘起来的数字是[0,1]，小数点会很大，对计算机来说会产生浮点数的下溢，不利于大规模词的计算，所以改成log就是每个词的概率相加，但是也会造成都是负数相加，数字就是一个很大的负数，所以把它归一化，通过除以翻译结果的单词数量。这样就是取每个单词的概率对数值的平均了，这样很明显地减少了对输出长的结果的惩罚。 其中长度T的次方α=0.7是一个超参，如果α等于1，就相当于完全用长度来归一化，如果α等于0，T_y的0次幂就是1，就相当于完全没有归一化，上图中的T_y就是在完全归一化和没有归一化之间。 关于B：B越大，你考虑的选择越多，你找到的句子可能越好，但是B越大，你的算法的计算代价越大，因为你要把很多的可能选择保存起来。B越小，效果越差，但是计算越快。一般对于产品系统来说，B=10的效果不错，100以上就太大了。 （4）集束搜索的误差分析大致是2种误差，一种是RNN选对了预测值，但是集束搜索没有找到它，反而选了更小的值。比如下图的Case 1，还有一种是Case 2：就是情况是$P（y^|x）$小于或等于$P（\hat{y}|x）$，case1和2中总有一个是真的。此例子中假设$y^$是比$\hat{y}$更好的翻译结果，不过case 2中根据RNN模型的结果反而搞反了，所以这就是RNN出现了问题，集束搜索没有问题。 统计这2类的错误情况，得出束搜索算法和RNN模型出错的比例是多少的一张表。当你发现是束搜索算法造成了大部分错误时，才值得花费努力增大集束宽度。如果你发现是RNN模型出了更多错，那么你可以进行更深层次的分析，来决定是需要增加正则化还是获取更多的训练数据，抑或是尝试一个不同的网络结构，或是其他方案。 参考：1.Seq2Seq简介：https://www.jianshu.com/p/b2b95f945a98 2.吴恩达视频：https://mooc.study.163.com/learn/2001280005?tid=2001391038#/learn/content?type=detail&amp;id=2001771063]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Seq2Seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[embedding]]></title>
    <url>%2F2019%2F09%2F05%2Fembedding%2F</url>
    <content type="text"><![CDATA[1.词汇表征（1）概念 最简单的是one-hot即独热码表示，但是缺点是向量之间没有关联性，因为每个向量之间的内积都是0，所以我们用词嵌入的表示来解决这个问题。设定每个词有300个特征，然后每个词对应的特征有一个相关度的衡量，这样每个词都有300维的一个特征向量来表示，并且，可以方便的计算每个向量之间的相关性等等，下图简单的表示词嵌入。 （2）可视化和为什么叫词嵌入？首先若把这些300维度的向量映射到2维，则可以看到向量在平面上的位置标示，看到下图的人、动物可以看作一个聚集，数字是在一起的，水果之间的更加相近，聚在一起，简单的可视化效果好点。 对于为什么叫词嵌入？很简单，若把300维度的看作是个高纬的空间，词就是镶嵌在这里面而已。 词嵌入还有一个优点是：当我们输入一个训练集中遇到一个未知的词“durian”（榴莲），词嵌入可以比较好的知道这个词和“orange”都是水果一类的词。这即是使用词嵌入的好处之一。 （3）词嵌入的特性如下图，有意思的是比如man-woman之间的值和king-queen之间的差值是一样的，或者是类似的。那当我们已知man - woman = king - ? ；去求解”?”处的时候就可以反着推导出来了。 再仔细思考下，其实问好处的向量，也可以等价于求下图中最下面最大化的式子，通过降到2维度可以比较好的展示出其实是2个可以平移后的重合或者非常相近的向量，但t-SNE的非线性映射这些方法中这种平行四边形的关系不会成立，维度太高。 SO，我们可以使用余弦相似度来表示这种相似性的关系：其实即向量之间的内积/各自向量的模的乘积 （4）嵌入矩阵我们要做的就是学习一个嵌入矩阵，它将是一个300×10,000的矩阵，如果你的词汇表里有10,000个，或者加上未知词就是10,001维。即下图中左上角的大矩阵。 假设这个嵌入矩阵叫做E矩阵，注意如果用E去乘以右边的one-hot向量（上图编号3所示），也就是O_6257，那么就会得到一个300维的向量，E是300×10,000的，O_6257是10,000×1的，所以它们的积是300×1的，即300维的向量。由于O_6257是独热码，所以乘积的结果就是orange那一列的向量。这样就把orange这个词用向量的形式表征了出来。 总的来说，就是嵌入矩阵E乘以一个独热码得到某个词的向量表征，实际操作的时候会有专门的函数提取E中的每个列，因为每次乘以独热码中的0计算慢且无效。 （5）学习词嵌入那词嵌入可以干什么？比如下面，可以先把句子词后编号，然后构造E得到对应的词向量输入神经网络，最后使用softmax预测，可以输入不同个数的词向量，得到多次预测结果，比如输入红色的2个向量预测的结果和下面黄色部分的预测结果，来选出概率最大的预测结果。 2.Word2Vec的简单铺垫（1）Word2Vec基本的理解之前的文章也有介绍Word2Vec，写的有缺点，Word2Vec有2种，一种是CBOW即周围词去预测中心词，一种是skip-gram即中心词去预测周围词。把上面小节（5）的模型简化为下图的方式：就是根据一个词c来推预测某一个词t的概率是多少？ 这里θ是一个与输出词 t 有关的参数（就是输出矩阵的参数，列向量转制），即某个词 t 和标签相符的概率是多少。我省略了softmax中的偏差项，想要加上的话也可以加上。然后分母是所有词的概率和，即输出当前预测词t的概率。 此优化对象的损失函数是： L(\hat{y}, y)=-\sum_{i=1}^{10,000} y_{i} \log \hat{y}_{i}$y_i$是只有一个1其他都是0的one-hot向量，类似的$\hat{y}$是一个从softmax单元输出的10,000维的向量，这个向量是所有可能目标词的概率。乘积就是每个单词的损失值，负号是log函数在底数大于1、x为0-1之间时候L值是负数，加负号利于惯性思维（即损失值越小越好）和优化。正好和红色的log函数相符，$\hat{y}$值越大，L损失值越小，然后最小化损失函数L即可。这里应该是y的转制乘以列向量log$\hat{y}$，否则不是数字了。 注意，这里由于分母很多向量求和导致计算量巨大，so采用一种分级（hierarchical）的softmax分类器和负采样（Negative Sampling），来加速优化。前面的大致就是说每一个结点都是一个softmax函数，把常见的高频词越放上面放置，目标词t在5k前面，还是在5k后面，若在5k前面，继续判别，发现目标词在2500后面…，二分法一直下去直到到达对应的目标词所在的softmax分类器，这样就遍历的快很多。 第二个负采样大致是对于我们的中心词c来说，常见词反而是我们不想要的，比如”the、of 、and、that”等等，你不会想要你的训练集都是这些出现得很频繁的词，因为这会导致你花大部分的力气来更新这些频繁出现的单词的、反而无意义。所以在采样中心词的时候，我们采用了不同的分级来平衡更常见的词和不那么常见的词。 （2）负采样构造一个新的监督学习问题，那么问题就是给定一对单词，比如orange和juice，我们要去预测这是否是一对上下文词-目标词（context-target），怎么做？这里是中心词一直去做预测，是orange后面的词就标记为1，否则为0，这样就得到了一个对应的0-1向量。0表示这个产生的目标词对于当前orange来说是负样本。 那么这样的过程应该做几次呢？小数据集的话，k从5到20比较好。如果你的数据集很大，就选的小一点。对于更大的数据集就等于2到5，数据集越小就越大。那么在这个例子中，我们就用4次。 接着，计算下图中的P的话每次需要1万维度的向量更新，计算量显然是巨大的，为了简化加快计算，每次负采样算法产生1个正样本和4（k）个负样本去做二分类，这样就简化了计算。 怎么产生负样本？一个办法是对中间的这些词进行采样，即候选的目标词，你可以根据其在语料中的经验频率进行采样，就是通过词出现的频率对其进行采样。但问题是这会导致你在like、the、of、and诸如此类的词上有很高的频率。另一个极端就是用1除以词汇表总词数v，即：$1/|v|$，均匀且随机地抽取负样本,这对于英文单词的分布是非常没有代表性的。所以论文的作者Mikolov等人根据经验，他们发现这个经验值的效果最好，它位于这两个极端的采样方法之间，既不用经验频率，也就是实际观察到的英文文本的分布，也不用均匀分布，他们采用以下方式： P\left(w_{i}\right)=\frac{f\left(w_{i}\right)^{\frac{3}{4}}}{\sum_{j=1}^{10,000} f\left(w_{j}\right)^{\frac{3}{4}}}即某个词频次的3/4次方在总的词相应频次和中的概率。这些大致就是skip-gram的内容。 3.GloVe词向量参考1：https://blog.csdn.net/littlely_ll/article/details/78941403 参考2：https://blog.csdn.net/cuipanguo/article/details/82864983]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>词嵌入、WordtoVec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LSTM简介]]></title>
    <url>%2F2019%2F09%2F03%2Flstm%2F</url>
    <content type="text"><![CDATA[1.LSTM简图在在序列中学习非常深的连接，LSTM（Long Short Term Memory）即长短时记忆网络，甚至比GRU更加有效。其实和GRU不同的是，LSTM是使用单独的更新门（update gate）、遗忘门（forget gate）、输出门（output gate），C和a也分开表示，不再一样。具体看下图。 下图是图形化的LSTM解释，发现LSTM在不同时序上会传递下去，比如更新门一直是0且遗忘门一直是1，那C的值和上一个时序的C值是一样的，即记忆细胞一直延续下去。LSTM与GRU的区别是使用了3个门来控制网络，更加复杂、性能也更好。 “窥视孔连接”（peephole connection）。虽然不是个好听的名字，“偷窥孔连接”其实意思就是门值不仅取决于上个时序t-1时候的a和t时刻的x，也取决于t-1时刻的记忆细胞值c。 2.双向神经网络简图（Bidirectional RNN）如下图所示，双向RNN中紫色的就是从做到右的传递信息，绿色部分就是从右到左的信息传递，期间还有向后传播（求偏导部分）的信息传递。比如在预测某个输出$\widetilde{y}^{}$时，前面X1、X2、X3和向右的三个a都传递到了第三个单元做预测，并且为了利用未来的信息，X4到向左的a4，再到向左的a3；这些未来的信息也传递给第三单元；再加上反向传播（求偏导部分），则最终预测出我们的$\widetilde{y}^{}$。 这些基本单元不仅仅是标准RNN单元，也可以是GRU单元或者LSTM单元。事实上，很多的NLP问题，对于大量有自然语言处理问题的文本，有LSTM单元的双向RNN模型是用的最多的。 3.Deep RNN(深层循环神经网络)深层RNN就是多个RNN并在一起，每一层都有共享的权重$W^{[2]}_a、b^{[2]}_a$，如何计算$a^{[2]}$的值： a^{[2]}=g(W^{[2]}_a[a^{[2]},a^{[1]}]+b^{[2]}_a)就是左边和下面结点的值传过去计算。由于时间的维度，RNN网络会变得相当大，即使只有很少的几层，很少会看到这种网络堆叠到100层。每一个单元可以是RNN、GRU、LSTM单元；也可以构建深层的双向RNN网络。由于深层的RNN训练需要很多计算资源，需要很长的时间，尽管看起来没有多少循环层，这个也就是在时间上连接了三个深层的循环层，你看不到多少深层的循环层。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RNN简介]]></title>
    <url>%2F2019%2F09%2F01%2FRNN%2F</url>
    <content type="text"><![CDATA[1.输入的数据和输出的数据如输入的数据和输出的数据是类似下图带有序列性质的数据，则标准的神经网络不好处理此类数据，因为输入数据和输出数据的长度不固定（填补0的方式不高效），而且并不共享从文本的不同位置上学到的特征。比如从位置1学到的Harry可能是人名的一部分，那么如果Harry出现在其他位置，比如位置t时，它也能够自动识别其为人名的一部分的话，这就是共享特征了。 2.RNN简图下图左右边都是循环神经网络的表示，左边更加详细。比如左边的输入就是每个单词$x^{}$，输出就是对应的$\hat{y}^{}$，中间每次回把上一个时间段的激活值$a$输入到下一个。 上面W参数的解释，其中$a^{}$是初始化的激活值，一般为0向量： 循环神经网络的缺点是：它只使用了这个序列中之前的信息来做出预测，若预测第三个词，则RNN会用到前面所有词的信息来帮助预测，但是后面的词并没有用到。 2.1 向前传播 g函数可以是S型函数或者tanh函数或者softmax函数，因需求而定。 下图是Andrew Ng展示简化RNN的过程，就是矩阵相乘，不过是矩阵拼凑起来的，简化矩阵个数，并且Wa，ba下标a表示用来计算变量a，第2个公式Wy、by的下标y表示这些参数是计算y的： 2.2向后传播 绿色的线是向前传播，红色的线就是反向传播的方向。每个对时间段t的损失函数是： L^{}(\hat{y},y)= -y^{}log\hat{y}^{}-(1-y^{})log(1-\hat{y}^{})每个时刻相加起来就是总的损失函数L： L(\hat{y},y)= \sum_{t=1}^{T_x}L^{}(\hat{y}^{},y^{})RNN反向传播示意图： 而图中最有趣的向后传播是叫做”时光倒流”的红色圈圈部分，因为这部分可以看作是顺序时间的逆向传播，正如吴恩达所说，就像是乘着一艘时光倒流的Machine穿梭于各个奇妙之间。真的很酷，这名字，我也喜欢，写到这里，我想我真的有点喜欢ML，但是自己基础薄弱，缺乏毅力，渐渐地失去自我，也想乘着这艘Machine时光倒流，可是，不会了，永远也不会了。不忏悔了，回忆录、忏悔录这世界上还少吗，不，只多不少。所以，Make your hands dirty. 2.3RNN的例子RNN可以有多对多，即输入输出都是多个但是下图中的多对多的输入和输出是一样的长度常用在命名实体识别中，也有多对1是情感分类，1对1是当去掉$a^{}$时它就是一种标准类型的神经网络，1对多是音乐生成或者序列生成，还有多对多的不同长度，常用在机器翻译，如下图： 2.4语言模型的例子语言模型可以计算某种句子的概率，比如下图输入初始化的a0=0和X1=0，然后第一个单词”Cats”，作为第二个输入得到预测概率$P(average|Cats)$，然后继续输入第二个单词”average”，得到一个输出概率$P(15|”Cats\ average”)$，然后依次计算后面的预测概率。把所有预测的概率相乘即为句子的总体概率，也可以根据L求出损失值。具体如下： 2.5 对新序列采样2.5.1基于词汇进行采样的模型在你训练一个序列模型之后，要想了解到这个模型学到了什么，一种非正式的方法就是进行一次新序列采样。 输入a=0，x1=0,然后输出softmax对第一个词的概率分布，随机采样一个即可。然后每一个采样出来的结果放到后面去作为输入，直到遇到停止条件。 停止条件为： （1）一直在字典集中进行采样，直到得到标识，这表明可以停止采样了。（2）如果在时间步的结尾，字典中没有这个词，你可以从20个或100个或其他单词中进行采样，然后一直抽样下去，直到达到设定的时间步。但是如果在此过程中又出现了 unknown word,则可以进行重采样，直到得到一个不是一个未知标识的词。如果不介意UNK的话，也可以忽略这些未知的单词。 这就是你如何从你的RNN语言模型中生成一个随机选择的句子。 2.5.2基于字符进行采样的模型基于字符的语言模型就是训练时输入的是句子中每个字符，比如生成的“Cats average 15 hours of sleep a day.”，那么$\hat{y}^{}$是字符’C’，然后$\hat{y}^{}$是字符’a’，然后每个字符都是后面的输入，直到结束。但是基于字符的采样模型缺点是计算太复杂、不利于捕捉长范围的关系(词汇的字符通常多于1个)、采样的模型会得到过长的序列，优点是不会出现未知的标识。 2.6梯度消失、梯度爆炸梯度消失例子： THe cat, which already ate a bunch of food that was delicious ….was full.THe cats, which already ate a bunch of food that was delicious ….were full.注意句式中的单复数问题，cat作为主语，则使用was,cats作为主语，则使用were. 主语和谓语的单复数关系，因为有从句(即主语和谓语之间的距离可以足够长)的关系，在时间上往往具有 很长时间的依赖效应(long-term dependencies) 但是普通的RNN并不擅长捕捉这种长期依赖效应(因为具有梯度消失的问题)。 梯度爆炸： 梯度不仅仅会指数级别的下降，也会指数级别的上升–即梯度爆炸(exploding gradients),导致参数的数量级及其巨大会看见许多NaN或者不是数字的情况–这意味着网络的计算出现了数值溢出。 如果出现了梯度爆炸的问题，一个解决方法就是使用梯度修剪(gradient clipping).即–设置一个梯度的天花板，梯度不能超过这个阈值，并对梯度进行缩放。 对于梯度消失和梯度爆炸比较好的理解如下图： 3.门控循环神经网络（GRU） Gate Recurrent Unit即GRU网络改变了RNN的隐层结构，可以捕捉深层连接，并改善了梯度消失问题。简单的讲就是根据门值和记忆细胞的值来判断是否更新记忆细胞的值，以此来优化梯度消失的问题。 下图是RNN单元的简图： GRU简介： 比如在句子：”The cat , which already ate …, was full”，为了记住话语中cat是单数还是复数，在时间T上需要使用记忆细胞memory cell 记住cat这个单词，并且同一个时序的C和a都是相等的。 在每一个时间步t,都将用一个候选值$\breve{c}^{}$重写记忆细胞的值 而： \breve{c}^{}=Tanh(W_{c}[c^{},x^{}]+b_{c}) \tag1 GRU中有一个门Gate($\Gamma_u$) 这是一个0到1之间的值： \Gamma_u=\sigma(W_{u}[c^{},x^{}]+b_{u}) \tag2(1)式是记忆细胞C的候选值，比如记录了复数为1，单数为0；(2)式计算的值用以控制是否采用(1)式进行更新 在此例中如果cat变为cats ，用公式（1）的结果控制was和were的值，而公式（2）的更新门用以控制 在何时将was变为were，即$\Gamma_u=1$就使用公式(1)更新值C，否则C值不变: C^{}=\Gamma_u * \breve{C}^{}+(1-\Gamma_u) * C^{}\tag3]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>GRU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[栈和堆]]></title>
    <url>%2F2019%2F08%2F09%2Fstack%2F</url>
    <content type="text"><![CDATA[1.栈（Stack）栈的定义如下图： top（栈顶：是单链表尾部，在栈中是顶部） button（栈底：单链表头部，在栈的底部） 入栈就是栈的插入操作：刚开始top和button是重合的，然后a元素入栈，top指向a，再插入一个元素b则top指向当前栈顶元素b，button指向栈底元素a；若还有元素依次往上面插入。 出栈是：栈的删除操作。出栈时候是从从网下出栈：即后进先出（last in first out）。 1.1 栈的顺序存储结构栈的顺序存储结构定义如下，注意入栈出栈都是同一个位置。 1.2栈的链式存储结构因为栈插入和删除都在栈顶，所以为了方便，采用链式存储结构来操作更好。即把栈顶指针和单链表的头部指针合二为一。栈一般采用顺序存储结构。 2.栈和堆的区别2.1栈和堆区别栈（操作系统）：由操作系统（编译器）自动分配释放 ，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。 堆（操作系统）： 一般由程序员分配释放， 若程序员不释放，程序结束时可能由OS回收，分配方式倒是类似于链表。 2.2题目1.设栈S和队列Q是初始状态为空,元素E1,E2,E3,E4,E5,E6依次通过栈S,一个元素出栈后即进入队列Q,若6个元素出列的顺序为E2,E4,E3,E6,E5,E1,则栈S的容量至少应该是（）.答案请点击此处链接]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>栈和堆</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L1正则为什么比L2稀疏？]]></title>
    <url>%2F2019%2F08%2F04%2Fregularization%2F</url>
    <content type="text"><![CDATA[知乎上看到的，直接贴出来： https://www.zhihu.com/question/37096933/answer/70507353 ；解释了为什么L1正则要比L2正则稀疏。 图直接上来，分别从优化角度、梯度角度、概率的角度思考： 其他正则讲解的参考： 点击此处；若侵权则删除。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>L1、L2正则</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GAN模型简介]]></title>
    <url>%2F2019%2F08%2F01%2FGAN%2F</url>
    <content type="text"><![CDATA[1.GAN模型半监督学习里面提到GAN（Generative Adversarial Network）模型了，如下图所示，通俗滴说就是一个生成模型一个判别模型在对抗，比如生成模型是学习一幅蒙娜丽莎的画作，生成模型就生成一幅类似蒙娜丽莎的画，然后判别模型就去判别此画像不像原画，若不像再重新学习，一直到判别模型说：嗯，你画的够像了，我已经分不清真画假画了，才停止训练。生成模型和判别模型都是在成长的。 2.最大似然估计（Maximum Likelihood Estimation）最大似然估计（MLE）假设样本之间都是独立同分布的，并且最大似然估计假设模型已定、参数未知。 3.GAN的损失函数其中,x 是真实样本,G(z) 是 G 生成样本。我们希望 D(x) 越大越好,D(G(z)) 越小越好。损失函数的前红色部分是判别模型D判别x为真画中样本的概率（判别为真画的期望，越大越好），后面绿色线部分是生成模型G把数据z生成的期望，即伪造噪音数据z而产生的损失，越小越好。总的来说就是生成模型的伪造数据z判别为真画的概率越大越好。 参考 李宏毅GAN视频：https://www.youtube.com/watch?v=0CKeqXl5IY0&amp;feature=youtu.be]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>GAN模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[降维算法简介]]></title>
    <url>%2F2019%2F07%2F23%2FPCA%2F</url>
    <content type="text"><![CDATA[1.PCA介绍1.1概念基础对样本降维，较为简单的做法就是对原始高纬空间进行线性变换。比如给定样本$X=(x_1,x2,…,x_m)∈R^{d\times m}$；降低到$d\ ^{‘}$后： Z = W^TX$z_i=W^Tx_i$即看作每一个原始样本与变换矩阵W做内积而得，且新空间的属性是原空间中的属性的线性组合。 1.2 PCA算法（Principal Component Analysis）主成分分析（PCA）是常用的降维方法之一。 原样本点$x_i$和降维后样本点$\hat{x}_i$的距离为： \begin{aligned} \sum_{i=1}^{m}\left\|\sum_{j=1}^{d^{\prime}} z_{i j} \boldsymbol{w}_{j}-\boldsymbol{x}_{i}\right\|^{2} &=\sum_{i=1}^{m} \boldsymbol{z}_{i}^{\mathrm{T}} \boldsymbol{z}_{i}-2 \sum_{i=1}^{m} \boldsymbol{z}_{i}^{\mathrm{T}} \mathbf{W}^{\mathrm{T}} \boldsymbol{x}_{i}+\text { const } \\ & \propto-\operatorname{tr}\left(\mathbf{W}^{\mathrm{T}}\left(\sum_{i=1}^{m} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\mathrm{T}}\right) \mathbf{W}\right) \end{aligned}上面式子如何推导？由式子$z_i=W^Tx_i，W^TW=I$代入上式，其中$\sum_{j=1}^{d^{\prime}} z_{i j} \boldsymbol{w}_{j}$就是降维后样本$\hat{x}_i$的对应特性向量$z_i$，$z_i^Tz_i$就是内积，也就是二范数的平方第1项${(\sum_{j=1}^{d^{\prime}} z_{i j} \boldsymbol{w}_{j})}^2$： \begin{aligned} \sum_{i=1}^{m}\left\|\sum_{j=1}^{d^{\prime}} z_{i j} \boldsymbol{w}_{j}-\boldsymbol{x}_{i}\right\|_{2}^{2} &=\sum_{i=1}^{m}\left\|\boldsymbol{W} \boldsymbol{z}_{i}-\boldsymbol{x}_{i}\right\|_{2}^{2} \\ &=\sum_{i=1}^{m}\left(\boldsymbol{W} \boldsymbol{z}_{i}\right)^{T}\left(\boldsymbol{W} \boldsymbol{z}_{i}\right)-2 \sum_{i=1}^{m}\left(\boldsymbol{W} \boldsymbol{z}_{i}\right)^{T} \boldsymbol{x}_{i}+\sum_{i=1}^{m} \boldsymbol{x}_{i}^{T} \boldsymbol{x}_{i} \\ &=\sum_{i=1}^{m} \boldsymbol{z}_{i}^{T} \boldsymbol{z}_{i}-2 \sum_{i=1}^{m} \boldsymbol{z}_{i}^{T} \boldsymbol{W}^{T} \boldsymbol{x}_{i}+\sum_{i=1}^{m} \boldsymbol{x}_{i}^{T} \boldsymbol{x}_{i} \\ &=\sum_{i=1}^{m} \boldsymbol{z}_{i}^{T} \boldsymbol{z}_{i}-2 \sum_{i=1}^{m} \boldsymbol{z}_{i}^{T} \boldsymbol{z}_{i}+\sum_{i=1}^{m} \boldsymbol{x}_{i}^{T} \boldsymbol{x}_{i} \\ &=-\sum_{i=1}^{m} \boldsymbol{z}_{i}^{T} \boldsymbol{z}_{i}+\sum_{i=1}^{m} \boldsymbol{x}_{i}^{T} \boldsymbol{x}_{i} \\ &=-\operatorname{tr}\left(\boldsymbol{W}^{T}\left(\sum_{i=1}^{m} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{T}\right) \boldsymbol{W}\right)+\sum_{i=1}^{m} \boldsymbol{x}_{i}^{T} \boldsymbol{x}_{i} \\ &\propto -\operatorname{tr}\left(\boldsymbol{W}^{T}\left(\sum_{i=1}^{m} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^{T}\right) \boldsymbol{W}\right) \quad\quad（公式1） \\ &其中，\sum_{i=1}^{m}x_i^Tx_i是常数，不影响优化。 \end{aligned}由上面公式1可知，最小化公式1即为优化目标： 1.3 PCA伪代码 2. Isomap算法2.1啥是流形？流形的定义是”在局部与欧式距离同胚的空间。”即它在局部具有欧式空间的性质，能用欧氏距离来进行距离计算。感觉这是数学家的定义。欧几里得空间如下图： 好抽象，再看另一组解释：流形是指连接在一起的区域。数学上，它是指一组点，且每个点都有其领域。一般的流形可以通过把许多平直的片折弯并粘连而成。 2.2测地距离测地线：表示在流形上连接两个点的最短距离。比如球面上的测地线就是球面上的大圆弧。 测地距离：测地线的长度。 由于在低纬度的流形上的测地距离不能用高纬空间的直线距离计算，比如地球表面上中国到美国的测地距离远远大于中国到美国在立体球间直线的距离（穿过地球内部的高维直线距离）。 根据流形在欧氏距离上面与欧式空间是同胚，即每个点基于欧氏距离找出其近邻点，然后建立一个近邻连接图，近邻点之间存在连接，非近邻点不存在连接，于是两点之间的测地线距离可以转化为近邻连接图之间的最短路径问题。就是用近邻点在欧式空间可计算的性质来近似测地线距离。 3.3Isomap算法伪代码 注意到Isomap算法只对训练样本降维，但是遇到新的样本怎么办？通常是把训练样本的高纬坐标作为输入，低维空间的坐标作为输出，训练一个回归算法对新样本进行预测。 3.LLE算法3.1 概念局部线性嵌入（LLE，Locally Linear Embedding）试图保持近邻样本之间的线性关系。假设样本之间的$x_i$的坐标可以通过其近邻样本$x_j、x_k、x_l$之间的线性组合重构而得： \boldsymbol{x}_{i}=w_{i j} \boldsymbol{x}_{j}+w_{i k} \boldsymbol{x}_{k}+w_{i l} \boldsymbol{x}_{l}\tag{10.26}那LLE希望这种线性关系在低维空间也继续保持。 LLE先为每个样本$x_i$找到其近邻样本$Q_i$，然后计算基于$Q_i$中的样本点对$x_i$进行线性重构的系数$w_i$: \begin{aligned} \min _{\boldsymbol{w}_{1}, \boldsymbol{w}_{2}, \ldots, \boldsymbol{w}_{m}} & \sum_{i=1}^{m}\left\|\boldsymbol{x}_{i}-\sum_{j \in Q_{i}} w_{i j} \boldsymbol{x}_{j}\right\|_{2}^{2} \\ \text { s.t. } & \sum_{j \in Q_{i}} w_{i j}=1 \end{aligned}\tag{10.27}其中$x_j$是$x_i$的近邻样本，目的就是最小化近邻样本之间的距离，约束条件为权值$w_{ij}$和为1。每一个原始样本$x_i$与若干个邻域样本$x_j$线性表示出的另一个近似的$x_i$，同时减小它们之际的距离即为优化目标。这样的样本有m个，把m个样本都最小化即可。 其$x_i$和$x_j$都是已知条件，令$C_{j k}=\left(\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right)^{\mathrm{T}}\left(\boldsymbol{x}_{i}-\boldsymbol{x}_{k}\right), w_{i j}$有闭式解： w_{i j}=\frac{\sum_{k \in Q_{i}} C_{j k}^{-1}}{\sum_{l, s \in Q_{i}} C_{l s}^{-1}}\tag{10.28}公式（10.28）参考链接：https://datawhalechina.github.io/pumpkin-book/#/chapter10/chapter10 Isomap算法与LLE算法的不同是：Isomap算法试图保持近邻样本之间的距离不变，而LLE算法试图保持近邻样本之间的线性关系不变。** 由于LLE保证在低维空间中权重$w_i$不变，所以$x_i$对应低纬空间坐标$z_i$可通过如下求解： \min _{\boldsymbol{z}_{1}, \boldsymbol{z}_{2}, \ldots, \boldsymbol{z}_{m}} \sum_{i=1}^{m}\left\|\boldsymbol{z}_{i}-\sum_{j \in Q_{i}} w_{i j} \boldsymbol{z}_{j}\right\|^{2}\tag{10.29}利用令$\mathbf{Z}=\left(z_{1}, \boldsymbol{z}_{2}, \ldots, \boldsymbol{z}_{m}\right) \in \mathbb{R}^{d^{\prime} \times m},(\mathbf{W})_{i j}=w_{i j}$: \mathbf{M}=(\mathbf{I}-\mathbf{W})^{\mathrm{T}}(\mathbf{I}-\mathbf{W})\tag{10.30}优化目标重新写为： \begin{array}{l}{\min _{\mathbf{Z}} \operatorname{tr}\left(\mathbf{Z} \mathbf{M} \mathbf{Z}^{\mathrm{T}}\right)} \\ {\text { s.t. } \mathbf{Z Z}^{\mathrm{T}}=\mathbf{I}}\end{array}\tag{10.31}3.2 LLE算法伪代码 4.度量学习4.1距离 看这个：https://www.hotheat.top/archives/af94264b.html 或者 https://www.cnblogs.com/daniel-D/p/3244718.html 4.2相关性 皮尔森Pearson相关系数 VS 斯皮尔曼Spearman相关系数请参考：https://blog.csdn.net/lambsnow/article/details/79972145 参考 西瓜书第10章节 PCA参考博客： https://www.cnblogs.com/wj-1314/p/8032780.html 流形介绍与深度学习：https://yifdu.github.io/2018/11/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/ 本章节公式推导链接：https://datawhalechina.github.io/pumpkin-book/#/chapter10/chapter10 各种度量： https://www.hotheat.top/archives/af94264b.html]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>PCA</tag>
        <tag>Isomap算法</tag>
        <tag>LLE算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KD树的构建]]></title>
    <url>%2F2019%2F07%2F21%2FKD-Tree%2F</url>
    <content type="text"><![CDATA[1.KD树的构建KD树本质上是一颗平衡二叉树，人狠话不多，直接上渣图： 看完以后，这啥！不管，其中红色的L = j mod k +1 ，L是表示从K个维度中选择某一个维度$x^{(L)}$作为中位数选择的范围，把所有$x^{(L)}$的值从小到大排序后，选择中位数的数字进行划分。 2.KD树的构建例子由于愚蠢和缺乏想象力，看不懂上面的伪代码。举例说明，由于深度j初始化从0开始，直接计算第一个维度选取L=0 mod 2 + 1 = 1，所以选择第一个维度属性$x^{(L)}$，即所有数据点的第一列值：2,5,9,4,8,7；排序后为2,4,5,7,8,9；选择中位数是7，然后在图中左边是二维表示，7是横坐标，故竖着划分数据点，左边是比中位数小的数据（2,3）、（4,7），（5,4），右边是（8,1）、（9,6）。接着把上一轮的L赋值给深度j，再计算L = 1 mod 2 + 1 = 2，所以考虑维度$x^{(2)}$，对于左边三个点来说，排序后是3,4,7，中位数是4，于是在平面中纵坐标y=4分割左侧3个数据点，上面大，下面小。对于右侧1,6，中位数选择6，于是纵坐标y=6分割右侧2个数据点，此时点（9,6）在线y=6上，下面是（8,1）。继续划分，L = 2 mod 2 + 1 = 1，于是又选择第一个维度作为划分属性，看左上角只有（4,7），直接选择x=4作为分割线；左下角只有（2,3），x=2为划分线；右下角只有（8,1）未划分，就一个数，中位数自然是本身了，故x=8为划分线；发现所有数据点都在线上，没有未划分的点在超矩形中；分割完毕。 树的排列：划分完以后，左子树的根结点是（5,4）,右结点的根结点是（9,6）。左侧上面数值大对应右子树，下面小对应左子树。右侧上面（9,6）大，放在右子树，下面(8,1)小，放在左子树。总的来说在2维下，就是”上右下左，左左右右”。瞎编的==，即上面数据大放右侧，下面数据小放左侧，左边数据小放左侧，右边数据大放右侧。 3.KD树的搜索（1）伪代码 KD树的平均计算复杂度是$O(logN)$，$N$是实例总数。 （2）KD搜索的例子我那愚蠢的弟弟丫 ==，还是看不懂。举个栗子？？先看看这个前序遍历，中序遍历，后序遍历：https://blog.csdn.net/qq_33243189/article/details/80222629 上面图中红色就是由第2节中例子划分好的KD树，假设求(2.1,3.1)的最近邻结点，怎么做？首先判别第一维变量2.1小于根结点的7，故（2.1,3.1）划分到左子树；接着判别第二个维度变量值3.1发现小于4，于是划分到（2,3），（2,3）是叶结点即终端结点，所以（2,3）是(2.1,3.1)的一个临时的近邻结点，以(2.1,3.1)为圆心，（2,3）到(2.1,3.1)的距离为半径作一个圆圈，检查右子树的结点是否在圈中，若在还需要再到右子树检查是否有最近邻结点，若没有就回溯。这里显然（4,7）不在上面那个圈内，所以回溯到上一层的结点(5,4)，判别(5,4)和当前最近结点(2,3)哪一个距离(2.1,3.1)近？显然是(2,3)，所以当前最近邻结点不变还是(2,3)。还需要判别上面那个圈与结点(5,4)相交？不相交，该(5,4)结点右侧不再搜索，因为距离肯定大。往上回溯到根结点（7,2），以（2.1,3.1）为圆心，以0.1414为半径的圆更不会与x = 7超平面交割，因此不用进入（7,2）右子空间进行查找。至此，搜索路径中的节点已经全部回溯完，结束整个搜索，返回最近邻点（2,3），最近距离为0.1414。 参考1.前序遍历，中序遍历，后序遍历：https://blog.csdn.net/qq_33243189/article/details/80222629 2.KD树：https://www.cnblogs.com/earendil/p/8135074.html]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>TD树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[损失函数和风险]]></title>
    <url>%2F2019%2F07%2F18%2Floss%2F</url>
    <content type="text"><![CDATA[1.损失、风险的概念人狠话不多，直接上渣图： 看完以后，这啥画质？！不管，常见的损失函数有四种，就是左边四个：0-1损失、平方损失、绝对损失、对数损失，都是分别对每一个样本的损失累加求平均。 右侧是三个常见的风险：经验风险、期望分享、结构风险。 （1）经验风险：就是只统计已知样本的损失，然后求平均。 （2）期望风险是针对未知样本，假设已知x,y的样本联合分布p(x，y)，则可以： R_{\mathrm{exp}}(f)=E_{P}[L(Y, f(X))]=\int_{x\times y} L(y, f(x)) P(x, y) \mathrm{d} x \mathrm{d} y（3）结构风险：是对经验风险和期望风险的折中，公式前者是经验风险，后面加了惩罚项(惩罚项 = 惩罚系数λ x 正则化项J(f))。 R_{\mathrm{mm}}(f)=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)2.联系与区别a)经验风险是局部的，基于训练集所有样本点损失函数最小化的 b) 期望风险是全局的，是基于所有样本点的损失函数最小化的 c) 经验风险函数是现实的，可求的 d) 期望风险函数是理想化的，不可求的 经验风险越小，模型决策函数越复杂，其包含的参数越多，当经验风险函数小到一定程度就出现了过拟合现象，因为它只看当前样本的损失。所以结构风险在经验风险后面加了算法复杂度的惩罚项，模型越复杂，惩罚程度越大，以此来制约经验风险的过拟合。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>损失函数</tag>
        <tag>结构风险</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DL第二门课脑图]]></title>
    <url>%2F2019%2F07%2F14%2FAssignment2%2F</url>
    <content type="text"><![CDATA[本页展示Andrew Ng深度学习的第二门课视频的思维导图，由于visio公式保存后会模糊，转化为gif放大后稍微清晰点。所有3周的脑图上传到此地址：点击此处获取脑图 。其中，原始3周脑图文件分别是” 2-1深度学习.xmind “和” 2-2优化算法.vsdx “，” 2-2优化算法.vsdx “里面有2周的脑图。这里贴的是第1周的pdf版本，和第2、3周的gif版本，gif版本用PC版”照片”打开放大后还算清楚，用”Windows照片查看器”打开并不清晰，100%时也一般。本来想都转成PDF版本，visio中有公式保存后再打开公式就糊了，所以所有格式都转的不清晰，xmind又不能插入公式，这点软件功能做的不好。作业部分暂时没有搞完，若完成也会贴出来。不过最近看算法大体都能理解，若不纠结公式细节，主体框架的理解比以前好很多。 以下是三周的陈列，有Dropout正则，梯度消失，梯度爆炸、优化算法：mini-banch、指数加权平均、动量梯度下降法、RMSprob、Adam优化算法、学习率衰减，Banch Norm、Softmaxt回归等内容。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Dropout正则化</tag>
        <tag>mini-banch、指数加权平均、动量梯度下降法、RMSprob、Adam优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聚类算法]]></title>
    <url>%2F2019%2F06%2F01%2Fclustering%2F</url>
    <content type="text"><![CDATA[1.距离的计算方式（1）距离有什么特性？ \begin{alignat}{2} 非负性：dist(x_i,x_j) &\ge 0\tag{1}\\ 同一性：dist(x_i,x_j) &= 0当且仅当x_i=x_j\tag{2}\\ 对称性：dist(x_i,x_j) &= dist(x_j,x_i)\tag{3}\\ 直递性：dist(x_i,x_j) &\leq dist(x_i,x_k)+dist(x_k,x_j)，似于三角不等式。\tag{4} \end{alignat}(2)距离的装逼方式有几种+_+A.闵可夫斯基距离（Minkowski distance）这名字一听就很能唬住人。其中有2个样本$\boldsymbol{x}_{i}=\left(x_{i 1} ; x_{i 2} ; \ldots ; x_{i n}\right)$和$\boldsymbol{x}_{j}=\left(x_{j 1} ; x_{j 2} ; \ldots ; x_{j n}\right)$，就是$x_i-x_j$的$L_p$范数，闵氏距离不是一种距离，而是一组距离的定义。如下： \operatorname{dist}_{\mathrm{mk}}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left(\sum_{u=1}^{n}\left|x_{i u}-x_{j u}\right|^{p}\right)^{\frac{1}{p}} \tag5其中p是一个变参数。当p=1时，就是曼哈顿距离，当p=2时，就是欧氏距离，当p→∞时，就是切比雪夫距离。 B.曼哈顿距离（Manhattan distance）公式（5）中P=1时候，即为曼哈顿距离，也叫作”街区距离”。 \operatorname{dist}_{\operatorname{man}}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right\|_{1}=\sum_{u=1}^{n}\left|x_{i u}-x_{j u}\right|\tag6如下图就是A和B坐标的横坐标之差的和，这图曼哈顿距离为7。 C.欧氏距离（Euclidean distance）这个较为熟悉了，就是绝对值的平方和开2倍根。最简单例子就是2点之间在平面间的距离公式。 \operatorname{dist}_{\mathrm{ed}}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right\|_{2}=\sqrt{\sum_{u=1}^{n}\left|x_{i u}-x_{j u}\right|^{2}}\tag7D.切比雪夫距离（Chebyshev distance） dist（A,B）=max(|X_A-X_B|,|Y_A-Y_B|)\tag8即两点横纵坐标差的最大值，如下图$dis=max(AC,BC)=AC=4$。 2.原型聚类什么叫原型，介个词有些科幻的意思。本文的原型指的是样本空间中具有代表性的点。基于原型的聚类是假设聚类结构可以通过一组原型刻画，较为常用。 （1）K-Means和KNN算法A. K均值算法K均值的算法原理较简单，首先它是一个无监督的算法，它的思想是先人为确定法要估计的类别数量k，然后从样本中随机指定k个类别中心点，然后计算所以点到这k个点的欧式距离，距离哪个中心点最近，样本就是哪一类，然后计算某类内均值作为新的中心点。一直到中心点不变为止。如下： K-means是一个反复迭代的过程： 1） 选取数据空间中的K个样本对象作为初始中心，每个对象代表一个聚类中心； 2） 对于样本中的数据对象，根据它们与这些聚类中心的欧氏距离，按距离最近的准则将它们分到距离它们最近的聚类中心所对应的类； 3） 更新聚类中心：计算每个类别中所有样本的均值作为该类别新的的聚类中心； 4） 判断聚类中心和目标函数的值是否发生改变，若不变，则输出结果，若改变，则返回2）。 B. KNN算法和K均值相似的有一个叫”K近邻算法（KNN，K-Nearest Neighbor）”，这是个有类标的有监督算法，KNN不是聚类算法，只是此文顺带一提。它的思想主要如下：思路是：如果一个样本在特征空间中的k个最邻近的样本中的大多数属于某一个类别，则该样本也划分为这个类别。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。算法如下： 1）计算测试数据与各个训练数据之间的距离； 2）按照距离的递增关系进行排序； 3）选取距离最小的前K个点； 4）统计前K个点所在类别的出现频率； 5）返回前K个点中出现频率最高的类别作为测试数据的预测分类 一句话概括就是”物以类聚”，身边哪个类标出现的多，自己就是哪一个类。KNN和K-Means是不同类型的算法，但是都以欧式距离为出发点做文章。 （2）学习向量量化(Learning Vector Quantization)核心思想：人工确定要聚类的簇，找出1个原型向量，使得周围样本X与此向量最近，则分好1簇，其它簇类似。其实和K均值想法类似，只不过LVQ算法是生成原型向量，找的方式都用到了最小距离，但是更新方式不同，Kmeans是求均值。LVQ算法如何找？看下图： 注意：(1) 原型向量初始化直接是样本中随机选择的，初始化原型向量的类标也是随机的 (2)然后计算所有原型向量与随机抽取的$X_j$之间的距离，找出最小的原型向量，若原型向量的类标与$X_j$原始类标一样，就把原型向量$P_i^*$靠近$X_j$，若原型向量的类标与$X_j$原始类标不一样，则把原型向量$P_i^*$远离$X_j$。然后再随机选中一个$X_j$，计算原型向量与$X_j$的距离，再根据公式更新P’ 。 (3) 满足条件是循环次数到预设值，或者原型向量更新很小，甚至不再更新。由于样本随机选择，不是所有的样本都会用到。 （3）高斯混合聚类(Mixture of Gaussian)A. 多元高斯分布 p(\boldsymbol{x})=\frac{1}{(2 \pi)^{\frac{n}{2}}|\mathbf{\Sigma}|^{\frac{1}{2}}} e^{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\mathrm{T}} \mathbf{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})}\tag9这里的$x$是样本空间$X$中含有n维的某样本，$μ$是$n$维均值向量，$Σ$是$n\times n$的协方差矩阵，高斯分布由$μ$和$Σ$这两个参数决定。 B. 高斯混合模型这里的高斯混合聚类(Mixture of Gaussian)就是高斯混合模型(Gaussian Mixture Model)，用EM算法来估计参数。 （i）混合高斯分布 p_{\mathcal{M}}(\boldsymbol{x})=\sum_{i=1}^{k} \alpha_{i} \cdot p\left(\boldsymbol{x} | \boldsymbol{\mu}_{i}, \boldsymbol{\Sigma}_{i}\right)\tag{10}就是K个高斯没事干，一起搞事情，$α_i$是每个高斯的贡献度，或者出力程度，或者来自此分布的概率。大家拧成一股绳：$\sum_{i=1}^{k} \alpha_{i}=1$。 （ii）混合成分的后验概率 $z_j∈\{1,2,…,k\}$是生成样本$x_j$的高斯混合成分，即随机变量$z_j$是高斯混合模型生成的样本。下面公式$α_i(i=1,…,k)$看作是$z_j$的先验概率，就是没有样本$x_j$的时候$z_j$的概率；$p_M(x_j|z_j=i)$是似然函数，看作是某个混合高斯成分$z_j$分布下生成$x_j$的概率。分母是所有K个高斯加起来起来的。 \begin{aligned} p_{\mathcal{M}}\left(z_{j}=i | \boldsymbol{x}_{j}\right) &=\frac{P\left(z_{j}=i\right) \cdot p_{\mathcal{M}}\left(\boldsymbol{x}_{j} | z_{j}=i\right)}{p_{\mathcal{M}}\left(\boldsymbol{x}_{j}\right)} \\ &=\frac{\alpha_{i} \cdot p\left(\boldsymbol{x}_{j} | \boldsymbol{\mu}_{i}, \boldsymbol{\Sigma}_{i}\right)}{\sum_{l=1}^{k} \alpha_{l} \cdot p\left(\boldsymbol{x}_{j} | \boldsymbol{\mu}_{l}, \mathbf{\Sigma}_{l}\right)} \end{aligned}\tag{11}所以样本的生成过程可以看作是：首先根据先验分布$α_1,α_2,α_3,…,α_k$选择高斯混合成分($α_i$就是每个混合成分的抽中概率)，然后根据该混合成分的概率密度函数来采样，生成新样本。 （iii）算法过程 EM算法中，其中隐变量对应模型的高斯混合成分，表示观测数据$x_j$具体是来自第k个分模型的数据是未知的，即对于给定的数据$x_j$，计算该数据属于第k个高斯混合分布生成的后验概率$γ_ji$是未知的隐变量。$γ_ji$也可以看作是第j个样本所对应的第i个高斯分布的概率。而观测数据$x_j$是已知变量。 EM算法的E步骤：利用当前估计的参数值计算对数似然的期望值。混合高斯模型这里是计算隐变量—-&gt;根据本文公式（11）计算后验概率，然后有了后验概率$γ_ji$； EM算法的M步骤：寻找能使E步似然期望（这里是后验概率）最大化的参数值—-&gt;根据导数为0求得公式迭代参数； 循环E步和M步，直到收敛。收敛条件可以是最大迭代次数或参数(μ，Σ，α)几乎不变。然后更新参数，最后划分样本的簇。具体划分规定如下图： ​ 可以根据最终参数计算k个高斯分布，直接从最大的后验分布概率中选一个作为最终划分的簇。 西瓜书具体公式推导参考南瓜书： https://datawhalechina.github.io/pumpkin-book/#/chapter9/chapter9 ，这里有几乎所有细致的公式推导。 C. GMM和K-means的异同点可以看出GMM和K-means还是有很大的相同点的。GMM中数据对高斯分量的响应度就相当于K-means中的距离计算，GMM中的根据响应度计算高斯分量参数就相当于K-means中计算分类点的位置。然后它们都通过不断迭代达到最优。不同的是：GMM模型给出的是每一个观测点由哪个高斯分量生成的概率，而K-means直接给出一个观测点属于哪一类。 3.密度聚类（1）DBSCAN算法此类算法假设数据可以通过样本分布的紧密程度来聚类。了解一个著名的密度聚类算法DBSCAN算法（DBSCAN，Density-Based Spatial Clustering of Applications with Noise），其中有一对参数(ε，MinPts)，ε表示到核心样本的距离（即样本集D中所有其它样本到某个样本$x_j$的距离不大于ε），MinPts表示ε-邻域中最低包含的样本个数。 先看下图四个概念： 核心对象就是以它为中心，其它样本都可以由它在ε-邻域内访问到，并且能访问到的个数&gt;=MinPts。 密度直达：可以由核心对象直接访问的样本，则称为$x_j$由核心对象$x_i$密度直达。(有向) 密度可达：通过其它核心对象间接访问。(有向) 密度相连：样本之间只能通过其它核心样本分别访问。（无向） （2）DBSCAN算法伪代码 1-7行就是找出所有核心对象，10-24行是以随机一个核心对象开始，找出可以都其密度可达的样本组合成聚类簇，直到所有核心对象被访问过。 A. 找出所有核心对象首先确定ε-邻域中ε的大小，就是ε-邻域内其他样本到核心对象的距离，这里默认是欧氏距离，然后统计ε-邻域内样本个数（即其它样本到核心对象的距离小于等于ε的样本个数），符合大于等于MinPts的话就添加该样本到核心对象集合Ω中，如此找出所有核心对象。 B. 找出聚类簇（i）初始化初始化聚类数量k、未访问的样本集合Γ。第10行伪代码当核心对象集合Ω不为空的时候，说明能以ε-邻域为范围的、符合样本个数MinPts限制的样本仍可以访问到，所以反过来思考：若核心对象集合Ω为空，说明当前已经聚完一个簇了（即说明能以ε-邻域为范围的、符合样本个数MinPts限制的样本都被访问到了）。 （ii）如何聚簇？未访问的样本集合$Γ$赋给$Γ_{old}$，随机选择一个核心对象$o$，初始化存放样本的队列$Q$；13行伪代码表示从未访问的样本集合$\Gamma$中去除已经访问的核心对象$o$，14-20行伪代码是真正的聚簇过程。当队列中样本不为空时一直循环：先取出队列Q中的第一个样本q，如果然后判断此样本q是核心对象？若样本q是核心对象，则生成一个新集合$Δ$，这个$Δ$是核心对象与未访问的样本集合$\Gamma$的交集，然后将此交集加入队列Q，再更新样本集合$\Gamma$ (注意19行：是把样本集合$\Gamma$中已经被访问到的样本去掉)。这样一个循环就把一个簇的样本从原始样本中去除了，那$C_k$就是第k个聚类簇，是把上一次未访问到样本集合$\Gamma_{old}$去掉本轮未访问的集合$\Gamma$，即为当前聚好的簇的样本集合$C_k$，更新聚类数量：k+1。最后再大循环即可聚多个类。 聚簇的过程有点类似于切蛋糕的过程，比如下图，通过(ε，MinPts)和密度相连的概念先把一个簇$C_1$切出来，然后在剩下的样本空间中切除第二个簇$C_2$，依次往下，可以聚出4个簇。当然DBSCAN可以有噪声，即可以有访问不到的样本存在。 这里注意到首先17行是求核心对象能访问到的样本与样本集合$Γ$的交集，为什么不直接将领域内的所有样本加入Q，多做一个交集$Δ$？因为可能某个样本$x_6$既可以被某核心对象$x_1$访问到，也可以被核心对象$x_2$访问，但是的样本$x_6$已经把归入队列Q，所以在循环到时候，已经放到队列Q中，所以不必加入队列Q中（即不用包括在集合中$\Delta$）。若样本q不是核心对象，继续取出队列中下一个样本再判断是否为核心对象。 （iii）算法例子对于愚蠢的偶来说，举了栗子好一些诶，于是有下图所示： 假设都是1x1的方格，样本分布如上图所示，设定参数(ε，MinPts)中ε=2，MinPts是3。以这个为例子，在找出所有的核心对象之前先统计ε=2内领域内样本密度可直达的样本和其个数。对于样本$x_1$来说1-5的样本与$x_1$距离在2以内都可以访问到，所以有5个样本密度直达，其他类似，如下所示。 \begin{align*} N_ε(x_1) &= \{x_1,x_2,x_3,x_4,x_5\}，5个样本\\ N_ε(x_2) &= \{x_1,x_2,x_3\}，3个样本\\ N_ε(x_3) &= \{x_1,x_2,x_3,x_4\}，4个样本\\ N_ε(x_4) &= \{x_1,x_3,x_4\}，3个样本\\ N_ε(x_5) &= \{x_1,x_5\}，2个样本\\ N_ε(x_6) &= \{x_6，x_7,x_8\}，3个样本\\ N_ε(x_7) &= \{x_6，x_7,x_8\}，3个样本\\ N_ε(x_8) &= \{x_6，x_7,x_8\}，3个样本\\ \end{align*}个数符合在3个及以上的只要把$N_ε(x_5) = \{x_1,x_5\}$去掉即可，有如下核心对象以及对应密度直达样本： \begin{align*} N_ε(x_1) &= \{x_1,x_2,x_3,x_4,x_5\}，5个样本\\ N_ε(x_2) &= \{x_1,x_2,x_3\}，3个样本\\ N_ε(x_3) &= \{x_1,x_2,x_3,x_4\}，4个样本\\ N_ε(x_4) &= \{x_1,x_3,x_4\}，3个样本\\ N_ε(x_6) &= \{x_6，x_7,x_8\}，3个样本\\ N_ε(x_7) &= \{x_6，x_7,x_8\}，3个样本\\ N_ε(x_8) &= \{x_6，x_7,x_8\}，3个样本\\ \end{align*}便找到了核心对象集合$Ω=\{x_1,x_2,x_3,x_4,x_6,x_7,x_8\}$，完成第一步。第二步骤就是找出聚类簇的对应样本。首先初始化当前未访问的样本集合$Γ=D=\{x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8\}$所有样本，随机选择一个核心对象o放入队列Q，我们这里假设随机选的是$x_1$放入$Q=\{x_1\}$，因为已经访问了核心样本o，所以要把此核心对象o从Γ中删去，则$Γ=\{x_2,x_3,x_4,x_5,x_6,x_7,x_8\}$。 当Q不为空时，一直做如下操作： 取出队列Q中首个样本q，此处为$x_1$， 判别$x_1$是否为核心对象？$x_1$确实为核心对象： ​ 求交集$\Delta$是： \begin{align*} \Delta &= N_ε(q)∩Γ = \{x_2,x_3,x_4,x_5\}\\ 其中Γ &= \{x_2,x_3,x_4,x_5,x_6,x_7,x_8\}\\ N_ε(q) &= N_ε(x_1) = \{x_1,x_2,x_3,x_4,x_5\} \end{align*} ​ 更新Q：将领域$N_ε(x_1)$内的未被访问的样本即交集$\Delta=\{x_2,x_3,x_4,x_5\}$加入到队列Q中，此时$Q=\{x_1,x_2,x_3,x_4,x_5\}$ ​ 更新Γ：去掉已经访问的样本集：$Γ = \{x_6,x_7,x_8\}$； 判别Q中下一个 样本$x_2$，$x_2$是核心对象，所以再求领域内未被访问的样本集$\Delta=\{\}$，发现是空集合，则后面两个操作等于没操作。然后继续往后判断样本$x_3$，不更新$\Delta 、Q、Γ$；判断样本$x_4$，不更新$\Delta 、Q、Γ$；再判断Q中的样本$x_5$，不更新$\Delta 、Q、Γ$。最后Q为空，结束本次循环。 然后得到一个新簇，簇的数量要更新：k+1，新簇$C_1=Γ_{old} \backslash Γ= \{x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8\}-\{x_6,x_7,x_8\}=\{x_1,x_2,x_3,x_4,x_5\}$，更新核心对象Ω，把当前簇中所有核心对象都去掉； 继续最外层的循环，寻找新乐子，不是 ，是新簇 = =，这里显然是簇$C_2=\{x_6,x_7,x_8\}$。 对于此例子来说，最终结果是划分为2个簇，如下图所示，蓝色线圈起来的，一个是簇$C_1$，一个是簇$C_2$： （3）DBSCAN优缺点优点： A. 与K-means相比，不需要设定簇的个数k B.对任意形状可以分，可以是非凸的样本分布 C.DBSCAN可以发现噪声样本，且算法较为稳定。 缺点： A. 对于密度不均匀的样本，效果不好 B.样本较多时，收敛时间长 C.参数(ε，MinPts)需要调参 4.层次聚类（1）数据集划分的方式自顶向下：数据集从一个整体一直划分成子项，子项再划分成孙子项，。。。一直划下去。 自底向上：数据集中样本从一个聚成2个1份，2份个聚成1个簇，最后有若干簇，若干簇合成一个整体的样本。 （2）AGNES算法核心思想就是采用自底向上的层次聚类方式，将样本中的每一个样本看作是初始聚类簇，然后找出距离最近的聚类簇进行合并，重复此过程，达到预定的聚类簇个数。 距离计算方式： 算法伪代码： 例子不举了，懒。 参考1.GMM介绍：https://blog.csdn.net/xmu_jupiter/article/details/50889023 2.西瓜书第九章]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>聚类算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Andrew Ng的深度学习作业1]]></title>
    <url>%2F2019%2F05%2F14%2FAssignment1%2F</url>
    <content type="text"><![CDATA[这里发布的是大神Andrew Ng第一门课—神经网络和深度学习的作业，网上下载请点击此处，里面已经包含第一课的所有四周的编程作业，里面是Jupyter Notebook文件。其他答案请参考： https://github.com/HuangCongQing/deeplearning.ai-note ，这个更加全面，有选择题测验。下面仅仅是第三周的作业展示，由于此周原始答案没有对隐层网络个数更新导致的错误，已经在下面更正。注意下面直接运行是不能有结果的，需要到我上传的作业地址里面下载其他辅助函数和文件。 Planar data classification with one hidden layerWelcome to your week 3 programming assignment. It’s time to build your first neural network, which will have a hidden layer. You will see a big difference between this model and the one you implemented using logistic regression. You will learn how to: Implement a 2-class classification neural network with a single hidden layer Use units with a non-linear activation function, such as tanh Compute the cross entropy loss Implement forward and backward propagation 1 - PackagesLet’s first import all the packages that you will need during this assignment. numpy is the fundamental package for scientific computing with Python. sklearn provides simple and efficient tools for data mining and data analysis. matplotlib is a library for plotting graphs in Python. testCases provides some test examples to assess the correctness of your functions planar_utils provide various useful functions used in this assignment 12345678910111213141516# Package importsimport numpy as npimport matplotlib.pyplot as pltfrom testCases import *import sklearnimport sklearn.datasetsimport sklearn.linear_modelfrom planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets%matplotlib inlinenp.random.seed(1) # set a seed so that the results are consistent'''原始答案不对，由于直接赋值n_h=4导致n_h一直没有更新，所以决策图像一直是4个节点的结果，本答案已经更新准确！注意到最后一个边界有一个弯曲边界，证明神经网络可以拟合非线性的模型（其实本来就可以，感知机(Perceptron)不能计算非线性问题，BP算法训练的单层神经网络就已经可以解决非线性问题了。参考blog： https://blog.csdn.net/liuzhongkai123/article/details/78775744 ''' 2 - DatasetFirst, let’s get the dataset you will work on. The following code will load a “flower” 2-class dataset into variables X and Y. 1X,Y = load_planar_dataset() Visualize the dataset using matplotlib. The data looks like a “flower” with some red (label y=0) and some blue (y=1) points. Your goal is to build a model to fit this data. 123# Visualize the data:# scatter参数： https://www.cnblogs.com/shanlizi/p/6850318.htmlplt.scatter(X[0, :], X[1, :], c=np.squeeze(Y), s=40, cmap=plt.cm.Spectral)#此处要squeeze一下，否则可能报错 You have:​ - a numpy-array (matrix) X that contains your features (x1, x2)​ - a numpy-array (vector) Y that contains your labels (red:0, blue:1). Lets first get a better sense of what our data is like. Exercise: How many training examples do you have? In addition, what is the shape of the variables X and Y? Hint: How do you get the shape of a numpy array? (help) 123456789### START CODE HERE ### (≈ 3 lines of code)shape_X = X.shapeshape_Y = Y.shapem = np.shape(X[0,:]) # training set size### END CODE HERE ###print ('The shape of X is: ' + str(shape_X))print ('The shape of Y is: ' + str(shape_Y))print ('I have m = %d training examples!' % (m)) 123The shape of X is: (2, 400)The shape of Y is: (1, 400)I have m = 400 training examples! 3 - Simple Logistic RegressionBefore building a full neural network, lets first see how logistic regression performs on this problem. You can use sklearn’s built-in functions to do that. Run the code below to train a logistic regression classifier on the dataset. 123# Train the logistic regression classifierclf = sklearn.linear_model.LogisticRegressionCV()clf.fit(X.T, Y.T.ravel()); #将多维数组降位一维 You can now plot the decision boundary of these models. Run the code below. 123456789# Plot the decision boundary for logistic regressionprint('Y.ravel()=',Y.ravel())plot_decision_boundary(lambda x: clf.predict(x), X, Y.ravel()) #Y要降维度plt.title("Logistic Regression")# Print accuracyLR_predictions = clf.predict(X.T)print ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) + '% ' + "(percentage of correctly labelled datapoints)") 123456789101112Y.ravel()= [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]Accuracy of logistic regression: 47 % (percentage of correctly labelled datapoints) Interpretation: The dataset is not linearly separable, so logistic regression doesn’t perform well. Hopefully a neural network will do better. Let’s try this now! 4 - Neural Network modelLogistic regression did not work well on the “flower dataset”. You are going to train a Neural Network with a single hidden layer. Here is our model: Mathematically: For one example $x^{(i)}$: \begin{eqnarray*} z^{[1] (i)} &=& W^{[1]} x^{(i)} + b^{[1] (i)}\tag{1} \\ a^{[1] (i)} &=& \tanh(z^{[1] (i)})\tag{2}\\ z^{[2] (i)} &=& W^{[2]} a^{[1] (i)} + b^{[2] (i)}\tag{3}\\ \hat{y}^{(i)} &=& a^{[2] (i)} = \sigma(z^{ [2] (i)})\tag{4}\\ y^{(i)}_{prediction} &=& \begin{cases} 1 & \mbox{if } a^{[2](i)} > 0.5 \\ 0 & \mbox{otherwise } \end{cases}\tag{5} \end{eqnarray*}上面公式的维度如下图所示： Given the predictions on all the examples, you can also compute the cost $J$ as follows: J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large\left(\small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right) \large \right) \small \tag{6}Reminder: The general methodology to build a Neural Network is to:​ 1. Define the neural network structure ( # of input units, # of hidden units, etc).​ 2. Initialize the model’s parameters​ 3. Loop:​ - Implement forward propagation​ - Compute loss​ - Implement backward propagation to get the gradients​ - Update parameters (gradient descent) You often build helper functions to compute steps 1-3 and then merge them into one function we call nn_model(). Once you’ve built nn_model() and learnt the right parameters, you can make predictions on new data. 一张A4纸讲清楚BP算法：：其中tanh(z)以及其导数: \begin{eqnarray*} \tanh (z) &=& \frac{\sinh (z)}{\cosh (z)}=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}} \\ \tanh ^{\prime}(z) &=&\frac{\left(e^{z}+e^{-z}\right)\left(e^{z}+e^{-z}\right)-\left(e^{z}-e^{-z}\right)\left(e^{z}-e^{-z}\right)}{\left(e^{z}+e^{-z}\right)^{2}}\\ &=& \frac{\left(e^{z}+e^{-z}\right)^{2}-\left(e^{z}-e^{-z}\right)^{2}}{\left(e^{z}+e^{-z}\right)^{2}}\\ &=& 1-\left(\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}\right)^{2} \\ &=&1-\tanh ^{2}(z) \end{eqnarray*}4.1 - Defining the neural network structureExercise: Define three variables:​ - n_x: the size of the input layer​ - n_h: the size of the hidden layer (set this to 4)​ - n_y: the size of the output layer Hint: Use shapes of X and Y to find n_x and n_y. Also, hard code the hidden layer size to be 4. 1234567891011121314151617181920# GRADED FUNCTION: layer_sizesdef layer_sizes(X, Y,n_hiden): """ 初始化输入层、隐藏层、输出层的参数 Arguments: X -- input dataset of shape (input size, number of examples) Y -- labels of shape (output size, number of examples) Returns: n_x -- the size of the input layer n_h -- the size of the hidden layer n_y -- the size of the output layer """ ### START CODE HERE ### (≈ 3 lines of code) n_x = X.shape[0] # size of input layer n_h = n_hiden #隐层就直接定4个结点 n_y = Y.shape[0] # size of output layer ### END CODE HERE ### return (n_x, n_h, n_y) 12345X_assess, Y_assess = layer_sizes_test_case()(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess,4)print("The size of the input layer is: n_x = " + str(n_x))print("The size of the hidden layer is: n_h = " + str(n_h))print("The size of the output layer is: n_y = " + str(n_y)) 123The size of the input layer is: n_x = 5The size of the hidden layer is: n_h = 4The size of the output layer is: n_y = 2 4.2 - Initialize the model’s parametersExercise: Implement the function initialize_parameters(). Instructions: Make sure your parameters’ sizes are right. Refer to the neural network figure above if needed. You will initialize the weights matrices with random values. Use: np.random.randn(a,b) * 0.01 to randomly initialize a matrix of shape (a,b). You will initialize the bias vectors as zeros. Use: np.zeros((a,b)) to initialize a matrix of shape (a,b) with zeros. 12345678910111213141516171819202122232425262728293031323334353637# GRADED FUNCTION: initialize_parametersdef initialize_parameters(n_x, n_h, n_y): """ Argument: n_x -- size of the input layer n_h -- size of the hidden layer n_y -- size of the output layer Returns: params -- python dictionary containing your parameters: W1 -- weight matrix of shape (n_h, n_x) b1 -- bias vector of shape (n_h, 1) W2 -- weight matrix of shape (n_y, n_h) b2 -- bias vector of shape (n_y, 1) """ np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random. ### START CODE HERE ### (≈ 4 lines of code) W1 = np.random.randn(n_h,n_x) * 0.01 b1 = np.zeros((n_h,1)) W2 = np.random.randn(n_y,n_h) * 0.01 # 大小是输出层个数*隐层个数 b2 = np.zeros((n_y,1)) #b2是输出层的阈值 ### END CODE HERE ### assert (W1.shape == (n_h, n_x)) assert (b1.shape == (n_h, 1)) assert (W2.shape == (n_y, n_h)) assert (b2.shape == (n_y, 1)) parameters = &#123;"W1": W1, "b1": b1, "W2": W2, "b2": b2&#125; return parameters 1234567n_x, n_h, n_y = initialize_parameters_test_case()parameters = initialize_parameters(n_x, n_h, n_y)print("W1 = " + str(parameters["W1"]))print("b1 = " + str(parameters["b1"]))print("W2 = " + str(parameters["W2"]))print("b2 = " + str(parameters["b2"])) 1234567891011输出：W1 = [[-0.00416758 -0.00056267] [-0.02136196 0.01640271] [-0.01793436 -0.00841747] [ 0.00502881 -0.01245288]]b1 = [[0.] [0.] [0.] [0.]]W2 = [[-0.01057952 -0.00909008 0.00551454 0.02292208]]b2 = [[0.]] 由于格式问题，后面的部分请参考本博客开头具体github里面的ipython文件。 参考 吴恩达深度学习第一课第三周课后作业blog： https://blog.csdn.net/liuzhongkai123/article/details/78775744 DL笔记以及代码： https://github.com/HuangCongQing/deeplearning.ai-note 子图绘制参考： https://blog.csdn.net/gatieme/article/details/61416645 Markdown下LaTeX公式、编号、对齐: https://www.zybuluo.com/fyywy520/note/82980 http://scs.ryerson.ca/~aharley/neural-networks/ http://cs231n.github.io/neural-networks-case-study/]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>BP算法实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVD简介]]></title>
    <url>%2F2019%2F05%2F09%2FSVD%2F</url>
    <content type="text"><![CDATA[1.SVD前奏1.1奇异值分解(SVD)优缺点 优点：简化数据，去除噪声点，提高算法的结果； 缺点：数据的转换可能难以理解； 适用于数据类型：数值型。 1.2 特征值和特征向量的定义回顾特征值和特征向量的定义如下： Ax=\lambda x其中矩阵$A$是一个$n×n$的实对称矩阵，$x$是一个$n$维向量，则我们说$λ$是矩阵$A$的一个特征值，而$x$是矩阵$A$的特征值$λ$所对应的特征向量。求出特征值和特征向量有什么好处呢？ 就是我们可以将矩阵A特征分解。如果我们求出了矩阵$A$的$n$个特征值$λ_1≤λ_2≤…≤λ_n$，以及这$n$个特征值所对应的特征向量${w_1,w_2,…w_n}$，，如果这$n$个特征向量线性无关，那么矩阵$A$就可以用下式的特征分解表示： A=W\Sigma W^{-1}其中$W$是这$n$个特征向量所张成的$n×n$维矩阵，而$Σ$为这$n$个特征值为主对角线的$n×n$维矩阵。一般我们会把$W$的这$n$个特征向量标准化，即满足$||wi||^2=1$， 或者说$w^T_iw_i=1$，此时$W$的$n$个特征向量为标准正交基，满足$W^TW=I$，即$W^T=W^{−1}$, 也就是说$W$为酉矩阵。于是上面的矩阵$A$可以表示为： A=W\Sigma W^TBUT，这个分解对矩阵$A$有要求，就是矩阵要为$n$维的方阵，若$A$不是方阵，那么上面没办法了，于是SVD闪亮登场 2.SVD(Singular Value Decomposition)任何一个矩阵可以分解为如下形式： A = U\Sigma V^T其中$U$是一个$m×m$的矩阵，$Σ$是一个$m×n$的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值，$V$是一个$n×n$的矩阵。$U$和$V$都是酉矩阵，即满足$U^TU=I,V^TV=I$。下图是关于SVD的维度理解： 如何求$U、\Sigma 、V$三个矩阵？ （1）求矩阵$V$如果我们将$A$的转置和$A$做矩阵乘法，那么会得到$n×n$的一个方阵$A^TA$。既然$A^TA$是方阵，那么我们就可以进行特征分解，得到的特征值和特征向量满足下式： (A^TA)v_i = \lambda_i v_i这样我们就可以得到矩阵$A^TA$的$n$个特征值和对应的$n$个特征向量$v$了。将$A^TA$的所有特征向量张成一个$n×n$的矩阵$V$，就是我们SVD公式里面的$V$矩阵了。一般我们将$V$中的每个特征向量叫做A的右奇异向量。 （2）求矩阵$U$如果我们将$A$和$A$的转置做矩阵乘法，那么会得到$m×m$的一个方阵$AA^T$。既然$AA^T$是方阵，那么我们就可以进行特征分解，得到的特征值和特征向量满足下式： (AA^T)u_i = \lambda_i u_i得到矩阵$AA^T$的$m$个特征值和对应的$m$个特征向量$u_i$了。将$AA^T$的所有特征向量$u_i$张成一个$m×m$的矩阵$U$，就是我们SVD公式里面的$U$矩阵了。一般我们将$U$中的每个特征向量叫做$A$的左奇异向量。 （3）求矩阵$\Sigma$由于$Σ$除了对角线上是奇异值其他位置都是0，那我们只需要求出每个奇异值$σ$就可以了。如下： A=U\Sigma V^T \Rightarrow AV=U\Sigma V^TV \Rightarrow AV=U\Sigma \Rightarrow Av_i = \sigma_i u_i \Rightarrow \sigma_i = Av_i / u_i那就求出奇异值矩阵$Σ$。 此外，特征值和奇异值满足如下关系： \sigma_i = \sqrt{\lambda_i}这样也就是说，我们可以不用$σi=Avi/ui$来计算奇异值，也可以通过求出$A^TA$的特征值$\lambda_i$取平方根来求奇异值。 （4）例子关于例子请直接看参考1里面的例子，写的好的，这篇这样此博客的复制。 3.应用SVD可以简化计算量，比如一幅图50000个像素点，用SVD处理过后的矩阵表示可以缩到10000个点就可以表示出来。可以点击此网站把玩。下图是参考2的部分解释，就是把所有的特征从大到小排列，然后可以选取前几个重要的特征来近似表示原始特征。 图像压缩前后的对比效果图如下，代码请阅读参考3： 4.SVD和聚类的思考4.1 $a_i$到$b_j$的转移概率今天又遇到了一篇有趣的博客，是苏剑林的博客，点击此处链接。SVD除了简化计算量、减少特征表示，还有其他的神奇作用？是的，在自然语言处理中，也有作用。可见其简单的数学工具在各个领域都可以广泛的应用，比如1+1=2在任何领域都会用到，O(∩_∩)O哈哈~。看下图： 把公式转化为图模式来解释可能更加清晰，如下： 一共是3个步骤，如上图中的标号所示： （1）红色标号1表示$p(c_l|a_i)$是$a_i$表现为类别$c_l$的概率； （2）黄色标号2表示$p(d_k|c_l)$是类别$c_l$后接类别$d_k$概率； （3）红色标号3表示$p(b_j|d_k)$是已知类别$d_k$时，元素为$b_j$的概率。 但是元素$a_i—&gt;b_j$可以在多个类别中依不同概率实现，于是有了示意图的下半部分，即公式： p(b_j|a_i) = \sum_{k,l}p(b_j|d_k)p(d_k|c_l)p(c_l|a_i)4.2 这和自然语言处理有什么关系？上文解释了$a_i$到$b_j$的转移概率如何表示，其实这已经把词给聚类好了。比如动词+名词在英语里面是一条规则。动词和名词就是要聚的类别。在计算某个动词$a_i$到名词$b_j$的时候，需要分解为三个矩阵相乘： P(B|A)=P(B|D)\times P(D|C)\times P(C|A)$P(B|A)$是语料中所有动词后面接名词的概率，$P(C|A)$其实就是已经自动聚类好的一个，表示所有词$a_i$组成的词A属于动词类别$C$的概率；$P(D|C)$是动词类别$C$后面接名词类别$D$的概率；$P(B|D)$是名词类别D中包涵词B的概率。显然，左、右奇异向量即$P(B|D)、P(C|A)$已经把类别聚好了，不过前者是某类别下有哪些词，后者是哪些词在某类里面。 本章节是自己的思考，可能不严谨 ==。 $Pas \ à \ pas，on\ va\ loin. $ 5.参考 摘自： https://www.cnblogs.com/pinard/p/6251584.html#!comments 统计网站： https://cosx.org/2014/02/svd-and-image-compression SVD博客参考： http://redstonewill.com/1529/ SVD分解是怎么聚类的： https://kexue.fm/archives/4216/comment-page-1#comment-11133]]></content>
      <categories>
        <category>矩阵分析</category>
      </categories>
      <tags>
        <tag>SVD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[矩阵求导简述]]></title>
    <url>%2F2019%2F05%2F05%2FMatrix-derivatives%2F</url>
    <content type="text"><![CDATA[下面是2份矩阵求偏导的pdf，上传以便查看。 上面两份pdf都有矩阵和向量相关的导数和偏导公式总结，按需查找即可。在下面找到的一份资料，也上传，这一份比较全面也复杂，若掌握了基本的ML求导求偏导就不在话下。]]></content>
      <categories>
        <category>矩阵分析</category>
      </categories>
      <tags>
        <tag>矩阵求导</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[skip-gram模型理解二]]></title>
    <url>%2F2019%2F05%2F04%2FNLP2%2F</url>
    <content type="text"><![CDATA[首先说明的是，此为CS224n课程的笔记，有不到之处，望留言提出，thanks。之前也写过word2vec模型，奈何没理解透，这次再理解下 ==。 1.Skip-Gram模型的简单理解1.1 简单理解独热码和分布式表示词向量和skip-gram简单介绍请看： 此链接。skip-gram说讲简单点就是用中心词去预测周围的左右m个词，它是基于神经网络和词向量模型构成，可以处理数十亿级别的词量，效果居然比以往好的出奇。预测周围t个词的概率是p(w-t | wt)，损失函数如下： 下面是skip-gram model的预测演示： 中心词预测周围词后的计算公式如下，就是下文的p(w_t+j | w_t;θ) 计算方式。 说下UoVc表示输出词向量Uo和中心词向量Vc的乘积值，这个值越大表示两个词越相似，相关性越强，也就越可能出现在中心词周围，然后归一化即可。即算出每个词与中心词的相似程度，再后面选一个最相似的即可（概率最大的）。 下面这一张是更加具体的演示： 下面一张图是对skip-gram的具像解释，比如一句话”I like deep learning and NLP.”第一个窗口以deep为中心词，划窗范围是左右2个词，就有了图中的划窗①，然后每次往后移动一个以’learning’为中心词再计算划窗②下周围词的概率。每个中心词在上图一个d x V维度的红色大矩阵里面有对应的位置的向量表示，每个周围词的向量表示在上图第二个V x d维的红色大矩阵里面取出来，2个直接拿出来代入下图的公式计算即可（输入每个词的独热码作用就是取出每个词在look-up table（look-up table就是矩阵d x V维度的第一个红色大矩阵W）里面中心词的对应词向量表示）。最后经过softmax函数处理再和真实类标比较减小误差，以优化2个红色矩阵里面的中心词和周围词矩阵表示，其实这2个红色矩阵就是类似BP算法里面的隐层权重，这里变为了中心词和周围词的向量表示。 显然，我们就是要使得周围词预测数来的概率最大化，即周围词p(w_t+j | w_t;θ) ，θ表示我们所有需要优化的参数，假设语料库是极大量的词，有T个单词，那就是把每一个中心词的前t个词和后t个词对应词的分布式表示（这里计算时候是概率）都乘起来最大化即可。第二个公式是为了最小化的同等变换。 1.2 模型求参方式现在就是要最小化上面负的对数似然函数，对于参数可以用随机梯度下降法（SGD）来求参数，于是求导。 其中的P(w_t+j | w_t;θ) 就是通过P(O | C)得出来，对Uo和Vc分别求偏如下： 1.3 模型做什么？针对Vc的求偏导，有一种解释： \begin{align*} \frac{∂J}{∂V_c}&=u_0-\sum_{x=1}^{V}\frac{exp(u_x^TV_c)}{\sum_{w=1}^{V}exp(u_w^TV_c)}u_{x}\\ &=u_{0}-\sum_{x=1}^{v} p(x | c) u_{x} \end{align*}$u_0$是实际输出的上下文向量，后面$p(x | c) $是后面$u_{x}$出现的似然概率的加权；$u_{x}$为期望向量，是上下文向量均值；模型做的事就是要使得$u_{0}-\sum_{x=1}^{v} p(x | c) u_{x}$越小越好。 2.参考1.CS224n视频链接，中英文字幕： https://www.bilibili.com/video/av41393758?t=4704&amp;p=2 2.CS224n视频链接，纯英文字幕： https://www.bilibili.com/video/av13383754?t=207&amp;p=2]]></content>
      <categories>
        <category>CS224n笔记</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[名人问题]]></title>
    <url>%2F2019%2F05%2F03%2Ffind-celebrity%2F</url>
    <content type="text"><![CDATA[1.题目描述 Leetcode 277. Find the Celebrity Suppose you are at a party with n people (labeled from 0 to n - 1) and among them, there may exist one celebrity. The definition of a celebrity is that all the other n - 1 people know him/her but he/she does not know any of them. Now you want to find out who the celebrity is or verify that there is not one. The only thing you are allowed to do is to ask questions like: “Hi, A. Do you know B?” to get information of whether A knows B. You need to find out the celebrity (or verify there is not one) by asking as few questions as possible (in the asymptotic sense). You are given a helper function bool knows(a, b) which tells you whether A knows B. Implement a function int findCelebrity(n), your function should minimize the number of calls to knows. Note: There will be exactly one celebrity if he/she is in the party. Return the celebrity’s label if there is a celebrity in the party. If there is no celebrity, return -1. 首先题目定义了：如果一个人A是名人，那么其他的n-1个人都认识A，而且A不认识其他的n-1个人。这题的目标是要从n个人中找出其中的名人，如果没有，则告知其中没有名人。我们可以调用knows(a,b)函数来询问a是否认识b，而我们要尽量少调用这个函数。 （1）n个人中最多可以有几个名人？ 答案：是1个。如果这n个人中有2个名人分别为A和B，那么按照定义，如果一个人A是名人，那么其他的n-1个人都认识A，而且A不认识其他的n-1个人，这也就是说A不认识B。但与此同时我们又定义了如果一个人B是名人，那么其他的n-1个人都认识B，那么A也应该认识B。这就产生了 contradiction了。因此其中不可以有2个或2个以上的名人。 (2) 如果其他n-1个人都认识A，但是A认识了n-1个人中其中一个人，那么A还是名人吗？ 答案：不是的。 (3) 如果A不认识其他的n-1个人，但是n-1个人中有人不认识A，那么A还是名人吗？ 答案：不是的。 这题最直接的想法应该是暴力，但是暴力的复杂度是多少呢？对于每个人我们需要询问: (1) 他是否认识剩下的n-1个人： 最坏的情况需要调用knows(a,b)函数n-1次 (2) 剩下的n-1个人是否认识他：最坏的情况需要调用knows(a,b)函数n-1次 有的可能会重复，但是总体来说需要询问的次数是n(n−1)2n(n−1)2。即时间复杂度为 O(n2n2) 有没有办法优化算法？ 如果我们从n个人中任意挑两个人a,b出来，询问啊是否认识b，那么只有两种情况： （1）a认识b：那么a肯定不是名人。 （2）b认识a：那么b肯定不是名人。 所以任何一种情况，我们都可以排除掉2个人中的1一个人。如果我们不断地重复的这个过程，直到只剩下一个人，那么我们会做n-1次对比。而剩下这个人是唯一可能成为名人的人，那么我们需要询问剩下的n-1个人是否认识他，也需要询问他是否认识剩下的n-1个人。因此我们一共需要询问3(n-1)次——时间复杂度为O(n)。 2.代码描述本题是参考的网友的博客: https://www.cnblogs.com/rgvb178/p/10117404.html 2.1 怎么样才算是名人？第1：假设某个名人Miss.Celebrity存在，则在这n人里面只会有一个人是名人。 第2：Miss.Celebrity 不认识剩下的n-1个人。正向思考：只要有一个人不认识其中某个人，他就有可能是名人，并且需要验证是否不认识剩下的n-1人。反向思考：若A认识B，则A就不是名人。 第3：所有人都认识此名人。反向思考：即只要某一位不被别人认识那此人就不是名人。其实就是名人在此处的定义为：”他/她 不认识其他所有人，但是其他所有人都认识他/她。” 2.2 python描述：123456789101112131415161718192021"""这里已经假设已有一个函数knows(a，b)，a认识b,函数返回True,否则返回False"""class Solution(object): def find_celebrity(self,n): # 考虑极端情况，即n=0，即没有人也就没名人（n也可能是其他怪异值） if n == 0: return -1 curr_stay = 0 #1.待验证的curr_stay认识i,证明curr_stay不是名人，需要验证后面一位大佬 == #2.等到knows(curr_stay,i)返回False,说明curr_stay可能是名人，于是往下验证 for i in range(1,n): if knows(curr_stay,i): curr_stay = i #3.对所有人验证curr_stay是不是名人？check2条：（1）curr_stay不认识所有人成立？（2）curr_stay被所有人认识成立？ for i in range(0,n): #自己都不认识自己了，那名人面子不要哒。&gt;o&lt; if i == curr_stay：continue #check:curr_stay若是名人,curr_stay不认识所有人返回False,不执行返回-1 if knows(curr_stay,i): return -1 #curr_stay若是名人,所有人认识curr_stay，not True不返回 if not knows(i,curr_stay): return -1 return curr_stay]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>名人问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基本的数据清洗]]></title>
    <url>%2F2019%2F04%2F26%2Fdata-cleaning%2F</url>
    <content type="text"><![CDATA[1.简介简单的把参考的2个微信讲解看了遍，贴出来标记一下。主要对数据检查是否有重复的，对缺失值统计和删除，最后利用KNN回归预测Titanic缺失数据年龄，以进行补偿。github地址：请点击这里 ，直径奔向简单的数据清洗即可。当然实际的数据清洗不会这么理想和简单的，数据量也远远少于实际量。 稍微简单说下最后一部分，用KNN来补偿缺失的年龄： 1.先把数据简单的删去无用的特征和特征类型转换 2.再分成2部分数据：一份年龄所在行数据都不缺失的，另一份是年龄是缺失的 3.KNN回归来把不缺失数据拿来训练：非年龄特征作为训练数据，年龄作为回归对象。然后到预测阶段，直接根据缺失数据的其他属性来回归预测实际年龄 2.代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798import pandas as pdfrom sklearn import neighbors#参考： https://www.jb51.net/article/60510.htm# 1. https://mp.weixin.qq.com/s?__biz=MzU0MDQ1NjAzNg==&amp;mid=2247486609&amp;idx=1&amp;sn=2865ab86b1ab0abf3e50e109bbf0ad14&amp;chksm=fb39a99acc4e208c3bbcb410742adca7839408fc4bed9f4aa2446d4f500fcaa360e388e3c6c8&amp;mpshare=1&amp;scene=23&amp;srcid=#rd# 2. https://mp.weixin.qq.com/s?__biz=MzU0MDQ1NjAzNg==&amp;mid=2247486622&amp;idx=1&amp;sn=d85a4866615f8a503151577abb28edcf&amp;chksm=fb39a995cc4e20835fec4a9dcb0ec9b0047265e24bc013e11e69a7b8c259deab0dbbf0e4f303&amp;mpshare=1&amp;scene=23&amp;srcid=#rd# 读入外部数据data3 = pd.read_excel(io=r'data3.xlsx') # pandas 更新到0.24即可# 1.查看数据的情况print(data3.shape)print(data3.dtypes)# 数值型转字符型data3['id'] = data3['id'].astype(str)# 字符型转数值型data3['custom_amt'] = data3['custom_amt'].str[1:].astype(float)# 字符型转日期型data3['order_date'] = pd.to_datetime(data3['order_date'], format = '%Y年%m月%d日')# 重新查看数据集的各变量类型print(data3.dtypes)print(data3.head())# 判断数据中是否存在重复数据print(data3.duplicated().any())'''需要说明的是，在使用duplicated“方法”对数据行作重复性判断时，会返回一个与原数据行数相同的序列（如果数据行没有重复，则对应False，否则对应True），为了得到最终的判断结果，需要再使用any“方法”（即序列中只要存在一个True，则返回True）。duplicated“方法”和drop_duplicates“方法”都有一个非常重要的参数，就是subset。默认情况下不设置该参数时，表示对数据的所有列进行重复性判断；如果需要按指定的变量做数据的重复性判断时，就可以使用该参数指定具体的变量列表。举例如下：'''df = pd.DataFrame(dict(name = ['张三','李四','王二','张三','赵五','丁一','王二'], gender = ['男','男','女','男','女','女','男'], age = [29,25,27,29,21,22,27], income = [15600,14000,18500,15600,10500,18000,13000], edu = ['本科','本科','硕士','本科','大专','本科','硕士']))# 默认情况下，对数据的所有变量进行判断df.drop_duplicates()print(df)df.drop_duplicates(subset=['name','age'],inplace=True) # name、age一样就判为一样的数据print(df,'\n####################################')'''2.缺失值的简单处理有：删除法、替换法和插补法'''# 判断各变量中是否存在缺失值print(data3.isnull().any(axis = 0))# 各变量中缺失值的数量print(data3.isnull().sum(axis = 0)) # axis为1代表统计行，0统计列#缺失值比例print(data3.isnull().sum(axis=0)/data3.shape[0])# 判断数据行中是否存在缺失值data3.isnull().any(axis = 1).any()# 删除字段 -- 如删除缺失率非常高的edu特征print(data3.drop(labels = 'edu', axis = 1, inplace=True))# 数据预览print(data3.head())# 删除观测，-- 如删除age变量中所对应的缺失观测data3_new = data3.drop(labels = data3.index[data3['age'].isnull()], axis = 0)# 查看数据的规模print(data3_new.shape)# 替换法处理缺失值data3.fillna(value = &#123;'gender': data3['gender'].mode()[0], # 使用性别的众数替换缺失性别 'age':data3['age'].mean() # 使用年龄的平均值替换缺失年龄 &#125;, inplace = True # 原地修改数据 )# 再次查看各变量的缺失比例print(data3.isnull().sum(axis = 0))'''3.KNN插补缺失值：以年龄不缺失的其他5特征为训练模型，训练时拟合已有年龄的数据，然后预测（拟合）缺失部分的年龄。'''# 读取数据titanic = pd.read_csv('Titanic.csv')# 删除缺失严重的Cabin变量titanic.drop(labels='Cabin', axis = 1, inplace=True)# 根据Embarked变量，删除对应的缺失行titanic.dropna(subset=['Embarked'], inplace=True)# 删除无关紧要的变量（这些变量对后面预测年龄没有太多的帮助）titanic.drop(labels=['PassengerId','Name','Ticket','Embarked'], axis = 1, inplace=True)# 将字符型的性别变量映射为数值变量titanic.Sex = titanic.Sex.map(&#123;'male':1, 'female':0&#125;)# 将数据拆分为两组，一是年龄缺失组，二是年龄非缺失组，后续基于非缺失值构建KNN模型，再对缺失组做预测nomissing = titanic.loc[~titanic.Age.isnull(),]missing = titanic.loc[titanic.Age.isnull(),]# X是除了age的所有列的index，即属性名字X = nomissing.columns[nomissing.columns != 'Age']# 构建模型knn = neighbors.KNeighborsRegressor()print('\n===========&gt;', nomissing[X].isnull().any(axis=1).any(), nomissing[X].isnull().any,nomissing[X].isnull().sum(axis=0))# 模型拟合knn.fit(nomissing[X], nomissing.Age)# 缺失表的年龄预测pred_age = knn.predict(missing[X])print(pred_age)# Titanic预测kaggle竞赛： https://www.jianshu.com/p/9b6ee1fb7a60?utm_campaign=maleskine&amp;utm_content=note&amp;utm_medium=seo_notes&amp;utm_source=recommendation 3.参考请点击：参考1 、参考2 $Je\ \ pense\ , \ donc\ \ je\ \ suis .$]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>数据清洗</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夸夸机器人]]></title>
    <url>%2F2019%2F04%2F20%2Fkuakua-robot%2F</url>
    <content type="text"><![CDATA[1.简介​ 按照网上的夸夸机器人进行了稍微的修改，按照微信性别来自动夸人。利用的是itchat API来自动回复的，没有技术含量。还看了一个用TF-IDF来匹配最相似的问题或者语料，然后匹配已经爬下来的夸夸回复，返回即可。这里只展示第一种，第二种不知道哪去了ozz 。这里是github地址，请点击这里 。对了，操作步骤是： 按照文末的参考1安装各种包 修改夸夸群名字 若系统是Mac、Linux选择enableCmdQR=2，运行程序 手机微信扫描run窗口的二维码，字体太大就调小点嘛，不知道？嘿嘿，不告诉你。 然后我被自己的智商拉低了100个高度，因为我发现登录账号怎么发咒语”夸我”都不灵，原来是要群成员发”夸我”，然后扫描的账号才从夸夸语料中抽取一条自动回复群成员 == ，刚开始以为是真的有随机账号来回复任意成员，没想到这个不是的。 然后就没了，尽情享受夸夸的乐趣吧！ 2.代码展示插个代码，以防github打不开或者打开太慢（其实是凑个页面==），没错，github打开贼慢。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345import itchat, refrom itchat.content import *import random""" Constants"""REPLY = &#123;'夸我0': ['你真是太优秀！', '啥也不说了,夸！', '每天看到你心情好呢！', '你真是一位可爱的小天使啊！', '一看你就是美丽与善良的化身 夸！', '你上辈子一定拯救了银河系吧，优秀！', '德才兼备说的就是你这样的社会主义接班人！', '以后你就是夸夸群里的元老，就是夸夸之父，简称夸父！', '你这句话完美的表达了你想被夸的坚定信念，你一定是一个执着追求自己理想的人！', '我不知道该怎样表达你留在我心中最强最深的印象，是你丰满颀长的身材，白皙的皮肤。乌黑幽深的眼睛，小巧红润的嘴唇，但还有一种说不出，捉不到的丰仪在煽动着我的心。', '遇见你之后，再看别的女人，就好象在侮辱自己的眼睛!', '你笑起来的样子最为动人，两片薄薄的嘴唇在笑，长长的眼睛在笑，腮上两个陷得很举动的酒窝也在笑。', '你从小就流露出才华横溢的天资来。', '仲老，真佩服，满腹经纶!这果然是奥妙!', '这番讲话，既有好教训又说得妙趣横生，给我们官兵以极深刻的印象。', '大贤世居大邦，见多识广，而且荣列胶庠，自然才贯二酉，学富五车了。', '庾信，字子山，南阳新野人也。……幼而俊迈，聪敏绝伦，博览群书，尤善《春秋左氏传》。', '人皆言子建出口成章，臣未深信。主上可召入，以才试之。', '俱怀逸兴壮思飞，欲上青天揽明月。', '这学生天资聪颖，文思敏捷，下笔成章，将来未可限量。', '远近书疏，莫不手答，笔翰如流未尝壅滞。', '你像一片轻柔的云在我眼前飘来飘去，你清丽秀雅的脸上荡漾着春天般美丽的笑容。', '你是那样地美，美得象一首抒情诗。', '你那瓜子形的形，那么白净，弯弯的一双眉毛，那么修长;水汪汪的一对眼睛，那么明亮!', '你总是说话得体，举止大方。不要吝啬赞美!因为你的赞美，对他人是一种鼓励，一种信任。', '因为你的赞美，对朋友可以传递友谊，因为你的赞美，对爱人可以体会到温情。不要吝啬赞美!因为你的赞美，对他人是一种鼓励，一种信任。因为你的赞美，对朋友可以传递友谊，因为你的赞美，对爱人可以体会到温情。', '你表现的很勇敢，是一个真正的男子汉!', '你表现这么优秀，和你在一起的时候压力好大啊!', '你的形体真健美!', '你的眼睛真有神!', '你工作的样子很迷人!', '我对你的表现非常满意。', '你就好像是上品的西湖龙井，那种淡淡的苦涩是你的成熟，越品你越有味道。', '尽管你身材纤弱娇小，说话柔声细气，然而却很有力量，这是一种真正的精神美!', '你娉婷婉约的风姿，娇艳俏丽的容貌，妩媚得体的举止，优雅大方的谈吐，一开始就令我刮目相看。', '你全身充溢着少女的纯情和青春的风采。留给我印象最深的是你那双湖水般清澈的眸子，以及长长的、一闪一闪的睫毛。像是探询，像是关切，像是问候。', '你是花丛中的蝴蝶，是百合花中的蓓蕾。无论什么衣服穿到你的身上，总是那么端庄、好看。', '你也许没有一簇樱唇两排贝齿，但你的谈吐也应该高雅脱俗，机智过人。', '你有点像天上的月亮，也像那闪烁的星星，可惜我不是诗人，否则，当写一万首诗来形容你的美丽。', '你看上去真精神。', '你是一个成熟的人!', '你是一个顾家的好男人。', '你是一个很专情的男人!', '你是一个有责任、有担当的男人!', '你是一个有责任心的人!', '瀑布一般的长发，淡雅的连衣裙，标准的瓜子脸，聪明的杏仁眼，那稳重端庄的气质，再调皮的人见了你都会小心翼翼。', '其实，我最先认识你是在照片上。照片上的你托腮凝眸，若有所思。那份温柔、那份美感、那份妩媚，使我久久难以忘怀。', '进步真快!', '你的观察力很强!', '你的想法很有创意!', '你的作品真棒!', '你很有想法哦!', '你真大方!', '你真能干!', '你真幽默!', '你真有思想!', '你很有想象力哦!', '智慧女人是金子，气质女人是钻石，聪明女人是宝藏，可爱女人是名画，据我考证，你是世界上最大的宝藏，里面装满了金子钻石名画!', '只有莲花才能比得上你的圣洁，只有月亮才能比得上你的冰清。', '在人流中，我一眼就发现了你。我不敢说你是她们中最漂亮的一个，可是我敢说，你是她们中最出色的一个。', '在你那双又大又亮的眼睛里，我总能捕捉到你的宁静，你的热烈，你的聪颖，你的敏感。', '你也许没有若隐若现的酒窝，但你的微笑一定是月闭花羞，鱼沉雁落。', '你也许没有水汪汪亮晶晶的眼睛，但你的眼神也应该顾盼多情，勾魂摄魄。', '你很有责任感哦!', '你今天的表现令大家都很开心!', '你今天给了我很多的惊喜!', '你学的真快!', '你真棒!', '你真聪明!', '你真的很能干哦!', '你真可爱!', '你真是个聪明的孩子!', '你做的非常好!', '你做对了', '做得好极了!', '你最近进步很大，继续保持。', '你的学习成绩提升的很快，希望你下面能再接再厉，再创辉煌!', '遇见你之后，再看别的女人，就好象在侮辱自己的眼睛!', '漂亮女孩，天生就漂亮。白皙的皮肤，大大的眼睛，秀气的鼻子，饱满的小嘴，再加上一头可爱的"自来卷"，构成一幅天然的美丽图画。', '清澈明亮的瞳孔，弯弯的柳眉，长长的睫毛微微地颤动着，白皙无瑕的皮肤透出淡淡红粉，薄薄的双唇如玫瑰花瓣娇嫩欲滴。', '若说她年纪轻轻，怎生得如此身段，且有一张勾魂摄魄的俏脸。', '那眼神优雅、娴静，双眼回盼流波，像是俏丽的江南女子;但又挂着一丝倔犟的波纹，又带着北国女儿的神韵。', '清水出芙蓉，天然去雕饰。——李白', '同样是美女，这个女孩给人最深刻的印象是她眉宇之间有种超越了她年龄的惊人的美丽，淡淡的柳眉分明仔细的修饰过，长长的睫毛忽闪忽闪的象两把小刷子，亮得让人觉得刺目的一双漂亮到心悸的大眼睛，异常的灵动有神。', '细致乌黑的长发，常常披于双肩之上，略显柔美，有时松散的数着长发，显出一种别样的风采，突然由成熟变得可爱，让人新生喜爱怜惜之情，洁白的皮肤犹如刚剥壳的鸡蛋，大大的眼睛一闪一闪仿佛会说话，小小的红唇与皮肤的白色，更显分明，一对小酒窝均匀的分布在脸颊两侧，浅浅一笑，酒窝在脸颊若隐若现，可爱如天仙。', '皓腕胜雪，乌发如云，她的眼眸水光潋滟、媚眼如丝，一双勾魂的眼，只一眼，就完全沉溺其中不可自拔。高挺鼻子下的那张玫色小嘴微微张着，如同妖艳的玫瑰。她的面容如娇嫩清雅，犹如杯中之莲，绝色之姿灵气逼人。那美，用怎样的辞藻来形容都是苍白而无力，真是只可意会，不可言传。她静若处子，似不食人间烟火，动若脱兔，眉间微存的稚气带着无比的灵动。', '世上美人众多，肥环燕瘦，无一人有她那样独特的气质。孤傲、无畏、自信、有着一股不羁的野性，她是最璀璨的光华结晶，如同一团烈火，激烈且张狂地燃烧着。她让美丽不再只是容貌上，而是由心真正的散发出来，她紧抓着众人的眼。再加之，一袭冰蚕丝纱裙所衬托出的空灵气质，与在阳光照射下而形成的淡淡光晕，更若天女下凡，绝美无双!', '一张清爽的鹅蛋脸，一头柔顺的直发。高挺的鼻梁，粉玫色的唇瓣，精致的单眼皮搭配着纤长的眼睫，微敛住深褐色的眼眸，闪烁着快乐的光芒，但光芒的深处，是一股浓到化不开的孤寂。', '双眉有如柳叶刀裁，盈盈笑意眉上来，一句“云髻峨峨，修眉联娟”得以道出碧瑶云云细眉。', '双目似有千情万怨，道不尽也诉不完，一句“巧笑倩兮，美目盼兮”可能描述碧瑶盈盈眼波。', '她那双大大的眼睛，闪烁着聪颖的光辉，像两颗朗朗的星星。', '她和她的哥哥一样，生就一副绝顶聪明的头脑，心灵得像窗纸，一点就透。', '聪明的人不是具有广博知识的人，而是掌握了有用知识的人。', '你那双乌黑晶亮的眼睛，骨碌碌地打转，显得很机灵懂事。', '别看你人小，心眼可灵啦，10个大人也比不上你，真是秤砣虽小能吊千斤。', '你是个聪明的孩子，精灵得像个小猴儿。', '别看阿墩胖得肥肉直打颤颤，动作却机警得像只与猎人周旋的豹子。', '她那双圆溜溜的大眼睛，镶了一圈乌黑闪亮的长睫毛，眨动之间，透出一股聪明伶俐劲儿。', '在午后的阳光下，没有丝毫红晕，清秀的脸上只显出了一种病态的苍白，却无时不流露出高贵淡雅的气质，配合你颀长纤细的身材。' '一张坏坏的笑脸，连两道浓浓的眉毛也泛起柔柔的涟漪，好像一直都带着笑意，弯弯的，像是夜空里皎洁的上弦月。白皙的皮肤衬托着淡淡桃红色的嘴唇，俊美突出的五官，完美的脸型，特别是左耳闪着炫目光亮的钻石耳钉，给你的阳光帅气中加入了一丝不羁。', '我和你在一起的时候压力好大啊!谁让你这么优秀啦!', '你乌灵的眼眸，倏地笼上层嗜血的寒意，仿若魔神降世一般，一双冰眸轻易贯穿人心，刺透心底最柔弱，舞衣的角落。', '你那红嘟嘟地脸蛋闪着光亮，像九月里熟透地苹果一样。', '你的耳朵白里透红，耳轮分明，外圈和里圈很匀称，像是一件雕刻出来地艺术品。', '你的眉毛时而紧紧地皱起，眉宇间形成一个问号;时而愉快地舒展，像个感叹号。', '浓密的眉毛叛逆地稍稍向上扬起，长而微卷的睫毛下，有着一双像朝露一样清澈的眼睛，英挺的鼻梁，像玫瑰花瓣一样粉嫩的嘴唇，还有白皙的皮肤。', '你有时候是不是特孤独?世界上这么优秀的人就只有你一个!', '你就好像是上品的西湖龙井那种淡淡的苦涩是你的成熟越品你越有味道。', '那个男子立体的五官刀刻般俊美，整个人发出一种威震天下的王者之气，邪恶而俊美的脸上此时噙着一抹放荡不拘的微笑。'], '夸我1': ['你真是太优秀！', '啥也不说了，夸！', '每天看到你心情好呢！', '你真是一位帅气的man！', '你上辈子一定拯救了银河系吧，优秀！', '德才兼备说的就是你这样的社会主义接班人！', '以后你就是夸夸群里的元老，就是夸夸之父，简称夸父！', '你这句话完美的表达了你想被夸的坚定信念，你一定是一个执着追求自己理想的人！', '你从小就流露出才华横溢的天资来。', '仲老，真佩服，满腹经纶!这果然是奥妙!', '这番讲话，既有好教训又说得妙趣横生，给我们以极深刻的印象。', '大贤世居大邦，见多识广，而且荣列胶庠，自然才贯二酉，学富五车了。', '庾信，字子山，南阳新野人也。……幼而俊迈，聪敏绝伦，博览群书，尤善《春秋左氏传》。', '人皆言子建出口成章，臣未深信。主上可召入，以才试之。', '俱怀逸兴壮思飞，欲上青天揽明月。', '这学生天资聪颖，文思敏捷，下笔成章，将来未可限量。', '远近书疏，莫不手答，笔翰如流未尝壅滞。', '你像一片轻柔的云在我眼前飘来飘去，你清丽秀雅的脸上荡漾着春天般美丽的笑容。', '因为你的赞美，对朋友可以传递友谊，因为你的赞美，对爱人可以体会到温情。不要吝啬赞美!因为你的赞美，对他人是一种鼓励，一种信任。因为你的赞美，对朋友可以传递友谊，因为你的赞美，对爱人可以体会到温情。', '你表现的很勇敢，是一个真正的男子汉!', '你表现这么优秀，和你在一起的时候压力好大啊!', '你的形体真健美!', '你的眼睛真有神!', '你工作的样子很迷人!', '我对你的表现非常满意。', '你就好像是上品的西湖龙井，那种淡淡的苦涩是你的成熟，越品你越有味道。', '你是一个成熟的人!', '你是一个顾家的好男人。', '你是一个很专情的男人!', '你是一个有责任、有担当的男人!', '你是一个有责任心的人!', '你进步真快!', '你的观察力很强!', '你的想法很有创意!', '你的作品真棒!', '你很有想法哦!', '你真大方!', '你真能干!', '你真幽默!', '你真有思想!', '你很有想象力哦!', '你很有责任感哦!', '你今天的表现令大家都很开心!', '你今天给了我很多的惊喜!', '你学的真快!', '你真棒!', '你真聪明!', '你真的很能干哦!', '你真可爱!', '你真是个聪明的孩子!', '你做的非常好!', '你做对了', '做得好极了!', '你最近进步很大，继续保持。', '你的学习成绩提升的很快，希望你下面能再接再厉，再创辉煌!', '你就好像是上品的西湖龙井那种淡淡的苦涩是你的成熟越品你越有味道。', '那个男子立体的五官刀刻般俊美，整个人发出一种威震天下的王者之气，邪恶而俊美的脸上此时噙着一抹放荡不拘的微笑。'], '夸我2': ['你真是太优秀！', '啥也不说了，夸！', '每天看到你心情好呢！', '你真是一位可爱的小天使啊！', '一看你就是美丽与善良的化身 夸！', '你上辈子一定拯救了银河系吧，优秀！', '德才兼备说的就是你这样的社会主义接班人！', '以后你就是夸夸群里的元老，就是夸夸之父，简称夸父！', '你这句话完美的表达了你想被夸的坚定信念，你一定是一个执着追求自己理想的人！', '我不知道该怎样表达你留在我心中最强最深的印象，是你丰满颀长的身材，白皙的皮肤。乌黑幽深的眼睛，小巧红润的嘴唇，但还有一种说不出，捉不到的丰仪在煽动着我的心。', '遇见你之后，眼睛就容不下其他女生了!', '你笑起来的样子最为动人，两片薄薄的嘴唇在笑，长长的眼睛在笑，腮上两个陷得很举动的酒窝也在笑。', '你从小就流露出才华横溢的天资来。', '庾信，字子山，南阳新野人也。……幼而俊迈，聪敏绝伦，博览群书，尤善《春秋左氏传》。', '人皆言子建出口成章，臣未深信。主上可召入，以才试之。', '俱怀逸兴壮思飞，欲上青天揽明月。', '这学生天资聪颖，文思敏捷，下笔成章，将来未可限量。', '远近书疏，莫不手答，笔翰如流未尝壅滞。', '你像一片轻柔的云在我眼前飘来飘去，你清丽秀雅的脸上荡漾着春天般美丽的笑容。', '你是那样地美，美得象一首抒情诗。', '你那瓜子形的形，那么白净，弯弯的一双眉毛，那么修长;水汪汪的一对眼睛，那么明亮!', '你总是说话得体，举止大方。不要吝啬赞美!因为你的赞美，对他人是一种鼓励，一种信任。', '因为你的赞美，对朋友可以传递友谊，因为你的赞美，对爱人可以体会到温情。不要吝啬赞美!因为你的赞美，对他人是一种鼓励，一种信任。因为你的赞美，对朋友可以传递友谊，因为你的赞美，对爱人可以体会到温情。', '你表现这么优秀，和你在一起的时候压力好大啊!', '你的眼睛真有神!', '你工作的样子很迷人!', '我对你的表现非常满意。', '尽管你身材纤弱娇小，说话柔声细气，然而却很有力量，这是一种真正的精神美!', '你娉婷婉约的风姿，娇艳俏丽的容貌，妩媚得体的举止，优雅大方的谈吐，一开始就令我刮目相看。', '你全身充溢着少女的纯情和青春的风采。留给我印象最深的是你那双湖水般清澈的眸子，以及长长的、一闪一闪的睫毛。像是探询，像是关切，像是问候。', '你是花丛中的蝴蝶，是百合花中的蓓蕾。无论什么衣服穿到你的身上，总是那么端庄、好看。', '你也许没有一簇樱唇两排贝齿，但你的谈吐也应该高雅脱俗，机智过人。', '你有点像天上的月亮，也像那闪烁的星星，可惜我不是诗人，否则，当写一万首诗来形容你的美丽。', '你看上去真精神。', '你是一个成熟的人!', '你是一个有责任心的人!', '瀑布一般的长发，淡雅的连衣裙，标准的瓜子脸，聪明的杏仁眼，那稳重端庄的气质，再调皮的人见了你都会小心翼翼。', '其实，我最先认识你是在照片上。照片上的你托腮凝眸，若有所思。那份温柔、那份美感、那份妩媚，使我久久难以忘怀。', '进步真快!', '你的观察力很强!', '你的想法很有创意!', '你的作品真棒!', '你很有想法哦!', '你真大方!', '你真能干!', '你真幽默!', '你真有思想!', '你很有想象力哦!', '智慧女人是金子，气质女人是钻石，聪明女人是宝藏，可爱女人是名画，据我考证，你是世界上最大的宝藏，里面装满了金子钻石名画!', '只有莲花才能比得上你的圣洁，只有月亮才能比得上你的冰清。', '在人流中，我一眼就发现了你。我不敢说你是她们中最漂亮的一个，可是我敢说，你是她们中最出色的一个。', '在你那双又大又亮的眼睛里，我总能捕捉到你的宁静，你的热烈，你的聪颖，你的敏感。', '你也许没有若隐若现的酒窝，但你的微笑一定是月闭花羞，鱼沉雁落。', '你也许没有水汪汪亮晶晶的眼睛，但你的眼神也应该顾盼多情，勾魂摄魄。', '你很有责任感哦!', '你今天的表现令大家都很开心!', '你今天给了我很多的惊喜!', '你学的真快!', '你真棒!', '你真聪明!', '你真的很能干哦!', '你真可爱!', '你真是个聪明的孩子!', '你做的非常好!', '你做对了', '做得好极了!', '你最近进步很大，继续保持。', '你的学习成绩提升的很快，希望你下面能再接再厉，再创辉煌!', '遇见你之后，再看别的女人，就好象在侮辱自己的眼睛!', '漂亮女孩，天生就漂亮。白皙的皮肤，大大的眼睛，秀气的鼻子，饱满的小嘴，再加上一头可爱的"自来卷"，构成一幅天然的美丽图画。', '清澈明亮的瞳孔，弯弯的柳眉，长长的睫毛微微地颤动着，白皙无瑕的皮肤透出淡淡红粉，薄薄的双唇如玫瑰花瓣娇嫩欲滴。', '那眼神优雅、娴静，双眼回盼流波，像是俏丽的江南女子;但又挂着一丝倔犟的波纹，又带着北国女儿的神韵。', '清水出芙蓉，天然去雕饰。', '同样是美女，这个女孩给人最深刻的印象是她眉宇之间有种超越了她年龄的惊人的美丽，淡淡的柳眉分明仔细的修饰过，长长的睫毛忽闪忽闪的象两把小刷子，亮得让人觉得刺目的一双漂亮到心悸的大眼睛，异常的灵动有神。', '细致乌黑的长发，常常披于双肩之上，略显柔美，有时松散的数着长发，显出一种别样的风采，突然由成熟变得可爱，让人新生喜爱怜惜之情，洁白的皮肤犹如刚剥壳的鸡蛋，大大的眼睛一闪一闪仿佛会说话，小小的红唇与皮肤的白色，更显分明，一对小酒窝均匀的分布在脸颊两侧，浅浅一笑，酒窝在脸颊若隐若现，可爱如天仙。', '世上美人众多，肥环燕瘦，无一人有她那样独特的气质。孤傲、无畏、自信、有着一股不羁的野性，她是最璀璨的光华结晶，如同一团烈火，激烈且张狂地燃烧着。她让美丽不再只是容貌上，而是由心真正的散发出来，她紧抓着众人的眼。再加之，一袭冰蚕丝纱裙所衬托出的空灵气质，与在阳光照射下而形成的淡淡光晕，更若天女下凡，绝美无双!', '一张清爽的鹅蛋脸，一头柔顺的直发。高挺的鼻梁，粉玫色的唇瓣，精致的单眼皮搭配着纤长的眼睫，微敛住深褐色的眼眸，闪烁着快乐的光芒，但光芒的深处，是一股浓到化不开的孤寂。', '双眉有如柳叶刀裁，盈盈笑意眉上来，一句“云髻峨峨，修眉联娟”得以道出碧瑶云云细眉。', '双目似有千情万怨，道不尽也诉不完，一句“巧笑倩兮，美目盼兮”可能描述碧瑶盈盈眼波。', '她那双大大的眼睛，闪烁着聪颖的光辉，像两颗朗朗的星星。', '聪明的人不是具有广博知识的人，而是掌握了有用知识的人。', '你那双乌黑晶亮的眼睛，骨碌碌地打转，显得很机灵懂事。', '她那双圆溜溜的大眼睛，镶了一圈乌黑闪亮的长睫毛，眨动之间，透出一股聪明伶俐劲儿。', '一张坏坏的笑脸，连两道浓浓的眉毛也泛起柔柔的涟漪，好像一直都带着笑意，弯弯的，像是夜空里皎洁的上弦月。白皙的皮肤衬托着淡淡桃红色的嘴唇，俊美突出的五官，完美的脸型，特别是左耳闪着炫目光亮的钻石耳钉，给你的阳光帅气中加入了一丝不羁。', '我和你在一起的时候压力好大啊!谁让你这么优秀啦!', '你乌灵的眼眸，倏地笼上层嗜血的寒意，仿若魔神降世一般，一双冰眸轻易贯穿人心，刺透心底最柔弱，舞衣的角落。', '你那红嘟嘟地脸蛋闪着光亮，像九月里熟透地苹果一样。', '你的耳朵白里透红，耳轮分明，外圈和里圈很匀称，像是一件雕刻出来地艺术品。'], 'default': ['太棒了！', '真不错！', '好开心！', '嗯哪！', '没什么好说的了，我送你一道彩虹吧！']&#125;@itchat.msg_register([TEXT], isGroupChat=True)def text_reply(msg): # group_name的值修改成你要夸的weixin群名 group_name = '桔子次苹果' if msg['User']['NickName'] == group_name: print('Message from: %s' % msg['User']['NickName']) # 发送者的昵称 username = msg['ActualNickName'] print('Who sent it: %s' % username) match = re.search('夸我', msg['Text']) or re.search('求夸', msg['Text']) user_info = itchat.search_friends(name='&#123;&#125;'.format(username)) print('user_info type是：', type(user_info)) user_info_str = str(user_info) print('user_info变更后type是：', type(user_info)) print('用户信息====', user_info) print('len(REPLY[\'夸我2\'])=', len(REPLY['夸我2'])) print('匹配到性别为：', re.search('\'Sex\': 1', user_info_str)) if match: # 获取发消息人的性别，然后依据性别来夸人。未标注的就随机夸。 if re.search('\'Sex\': 1', user_info_str) is not None: print('-+-+' * 5) print('Message content:%s' % msg['Content']) print('夸我 is: %s' % (match is not None)) randomIdx = random.randint(0, len(REPLY['夸我1']) - 1) itchat.send('%s' % (REPLY['夸我1'][randomIdx]), msg['FromUserName']) elif re.search('\'Sex\': 2', user_info_str) is not None: print('-+-+' * 5) print('Message content:%s' % msg['Content']) print('夸我 is: %s' % (match is not None)) randomIdx = random.randint(0, len(REPLY['夸我2']) - 1) itchat.send( '%s' % (REPLY['夸我2'][randomIdx]), msg['FromUserName']) else: print('-+-+' * 5) print('Message content:%s' % msg['Content']) print('夸我 is: %s' % (match is not None)) randomIdx = random.randint(0, len(REPLY['夸我0']) - 1) itchat.send('%s' % (REPLY['夸我0'][randomIdx]), msg['FromUserName']) print('isAt is:%s' % msg['isAt']) if msg['isAt']: randomIdx = random.randint(0, len(REPLY['default']) - 1) itchat.send('@' + '%s\n%s' % (username, REPLY['default'][randomIdx]), msg['FromUserName']) print('-+-+'*5)# Windows系统，enableCmdQR=Trueitchat.auto_login(enableCmdQR=True, hotReload=True)# 若操作系统是Mac、Linux，enableCmdQR=2# itchat.auto_login(enableCmdQR=2, hotReload=True)itchat.run()'''# 获取好友列表friends = itchat.get_friends()[0:]# 男性male = 0# 女性female = 0# 未知性别other = 0for i in friends[1:]: sex = i['Sex'] if sex == 1: male += 1 elif sex == 2: female += 1 else: other += 1# 微信好友数量total = len(friends[1:])print("微信好友总数:%d" % (total))print("男性好友:%.2f%%" % (float(male) / total * 100))print("女性好友:%.2f%%" % (float(female) / total * 100))print("未知性别好友:%.2f%%" % (float(other) / total * 100))''' 3.参考1.参考blog： https://www.cnblogs.com/geeksongs/p/10581302.html 2.itchart说明文档: https://itchat.readthedocs.io/zh/latest/]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>夸夸机器人</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bagging简介]]></title>
    <url>%2F2019%2F03%2F25%2Fbagging%2F</url>
    <content type="text"><![CDATA[1.Bagging算法介绍1.1 思想介绍介绍完AdaBoost，来学习下Bagging算法，这是集成算法里面另一个集大成者，代表有随机森林等算法。先看看Bagging是什么。它是并行式集成学习最著名的代表作，基于自主采样法（bootstrap sampling）。给定m个样本的数据集，随机抽取一个样本放入采样集，再把样本放回初始数据集，使得下次再有可能抽到此样本。这样经过m次抽样得到含m个样本的采样集；但是原始样本集里面有的样本被多次抽到，有的则一次也没有抽到。这样，我们可以得到T个包含m个样本的训练集，然后每一个训练集训练一个基学习器，再将这些基学习器进行结合即可。一般做分类的话就是最后基学习器的投票多数类即为最后预测类标，若做回归任务，通常使用简单平均法，对T个弱学习器得到的回归结果进行算术平均得到最终的模型输出。 1.2 Bagging算法流程输入为样本集$D={(x_1,y_1),(x_2,y_2),…(x_m,y_m)}$，弱学习器算法, 弱分类器迭代次数T。输出为最终的强分类器$f(x)$ 1）对于$t=1,2…,T$： a)对训练集进行第$m$次随机采样，共采集$t$轮，得到包含$m$个样本的采样集$D_t$ b)用采样集$D_t$训练第$t$个弱学习器$G_t(x)$ 2) 如果是分类算法预测，则$T$个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，$T$个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。 1.3 自助采样法的采样极限因为Bagging的子采样是放回采样，所以有部分是一直采不到的。我们简单计算一下这个”踩不到”的部分有多少。因为是有放回的采样，所以每次从m个样本中采样一个样本的概率是1/m，那踩不到的概率就是1-1/m，有这样m次采样，所以所有踩不到的概率是$(1-\frac{1}{m})^m$，由第二重要极限$\lim\limits_{m \rightarrow \infty} (1+\frac{1}{m})^m=e$ 知： \lim\limits_{m \rightarrow \infty} (1-\frac{1}{m})^m=\lim\limits_{m \rightarrow \infty} [(1+\frac{1}{-m})^{（-m）}]^{-1}=\frac{1}{e}≈0.368就是说有36.8%的样本会没有抽到，即没有用于训练。 顺便一提，假定基学习器的计算复杂度是$O(m)$，则Bagging的复杂度是$T(O(m)+O(s))$，但是参与投票/平均的过程$O(s)$和T都不是很大，所以训练一个Bagging集成和直接使用基学习器算法训练一个学习器的复杂度是同阶的。故Bagging是一个高效的集成学习算法。 2.随机森林随机森林（Random Forest）是Bagging一个开展体。传统决策树在选择划分属性时是在当前节点的属性集合里面选择最优的一个属性；而在RF中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集里面选择一个最优的属性用于划分。这里的k控制了随机性的引入程度。若k=d，则基决策树的构建与传统决策树一致，若令k=1,则随机选择一个属性用于划分；一般地，推荐使用$k=log_2d$。 随机森林简单、容易实现、计算开销小、性能也是优的。 3.Bagging和Boosting的区别？（1）Bagging，Boosting二者之间的区别请参考：https://www.cnblogs.com/earendil/p/8872001.html （2）如何理解bagging是减少variance，而boosting是减少bias?请参考：https://blog.csdn.net/anshuai_aw1/article/details/82911612 boosting是把许多弱的分类器组合成一个强的分类器。弱的分类器bias高，而强的分类器bias低，所以说boosting起到了降低bias的作用。variance不是boosting的主要考虑因素。bagging是对许多强（甚至过强）的分类器求平均。在这里，每个单独的分类器的bias都是低的，平均之后bias依然低；而每个单独的分类器都强到可能产生overfitting的程度，也就是variance高，求平均的操作起到的作用就是降低这个variance。参考链接：https://www.zhihu.com/question/26760839/answer/34178436，来源：知乎。 4.决策树和随机森林代码可耻的掉包了，嗯，按照最后参考中的第三个书籍，需要对照书本Robert Layton著作，杜春晓翻译的《Python数据碗挖掘入门与实践》P31：第三章节来看。这里贴上代码github地址： 请点击这里 ，下面是代码展示，数据集请去GitHub下载。主要是提取特定的特征然后进行NBA球赛的分类预测，分类器用到了决策树、随机森林。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145import pandas as pdfrom collections import defaultdictfrom sklearn.grid_search import GridSearchCVfrom sklearn.model_selection import cross_val_scorefrom sklearn.tree import DecisionTreeClassifierimport numpy as npfrom sklearn.preprocessing import LabelEncoderfrom sklearn.preprocessing import OneHotEncoderfrom sklearn.ensemble import RandomForestClassifier"""用决策树预测获胜球队"""dataset = pd.read_csv('sportsref_download (1).csv', parse_dates=["Date"], usecols=[0,1,2,3,4,5,6,9]) #read_csv()参数介绍：https://www.cnblogs.com/datablog/p/6127000.htmldataset.columns = ['Date', 'Score Type', 'Visitor Team', 'VisitorPts', 'Home Team', 'HomePts', 'OT?', 'Notes'] # 修改列名字# # Note：筛选某一列名的所在行# for i in range(len(dataset)):# if dataset['Score Type'][i] == '8:00p':# print(dataset[i:i+1])# 主场胜就是1，否则客场胜为0dataset['HomeWin'] = dataset["VisitorPts"] &lt; dataset["HomePts"]y_true = dataset["HomeWin"].values# 新增2列值dataset["HomeLastWin"] = Falsedataset["VisitorLastWin"] = False# print(dataset.ix[:5])print("Home Win percentage: &#123;0:.1f&#125;%".format(100 * dataset["HomeWin"].sum() / dataset["HomeWin"].count()))# 遍历所有数据，并设置主、客场上一场的胜负情况数据won_last = defaultdict(int)for index, row in dataset.iterrows(): # Note that this is not efficient home_team_name = row["Home Team"] visitor_team_name = row["Visitor Team"] row["HomeLastWin"] = won_last[home_team_name] # 初始化都是False row["VisitorLastWin"] = won_last[visitor_team_name] dataset.ix[index] = row # Set current win won_last[home_team_name] = row["HomeWin"] # 每一次上次某行的主、客队伍胜负情况都会被当前的覆盖 won_last[visitor_team_name] = not row["HomeWin"]# print(dataset.ix[:5])# ########### No1. 2列特征做训练# 其取值不变时，用相同的训练集建树得到的结果一模一样，对测试集的预测结果也是一样的；其值改变时，得到的结果不同；若不设置此参数，则函数会自动选择一种随机模式，每次得到的结果也就不同。clf = DecisionTreeClassifier(random_state=12)X_previouswins = dataset[['HomeLastWin', 'VisitorLastWin']].values # 把数据集中某几列凑成list单独拿出来scores = cross_val_score(clf, X_previouswins, y_true, cv=10, scoring='accuracy')print('主客队上场比赛结果为特征的Accuracy:&#123;0:.1f&#125;%'.format(np.mean(scores) * 100))# ######## No2. 用2013年战绩来添加一个“主场对是否比对手水平高”的属性,3列特征做训练standing = pd.read_csv("expend_standings.csv", skiprows=[0]) # 表名字不要# print("取出某球队名字的行的所在'Team'列的值：", standing[standing['Team'] == 'Denver Nuggets']["Rk"].values[0])dataset['HomeTeamRanksHigher'] = 0for index, row in dataset.iterrows(): home_team_name = row["Home Team"] visitor_team_name = row["Visitor Team"] if home_team_name == "New Orleans Pelicans": # 检查名字是否更名 home_team_name = "New Orleans Hornets" elif visitor_team_name == "New Orleans Pelicans": visitor_team_name = "New Orleans Hornets" home_rank = standing[standing['Team'] == home_team_name]["Rk"].values[0] visitor_rank = standing[standing['Team'] == visitor_team_name]["Rk"].values[0] row['HomeTeamRanksHigher'] = int(home_rank &gt; visitor_rank) dataset.ix[index] = row# print(dataset.ix[:5])# No3.再添加一个新的特征：上一场比赛情况X_homehigher = dataset[["HomeLastWin", "VisitorLastWin", "HomeTeamRanksHigher"]].valuesclf = DecisionTreeClassifier(random_state=14)scores = cross_val_score(clf, X_homehigher, y_true, scoring="accuracy")print("添加比赛等级特征后的Accuracy:&#123;0:.1f&#125;".format(np.mean(scores) * 100))last_math_winer = defaultdict(int)dataset["HomeTeamWonLast"] = 0for index, row in dataset.iterrows(): home_team = row["Home Team"] visitor_team = row["Visitor Team"] teams = tuple(sorted([home_team, visitor_team])) # 球队名字按照字母排序，上一场比赛的赢得为键 row["HomeTeamWonLast"] = 1 if last_math_winer[teams] == row["Home Team"] else 0 dataset.ix[index] = row winner = row["Home Team"] if row["HomeWin"] else row["Visitor Team"] last_math_winer[teams] = winnerX_lastwinner = dataset[["HomeTeamRanksHigher", "HomeTeamWonLast"]].valuesclf = DecisionTreeClassifier(random_state=14)scores = cross_val_score(clf, X_lastwinner, y_true, scoring="accuracy")print("根据上次比赛胜负和主队以往排名为特征的Accuracy:&#123;0:.1f&#125;%".format(np.mean(scores) * 100))# No4.球队名字用独热码表示encoding = LabelEncoder()encoding.fit(dataset["Home Team"].values)home_teams = encoding.transform(dataset["Home Team"].values)visitor_teams = encoding.transform(dataset["Visitor Team"].values)x_teams = np.vstack([home_teams, visitor_teams]).T# print(x_teams)onehot = OneHotEncoder()x_teams_expand = onehot.fit_transform(x_teams).todense()clf = DecisionTreeClassifier(random_state=14)scores = cross_val_score(clf, x_teams_expand, y_true, scoring="accuracy")print("球队名字用独热码表示后：Accuracy:&#123;0:.1f&#125;%".format(np.mean(scores) * 100))# ######################## 随机森林 ####################################clf = RandomForestClassifier(random_state=14)score = cross_val_score(clf, x_teams_expand, y_true, scoring="accuracy")print("随机森林做分类器后的Accuracy：&#123;0:.1f&#125;%".format(np.mean(score)*100))# 多加入几个特征试试效果x_all = np.hstack([X_homehigher, x_teams_expand])print('测试特征有：', np.shape(x_all))clf = RandomForestClassifier(random_state=14)score = cross_val_score(clf, x_all, y_true, scoring="accuracy")print("随机森林后的Accuracy：&#123;0:.1f&#125;%".format(np.mean(score)*100))parameter_space = &#123; "max_features": [2, 10, 'auto'], "n_estimators": [100], "criterion": ["gini", "entropy"], "min_samples_leaf": [2, 4, 6] &#125;clf = RandomForestClassifier(random_state=14)grid = GridSearchCV(clf, parameter_space)grid.fit(x_all, y_true)print("随机森林+网格搜索后的Accuracy：&#123;0:.1f&#125;%".format(grid.best_score_ * 100))print(grid.best_params_)# 参数最好的设置clf = RandomForestClassifier(bootstrap=True, random_state=14, criterion="entropy", max_depth=None, max_features=2, max_leaf_nodes=None, min_samples_leaf=6, min_samples_split=2, n_estimators=100, n_jobs=2, oob_score=False, verbose=0, )score = cross_val_score(clf, x_all, y_true, scoring="accuracy")print("随机森林参数设置后的Accuracy：&#123;0:.1f&#125;%".format(np.mean(score)*100)) 5.参考 老刘的博客： https://www.cnblogs.com/pinard/p/6156009.html 周志华《机器学习》 Robert Layton著作，杜春晓翻译的《Python数据碗挖掘入门与实践》，人民邮电出版社 总有一天，这世界会再次焕发光彩，我相信，一定会到来。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Bagging</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性判别分析]]></title>
    <url>%2F2019%2F03%2F21%2FLDA%2F</url>
    <content type="text"><![CDATA[1.线性判别分析介绍线性判别分析（Linear Discriminant Analysis，LDA）是一种经典的线性学习方法，可以分类和预测。还有一个也叫LDA是主题模型里面的，这里不提。简单的说此LDA就是如下图所示，把同一类的点都尽可能地集中某条线段上，不同类的尽可能分开。总结：同类的投影点越集中，异类的投影点要越分开。 2.LDA二分类证明主要参考周志华的西瓜书和这个链接就直接上笔记，打公式太麻烦，嗯。 上图中因为分子分母都是关于$w$的二次项，所以解与$w$的长度无关，只与它的方向有关，所以令$w^Ts_ww=1$以简化计算。具体参考《机器学习》P61. 3.代码参考代码请参考： 此链接 ，已有数据集。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>线性判别分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AdaBoost简介]]></title>
    <url>%2F2019%2F03%2F14%2FAdaBoost%2F</url>
    <content type="text"><![CDATA[1.基本概念 我们可以对集成学习的思想做一个概括。对于训练集数据，我们通过训练若干个个体学习器，通过一定的结合策略，就可以最终形成一个强学习器。 个体学习器 由一些异质集成的个体学习器由不同算法构成，通常叫做“组件学习器”，有时候直接叫做个体学习器。 基学习器 就是同质的（同一个算法或者相似算法）学习算法集成在一起个体学习器叫做“基学习器”。 比如下图三个基学习器通过一定的结合策略，组合成一个强学习器。 弱分类器 就是通常是指的是泛化性能略优于随机猜测的学习器，基学习器有时候也被称为弱学习器，集成后，通常来讲组合的弱学习器有一个很好的提升。 对基学习器的期望 我们希望每个基学习器好而不同，就是性能很好，并且保持多样性。比如下图（a）中的三个基学习器在不同样本间分类正确（打勾表示分类正确，否则错误），集成后子分类器的准确率是66%，但是集成后是100%。效果提升。（b）并没有提升最终效果。（c）效果从33%降到了0，起负作用了。 2.AdaBoost介绍2.1 思想集成学习大致分为2类，一个是个体学习器之间存在强的依赖关系、必须串行生成的序列方法；另一种是个体学习器之间不存在强依赖关系、可同时生成的并行化方法。前者代表是AdaBoost算法，后者代表是Bagging和随即森林。 Boosting是一族可以将弱学习器提升为强学习器的算法。这族算法的工作机制类似：先从初始训练集训练出一个基学习器，使得先前的基学习器做错的训练样本在后面受到更加多的关注，然后基于调整过后的样本分布来训练下一个基学习器；如此往复进行，直到基学习器数目达到事先预定的数量T值，最终我们将这些T个基学习器进行加权结合。 注意2点：AdaBoost有两种权值的调整，一种是针对分类错误的样本，会把分错的样本在下一个基分类器训练前提高权重；另一个是针对基分类ht，对分类错误率较高的基分类器也会降低权重。参考下图：显然这是个串行的方式，每一次对样本有一个权重调整后再放入下一个弱学习器训练，权重是根据分类的误差率来调整的；最后把所有带权重的弱分类器线性总和即为我们集成的强分类器。有点三个臭裨将顶个诸葛亮的意思。 2.2 算法框架 上面是周志华《机器学习》P174页的算法框架，大致和前面说的差不多但是会有疑惑的点是错误率如何来的？还有最后的αt和数据集Dt分布如何更新？针对这3个问题，有了第三小节。 2.3 公式推导参考自机器学习P175页，错误率就是每次样本分错权重的归一化值。对于第6行怎么来的，参考下图：由于是二分类问题，yi的预测值不是1就是-1，所以只分为2种情况，即预测准确的期望值和预测错误的期望值。 对于第7行怎么来的，看下图：这里参考了索引1，点击这里访问。但是他里面有2个错误，一个是红色虚线部分，一个最后求L的最后第2步，杠掉Wm,i部分。图中已更正! 字丑，见谅。 上图是对基分类器权重αm用另一种方式的推导。 2.4 代码参考这里是网上的一个代码，只包含训练模型，没有预测部分，建议先看博客： https://www.cnblogs.com/davidwang456/articles/8927029.html 里面的例子，再看代码更好理解。写的蛮好的。代码也可以在我的github上面克隆或下载： https://github.com/ArmigoPancho/mechine-learning-codes/tree/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0python%E4%BB%A3%E7%A0%81 。 One more thing：热爱生活，并为之砥砺前行。 3.参考资料1.AdaBoost原理详：https://www.cnblogs.com/ScorpioLu/p/8295990.html 2.老刘博客：请点击这里 3.周志华《机器学习》]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>AdabBoost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Word2vec]]></title>
    <url>%2F2019%2F03%2F04%2Fword2vec%2F</url>
    <content type="text"><![CDATA[1.Word2vec向量是什么？1.1独热编码表示（One Hot Representation）Word2vec，由 Google 于 2013 年发表，是一种神经网络实现，可以学习单词的分布式表示。在此之前已经提出了用于学习单词表示的其他深度或循环神经网络架构，但是这些的主要问题是训练模型所需时长间。 Word2vec 相对于其他模型学习得快。Word2Vec 不需要标签来创建有意义的表示。这很有用，因为现实世界中的大多数数据都是未标记的。如果给网络足够的训练数据（数百亿个单词），它会产生特征极好的单词向量。具有相似含义的词出现在簇中，并且簇具有间隔，使得可以使用向量数学来再现诸如类比的一些词关系。着名的例子是，通过训练好的单词向量，“国王 - 男人 + 女人 = 女王”。上面kaggle对word2vec的介绍，看完还是不了解具体它做了什么。理解Wordvec之前先了解One hot representation模式：向量中每一个元素都关联着词库中的一个单词，指定词的向量表示为：其在向量中对应的元素设置为1，其他的元素设置为0。比如下面图所示：只有1处表示当前单词queen，其他单词的位置都是0，one-hot就是这样子表示每一个单词。所以每个单词相互正交，所以也不会有单词之间的联系了。 再如考虑下面的三个特征： [“male”, “female”] [“from Europe”, “from US”, “from Asia”] [“uses Firefox”, “uses Chrome”, “uses Safari”, “uses Internet Explorer”] 将它换成独热编码后，应该是： feature1=[01,10] feature2=[001,010,100] feature3=[0001,0010,0100,1000] 优点：一是解决了分类器不好处理离散数据的问题，二是在一定程度上也起到了扩充特征的作用。 缺点：在文本特征表示上有些缺点就非常突出了。首先，它是一个词袋模型，不考虑词与词之间的顺序（文本中词的顺序信息也是很重要的）；其次，它假设词与词相互独立（在大多数情况下，词与词是相互影响的）；最后，它得到的特征是离散稀疏的。 1.2分布式表示（Distributed Representation）Distributed representation可以解决One hot representation的维度灾难和稀疏问题，它的思路是通过训练，将每个词都映射到一个较短的词向量上来。比如下图我们将词汇表里的词用”Royalty”,”Masculinity”, “Femininity”和”Age”4个维度来表示，King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)(0.99,0.99,0.05,0.7)。当然在实际情况中，我们并不能对词向量的每个维度做一个很好的解释。 有了上面的表示方式，我们就可以用向量来分析词语词之间的关系。比如下面一个有趣的公式： \vec {King} - \vec {Man} + \vec {Woman} = \vec {Queen}怎么训练得到合适的词向量呢？一个很常见的方法是使用神经网络语言模型。 2.神经网络语言模型神经网络语言模型采用的方法一般是一个三层的神经网络结构（当然也可以多层），分为输入层，隐藏层和输出层(softmax层)。这个模型是如何定义数据的输入和输出呢？Word2vec模式中一般分为CBOW(Continuous Bag-of-Words 与Skip-Gram两种模型。CBOW模型根据中心词W(t)的周围词来预测中心词，Skip-gram模型则根据中心词W(t)来预测周围词。两者类似相反的过程。 2.1CBOW模型（Continuous Bag-of-Words ）先看一个网上的例子： 假设我们现在的Corpus是这一个简单的只有四个单词的document：{I drink coffee everyday}我们选coffee作为中心词，window size设为2也就是说，我们要根据”I”,”drink”和”everyday”三个周围词来预测一个中心单词，并且我们希望这个单词是coffee。 （1）首先，初始化周围词的独热码。”I”,”drink”和”everyday”三个周围词的独热码是作为输入层的3个向量（单词向量空间dim为1 x V），目标是预测”coffee”的独热码。上图中的window size：表示当前词与预测词在一个句子中的最大距离是多少，其他参数请参考：https://blog.csdn.net/szlcw1/article/details/52751314 （2）初始化输入权重矩阵W（单词向量空间dim为1 x V，W的维度是V x N，N为自己设定的数），再把所有one hot分别乘以共享的输入权重矩阵W得到向量Vi，图中的hi就是这里的Vi。 （3）然后把所有每个分量Vi相加求平均值得到一个隐层向量V，维度是Nx1，行维度与W的行维度一致。 （4）初始化输出权重W’ （维度N x V），由W’ x V = U可得到我们的中间矩阵U，为后面的输出矩阵做准备。 （5）最后一步就是中间矩阵U输入到softmax分类函数里面去得到最后的输出向量y。其中的每一维斗代表着一个单词，概率最大的index所指示的单词为预测出的中间词（target word）。 （6）预测出的中间词与true label的onehot做比较，误差越小越好。 2.2Skip-Gram模型2.2.1 Skip-Gram介绍以下翻译自CS 224D: Deep Learning for NLP。Skip-Gram Model就是与CBOW相反的情况，根据某个中心词来预测周围c/2个词，就是向左c/2个词，向右c/2个词。总的框架如下图所示： 符号标记：一个中心词$x$；输出向量是$y^j$；V是输入单词矩阵（维度n x V）；$v_i$是$V$的第$i$列，表示单词$w_i$的输入向量；$U$代表输出矩阵（维度n x V）；$u_i$表示$U$的第$i$行，表示单词$w_i$的输出向量。 步骤1：生成输入向量的独热码x 步骤2：我们由上下文$v_c = V x$得到嵌入词单向量 步骤3：由于没有平均值，只需设置$\hat{v}= vc$ 步骤4：用$ u = U v_c=$来计算$u_{c-m}, . . . , u_{c-1}, u_{c+1}, . . . , u_{c+m}$，以此生成2 x m个分数向量 步骤5：用公式$y = softmax(u)$把每一个分数转为概率 步骤6：用我们预测的概率向量$y^{(c-m)}, . . . , y^{(c-1)}, y^{(c+1)}, . . . , y^{(c+m)}$来和真实的独热码进行比较，越接近越好 2.2.2 隐层细节训练模型的真正目的是获得模型基于训练数据学得的隐层权重。为了得到这些权重，我们首先要构建一个完整的神经网络作为我们的“Fake Task”。模型的输入如果为一个10000维的向量，那么输出也是一个10000维度（词汇表的大小）的向量，它包含了10000个概率，每一个概率代表着当前词是输入样本中output word的概率大小。由于输入向量X是one-hot编码，那么只有输入权重向量W中的非零元素才能对隐藏层产生输入。比如下面公式： h = x^TW=W_{k,.}:=v_{wI}\tag{$1$}看不懂的话，看下图，由于左边的独热码矩阵在维度很高时候做相乘的操作时，非常消耗资源，我们这里只要根据独热码不为0的列号取输入矩阵W所对应的行，所以模型中的隐层权重矩阵便成了一个”查找表“（lookup table）。 2.2.3模型优化目标在CBOW中，我们需要设定一个目标函数来估计我们的模型。但是这里不同的是，因为这里假设是用朴素贝叶斯来断开概率之间的联系，就是说，这里假设中心词一旦给定，那所有输出的单词都是相互独立的。故有如下优化目标函数J： 要是得周围词是最佳的词，就是所有预测出来单词的概率是最大，P(Wc-m,…,Wc+m | Wc)是概率在0-1之间，所以底数&gt;1的情况下，log(P(Wc-m,…,Wc+m | Wc))&lt;0，加上负号表示概率越小，损失越大，周围词就越不好。相反概率越大越好。由于所有输出单词都是假设独立的，所以每个单词概率简单的相乘就是总体的概率。第四个等号后面是用softmax函数对概率进行处理了。最后一步拆开来即可。在上面的优化目标下，这里采用随机梯度下降（Stochastic Gradient Descent）来计算未知参数的梯度。 简单的总体讲一下，就是三层网络模型，第一层是输入层，中间一层是隐藏结点，最后一层是输出层。模型需要训练的参数是输入权重和输出权重。建议看论文原文，skip-gram写的不好，没理解透。其实还有针对模型提出的3个训练方法，有兴趣的可以在刘建平的这3篇博客了解。 3.参考1.CBOW：https://www.zhihu.com/question/44832436/answer/266068967 2.CS224D笔记，百度网盘链接：https://pan.baidu.com/s/1UZygZEVFbPO5fcdncJDvow提取码：kxu1 3.刘建平的CBOW与Skip-Gram模型基础：https://www.cnblogs.com/pinard/p/7160330.html) 4.skip介绍： http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/ 5.skip-gram视频，需要翻墙： https://www.youtube.com/watch?v=EHqXB6P1PzA]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>word2vec</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[情感分析案例]]></title>
    <url>%2F2019%2F02%2F27%2FSentiment%20Analysis%2F</url>
    <content type="text"><![CDATA[１．数据集介绍本案例是在来自Kaggle网站，所有详细信息均可以参考网址： https://www.kaggle.com/c/word2vec-nlp-tutorial ，本次案例是Bag of Words Meets Bags of Popcorn，我翻译成词带模型爱上爆米花，anyway，数据集有四个文件一个有类标的训练集labeledTrainData如下图所示，就是三列：id、sentiment、review。每条评论是对某电影的评论，只是简单的二分类问题，1表示IMDB评分&gt;=7，若某评论的IMDB评级&lt;5会导致情绪评分为0，这里已经打好了类标记。 testData是一个没有类标的测试集，预测提交到kaggle自然会打分，sampleSubmission是以正确格式的逗号分隔的示例提交文件，还有unlabeledTrainData**是另外50,000个IMDB评论没有提供任何评级标签。 2.TF-IDF和Word2vec在贝叶斯、逻辑回归方法中的比较下面两段代码都是来自kaggle网上和其他参考资料，已经过不动脑子的誊写 :）+运行，把数据放在一个py文件同目录下即可： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110import reimport pandas as pdfrom bs4 import BeautifulSoupimport numpy as npfrom sklearn.model_selection import cross_val_predictfrom sklearn.feature_extraction.text import TfidfVectorizer as TFIDFfrom sklearn.naive_bayes import MultinomialNB as MNBfrom sklearn.linear_model import LogisticRegression as LRfrom sklearn.grid_search import GridSearchCVfrom sklearn.model_selection import cross_val_score# 参考：http://blog.csdn.net/longxinchen_ml/article/details/50629613def review_to_wordlist(review): ''' 把IMDB的评论转成词序列 参考：http://blog.csdn.net/longxinchen_ml/article/details/50629613 ''' # 去掉HTML标签，拿到内容 review_text = BeautifulSoup(review, "html.parser").get_text() # 用正则表达式取出符合规范的部分 review_text = re.sub("[^a-zA-Z]", " ", review_text) # 小写化所有的词，并转成词list words = review_text.lower().split() # 返回words return wordsroot_dir = "../word2vec-nlp-tutorial"train = pd.read_csv('%s/%s' % (root_dir, 'labeledTrainData.tsv'), header=0, delimiter="\t", quoting=3)test = pd.read_csv('%s/%s' % (root_dir, 'testData.tsv'), header=0, delimiter="\t", quoting=3)# 预处理数据label = train['sentiment']train_data = []for i in range(len(train['review'])): train_data.append(' '.join(review_to_wordlist(train['review'][i])))test_data = []for i in range(len(test['review'])): test_data.append(' '.join(review_to_wordlist(test['review'][i])))"""特征处理直接丢给计算机这些词文本，计算机是无法计算的，因此我们需要把文本转换为向量，有几种常见的文本向量处理方法，比如：1)单词计数2)TF-IDF向量3)Word2vec向量我们先使用TF-IDF来试一下min_df: 最小支持度为2（词汇出现的最小次数）max_features: 默认为None，可设为int，对所有关键词的term frequency进行降序排序，只取前max_features个作为关键词集strip_accents: 将使用ascii或unicode编码在预处理步骤去除raw document中的重音符号analyzer: 设置返回类型token_pattern: 表示token的正则表达式，需要设置analyzer == 'word'，默认的正则表达式选择2个及以上的字母或数字作为token，标点符号默认当作token分隔符，而不会被当作tokenngram_range: 词组切分的长度范围use_idf: 启用逆文档频率重新加权use_idf：默认为True，权值是tf*idf，如果设为False，将不使用idf，就是只使用tf，相当于CountVectorizer了。smooth_idf: idf平滑参数，默认为True，idf=ln((文档总数+1)/(包含该词的文档数+1))+1，如果设为False，idf=ln(文档总数/包含该词的文档数)+1sublinear_tf: 默认为False，如果设为True，则替换tf为1 + log(tf)stop_words: 设置停用词，设为english将使用内置的英语停用词，设为一个list可自定义停用词，设为None不使用停用词，设为None且max_df∈[0.7, 1.0)将自动根据当前的语料库建立停用词表"""tfidf = TFIDF(min_df=2, max_features=None, strip_accents='unicode', analyzer='word', token_pattern=r'\w&#123;1,&#125;', ngram_range=(1, 3), # 二元文法模型 use_idf=1, smooth_idf=1, sublinear_tf=1, stop_words = 'english') # 去掉英文停用词# 合并训练和测试集以便进行TFIDF向量化操作data_all = train_data + test_datalen_train = len(train_data)tfidf.fit(data_all)data_all = tfidf.transform(data_all)# 恢复成训练集和测试集部分train_x = data_all[:len_train]test_x = data_all[len_train:]print('TF-IDF处理结束.')print("train: \n", np.shape(train_x[0]))print("test: \n", np.shape(test_x[0]))"""1.朴素贝叶斯训练模型"""model_NB = MNB()MNB(alpha = 1.0,class_prior=None,fit_prior=True)model_NB.fit(train_x,label)print("多项式贝叶斯模型10折交叉验证得分：",np.mean( cross_val_score(model_NB, train_x, label, cv = 10, scoring ='roc_auc')))"""2.使用逻辑回归模型来测试一遍"""#设定网格搜索参数grid_values = &#123;'C':[30]&#125;#设定打分为roc_aucmodel_LR = GridSearchCV(LR(penalty='l2',dual=True,random_state =0),grid_values,scoring = 'roc_auc',cv=2)#放入数据model_LR.fit(train_x,label)print("训练结果：",model_LR.grid_scores_, '\n', model_LR.best_params_, model_LR.best_score_)#预测结果test_results = np.array(model_LR.predict(test_x))print("预测结果是")submission_df = pd.DataFrame(data=&#123;'id':test['id'],'sentiment':test_results&#125;)print(submission_df.head(10))submission_df.to_csv('../submission_br.csv',columns = ['id','sentiment'], index = False) 下面是Word2vec在GNB中的运用： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140"""3.高斯贝叶斯+Word2vec训练"""import gensimimport nltkimport reimport numpy as npimport pandas as pdfrom nltk.corpus import stopwordsfrom bs4 import BeautifulSoupimport timefrom gensim.models import Word2Vecfrom sklearn.naive_bayes import GaussianNB as GNBfrom sklearn.cross_validation import cross_val_scoreimport warningswarnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')import gensim#文本预处理root_dir = "../word2vec-nlp-tutorial"train = pd.read_csv('%s/%s' % (root_dir, 'labeledTrainData.tsv'), header=0, delimiter="\t", quoting=3)test = pd.read_csv('%s/%s' % (root_dir, 'testData.tsv'), header=0, delimiter="\t", quoting=3)tokenizer = nltk.data.load('file:D:/nltk_data/tokenizers/punkt/english.pickle') #若没有file：开头，误以为是http协议开头，自然找不到或格式错误了 def review_to_wordlist(review, remove_stopwords=False): # review = BeautifulSoup(review, "html.parser").get_text() review_text = re.sub("[^a-zA-Z]"," ", review) words = review_text.lower().split() if remove_stopwords: stops = set(stopwords.words("english")) words = [w for w in words if not w in stops] return(words)def review_to_sentences( review, tokenizer, remove_stopwords=False ): ''' 将评论段落转换为句子，返回句子列表，每个句子由一堆词组成 ''' raw_sentences = tokenizer.tokenize(review.strip()) sentences = [] for raw_sentence in raw_sentences: if len(raw_sentence) &gt; 0: # 获取句子中的词列表 sentences.append( review_to_wordlist( raw_sentence, remove_stopwords )) return sentences# 预处理数据label = train['sentiment']train_data = []for i in range(len(train['review'])): train_data.append(' '.join(review_to_wordlist(train['review'][i])))test_data = []for i in range(len(test['review'])): test_data.append(' '.join(review_to_wordlist(test['review'][i])))sentences = []for i, review in enumerate(train["review"]): # print(i, review) sentences += review_to_sentences(review, tokenizer, True)print(np.shape(train["review"]))print(np.shape(sentences)) # 模型参数num_features = 300 # Word vector dimensionality min_word_count = 40 # 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5 num_workers = 4 # Number of threads to run in parallelcontext = 10 # Context window size downsampling = 1e-3 # Downsample setting for frequent words# 训练模型#word2vec参数解释： https://blog.csdn.net/laobai1015/article/details/86540813print("训练模型中...")model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count=min_word_count, window=context, sample=downsampling)print("训练完成")print('保存模型...')model.init_sims(replace=True)model_name = "%s/%s" % (root_dir, "300features_40minwords_10context.txt")model.save(model_name)print('保存结束')def makeFeatureVec(words, model, num_features): ''' 对段落中的所有词向量进行取平均操作 ''' featureVec = np.zeros((num_features,), dtype="float32") nwords = 0. # Index2word包含了词表中的所有词，为了检索速度，保存到set中 index2word_set = set(model.wv.index2word) for word in words: if word in index2word_set: nwords = nwords + 1. featureVec = np.add(featureVec, model[word]) # 取平均 featureVec = np.divide(featureVec, nwords) return featureVecdef getAvgFeatureVecs(reviews, model, num_features): ''' 给定一个文本列表，每个文本由一个词列表组成，返回每个文本的词向量平均值 ''' counter = 0 reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype="float32") for review in reviews: if counter % 5000 == 0: print("Review %d of %d" % (counter, len(reviews))) reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features) counter = counter + 1 return reviewFeatureVecstrainDataVecs = getAvgFeatureVecs(train_data, model, num_features)print(np.shape(trainDataVecs))testDataVecs = getAvgFeatureVecs(test_data, model, num_features)print(np.shape(testDataVecs))model_GNB = GNB()model_GNB.fit(trainDataVecs,label)print("高斯贝叶斯分类器10折交叉验证得分: ", np.mean(cross_val_score(model_GNB, trainDataVecs, label, cv=10, scoring='roc_auc')))print("保存结果。。。")result = model_GNB.predict(testDataVecs)submission_df = pd.DataFrame(data=&#123;'id':test['id'],'sentiment':result&#125;)print(submission_df.head(10))submission_df.to_csv('C:/Users/asus1/Desktop/gnb_word2vec.csv',columns = ['id','sentiment'], index = False)]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP，情感分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《亡徵》]]></title>
    <url>%2F2018%2F12%2F24%2Fperdition%2F</url>
    <content type="text"><![CDATA[1.《韩非子》战国 (公元前475年 - 公元前221年)时期，百家争鸣，百花齐放，实在是难得。韩非（约公元前280年—公元前233年），战国时期韩国都城新郑（今河南省郑州市新郑市）人 [1] ，法家代表人物，杰出的思想家、哲学家和散文家。韩王之子，荀子学生，李斯同门师兄。下面是《四部丛刊初编》中第350～352册。景上海涵芬楼藏景宋钞校本，本书二十卷，完整版见此链接： https://ctext.org/library.pl?if=gb&amp;file=77707&amp;page=4&amp;remap=gb 。 2.《亡徵》原文​ 凡人主之国小而家大，权轻而臣重者，可亡也。简法禁而务谋虑，荒封内而恃交援者，可亡也。群臣为学，门子好辩，商贾外积，小民右仗者，可亡也。好宫室台榭陂池，事车服器玩好，罢露百姓，煎靡货财者，可亡也。用时日，事鬼神，信卜筮，而好祭祀者，可亡也。听以爵不待参验，用一人为门户者，可亡也。官职可以重求，爵禄可以货得者，可亡也。缓心而无成，柔茹而寡断，好恶无决，而无所定立者，可亡也。饕贪而无餍，近利而好得者，可亡也。喜淫而不周于法，好辩说而不求其用，滥于文丽而不顾其功者，可亡也，浅薄而易见，漏泄而无藏，不能周密，而通群臣之语者，可亡也。很刚而不和，愎谏而好胜，不顾社稷而轻为自信者，可亡也。恃交援而简近邻，怙强大之救，而侮所迫之国者，可亡也。羁旅侨士，重帑在外，上闲谋计，下与民事者，可亡也。民信其相，下不能其上，主爱信之而弗能废者，可亡也。境内之杰不事，而求封外之士，不以功伐课试，而好以名问举错，羁旅起贵以陵故常者，可亡也。轻其适正，庶子称衡，太子未定而主即世者，可亡也。大心而无悔，国乱而自多，不料境内之资而易其邻敌者，可亡也。国小而不处卑，力少而不畏强，无礼而侮大邻，贪愎而拙交者，可亡也。太子已置，而娶于强敌以为后妻，则太子危，如是，则群臣易虑，群臣易虑者，可亡也。怯慑而弱守，蚤见而心柔懦，知有谓可，断而弗敢行者，可亡也。出君在外而国更置，质太子未反而君易子，如是则国携，国携者，可亡也，挫辱大臣而狎其身，刑戮小民而逆其使，怀怒思耻而专习则贼生，贼生者，可亡也。大臣两重，父兄众强，内党外援以争事势者，可亡也。婢妾之言听，爱玩之智用，外内悲惋而数行不法者，可亡也。简侮大臣，无礼父兄，劳苦百姓，杀戮不辜者，可亡也。好以智矫法，时以行集公，法禁变易，号令数下者，可亡也。无地固，城郭恶，无畜积，财物寡，无守战之备而轻攻伐者，可亡也。种类不寿，主数即世，婴儿为君，大臣专制，树羁旅以为党，数割地以待交者，可亡也。太子尊显，徒属众强，多大国之交，而威势蚤具者，可亡也。变褊（biǎn，狭隘）而心急，轻疾而易动发，心悁忿（yuān fèn，怨怒）而不訾（zī，估量、思考）前后者，可亡也。主多怒而好用兵，简本教而轻战攻者，可亡也。贵臣相妒，大臣隆盛，外藉敌国，内困百姓，以攻怨雠，而人主弗诛者，可亡也。君不肖而侧室贤，太子轻而庶子伉，官吏弱而人民桀，如此则国躁，国躁者，可亡也。藏怒而弗发，悬罪而弗诛，使群臣阴憎而愈忧惧，而久未可知者，可亡也。出军命将太重，边地任守太尊，专制擅命，径为而无所请者，可亡也。后妻淫乱，主母畜秽，外内混通，男女无别，是谓两主，两主者，可亡也。后妻贱而婢妾贵，太子卑而庶子尊，相室轻而典谒重，如此则内外乖，内外乖者，可亡也。大臣甚贵，偏党众强，壅塞主断而重擅国者，可亡也。私门之官用，马府之世，乡曲之善举，官职之劳废，贵私行而贱公功者，可亡也。公家虚而大臣实，正户贫而寄寓富，耕战之士困，末作之民利者，可亡也。见大利而不趋，闻祸端而不备，浅薄于争守之事，而务以仁义自饰者，可亡也。不为人主之孝，而慕匹夫之孝，不顾社稷之利，而听主母之令，女子用国，刑馀用事者，可亡也。辞辩而不法，心智而无术，主多能而不以法度从事者，可亡也。亲臣进而故人退，不肖用事而贤良伏，无功贵而劳苦贱，如是则下怨，下怨者，可亡也。父兄大臣禄秩过功，章服侵等，宫室供养太侈，而人主弗禁，则臣心无穷，臣心无穷者，可亡也。公婿公孙与民同门，暴傲其邻者，可亡也。亡徵者，非曰必亡，言其可亡也。夫两尧不能相王，两桀不能相亡，亡王之机，必其治乱、其强弱相踦者也。木之折也必通蠹，墙之坏也必通隙。然木虽蠹，无疾风不折；墙虽隙，无大雨不坏。万乘之主，有能服术行法以为亡徵之君风雨者，其兼天下不难矣。 3.《亡徵》译文​ 凡属君主国家弱小而臣下强大的，君主权轻而臣下权重的，可能灭亡。轻视法令而好用计谋，荒废内政而依赖外援的，可能灭亡。群臣喜欢私学，贵族子弟喜欢辩术，商人在外囤积财富，百姓崇尚私斗的，可能灭亡。嗜好宫殿楼阁池塘，爱好车马服饰玩物，喜欢让百姓疲劳困顿，压榨挥霍钱财的，可能灭亡。选吉日，信鬼神，迷信占卜而讲究祭祀的，国家就可能灭亡。听凭有爵位的人的意见而不考核验证，只听一个人的话而且由他去办的，国家就可能灭亡。官职可以依靠重臣求得，爵禄可以用财宝换取的，国家就可能灭亡。办事拖拉而无成效，优柔寡断，好坏不分又无主见、行动不定的，国家就可能灭亡。贪得无厌，唯利是图的，国家就可能灭亡。喜欢玩弄词藻但不合乎法规，讲究巧辩而不求实用，沉溺于华美的文采而不顾它的功效的，国家就可能灭亡。君主不学无术而且轻易表露情态，泄露机密而不知隐藏，不能严密防范而又把臣下的进言透露出去的，国家就可能灭亡。凶狠暴虐而不平和，不听别人的忠谏而逞强好胜，不顾国家安危而轻率自信的，国家就可能灭亡。仰仗外国的支援而怠慢近邻，依靠强国的救援而侮辱邻国的，国家就可能灭亡。寄居国外的游客，把大批财宝存放在国外，对上刺探国家机密，对下干预民事的，国家就可能灭亡。百姓相信相国，臣下轻视君主，君主宠爱相国，相信他又不能废黜的，国家就可能灭亡。国内的杰出人才闲置不用，反而去寻求国外的人，不按功劳大小去考核，而喜欢用虚名，把寄寓作客的人破格起用而凌驾故旧的，国家就可能灭亡。轻视嫡子而庶出的掌权，太子还没有确定而君主过世的，国家就可能灭亡。君主狂妄自大而不悔悟，国家混乱而自以为美，不估量本国的实力而看轻邻近敌国的，国家就可能灭亡。国家弱小而又不卑躬谦下，国力不足而又不服强国，没有礼貌而又侮辱强大的邻国，贪婪任性而不善于外交的，国家就可能灭亡。已经有了太子，而又从强大的敌国娶来正妻，那太子就有危险了，这样群臣也会变心；群臣变心的，国家就可能灭亡。胆小怕事而又不敢坚持己见，问题早已发现而因内心懦弱，也知道可以解决，决定了却又不敢执行的，国家就可能灭亡。出国的君主还在国外而国内已另立新君，在国外做人质的太子还没有回国而君主又另立太子，这样群臣就会有二心，群臣有二心的，国家就可能灭亡。打击侮辱大臣而又戏弄他们，杀戮小民而又一反常规地役使他们，小民心怀怨恨牢记耻辱，而君主又专心一意地宠幸而又戏辱他们，于是劫杀的事就会发生；发生劫杀之事的，国家就可能灭亡。两个大臣同时当权，君主的叔伯兄弟又多又强，国内结党谋取外援而争夺权势的，国家就可能灭亡。 ​ 只听侍从妃妾的话，只听宠幸近臣的计策，朝野内外悲愁怨恨而又屡行不法的，国家就可能灭亡。怠慢侮辱大臣，对叔伯兄弟又没有礼貌，使百姓劳乏困苦，杀戮无辜的，国家就可能灭亡。喜欢用自己的小聪明来改变法规，时常把自己的私利混杂在公务之中，法制、禁令常常改动，号令下达无数的，国家就可能灭亡。没有险要的地势可守，城墙不坚固，无粮食储备，财力物力贫乏，没有防守和攻战的准备，又轻举妄动去进攻的，国家就可能灭亡。君主的族人寿命不长，君主又接连死去，婴儿做了君主，奸臣专权，拉拢国外的游勇结为私党，一再割地以求得敌国青睐的，国家就可能灭亡。太子受到尊重而名声显赫，侍从、属下及其党羽众多而且强盛，又与许多大国友好交往，而且很早就有威势的，国家就可能灭亡。心情偏邪狭小而又急躁，轻浮而又极易冲动，当他愤怒时从不慎重思考的，国家就可能灭亡。君主多动怒而好用兵打仗，放松农业生产和平时练兵的，国家就可能灭亡。贵臣互相妒嫉，重臣权势又大，在外借重敌国的势力，对内压榨百姓，用以攻击和自己有怨仇的人，而君主不加诛戮的，国家就可能灭亡。君主不贤而庶出贤，太子轻薄而庶子高尚，官吏懦弱而人民豪横，这样的国家就会动荡不安；动荡不安的国家就可能灭亡。心藏怨恨不敢发作，对罪臣迟迟不予惩办，使群臣在暗中怀恨而更加忧虑畏惧，过了很长时间也不知会有什么结果的，国家就可能灭亡。发兵任命的将领权势太重，驻守边疆的官员地位太高，他们独断专行，直接处理问题而不向君主请示的，国家就可能灭亡。太后淫乱，主母养奸，内外不分，男女无别，这就形成后党一方和君主一方的两种势力、两个主子；有了两个主子的，国家就可能灭亡。王后卑微而婢妾尊贵，太子卑微而庶子尊贵，丞相权轻而掌宾客的官吏权重，这样就会里外不分而轻重颠倒；里外不分、轻重颠倒的，国家就可能灭亡。大臣异常显贵，私党人多势众，阻闭君主的视听，阻碍君主决断而独揽大权的，国家就可能灭亡。豪门贵族私人的属下可以被任用，立过军功的后代可以被排挤，偏僻乡村里的有善名的被举荐，在职官吏的功劳被埋没，重视谋私利的人而看不起为国立功的，国家就可能灭亡。国家的府库空虚而大臣却很富厚，本地居民贫苦而客居之人却很富足，耕作出征之家困乏而工商之家得利的，国家就可能灭亡。看到国家的利益而不赶紧去办，听到有祸乱的苗头而不去及早设防，征战守备之事流于轻浮，只是想着用仁义装扮自己的，国家就可能灭亡。不是想着祖先的社稷，只是羡慕小民对父母的孝敬，不顾国家的利益，而听从王后的意旨，妇女掌政，宦官弄权的，国家就可能灭亡。能言善辩而不合乎法度，聪明伶俐而没有法术，君主多才多艺却不按法度行事的，国家就可能灭亡。新任之臣晋升，原有之臣被辞退，无德无才的人管事，有德有才的靠边，没有功劳的人地位显贵，劳苦为国的人地位卑贱，这样臣下就会怨恨；臣下怨恨的，国家就可能灭亡。君主的叔伯兄弟以及大臣的俸禄品位高过他们的功劳，服饰高过他们的等级，宫室华丽以及供养、消费太奢侈，而君主并不禁止，臣下的贪求就无止境；臣下贪求无止境的，国家就可能灭亡。驸马或公子王孙与百姓同住在一条小巷里，对邻里强横残暴的，国家就可能灭亡。 ​ 说它有灭亡的征兆，并不是说它必然就会灭亡，只是说它有灭亡的可能。两个唐尧不可能相互统一天下，两个夏桀不可能相互灭亡；灭亡与统一天下的机遇，就要看国家的治与乱、强与弱的两端哪一头重了。大树的折断，必定是由于蛀虫蛀蚀的结果；大墙的倒塌，必定是由于缝隙裂开的缘故。然而大树虽说被蛀蚀，没有大风也不会折断；大墙虽然有了裂缝，没有大雨也不会倒塌。万乘大国君主，如果有推行法术的人协助，一定会像暴风骤雨那样，很容易就能摧毁有灭亡的征兆的国家，而兼并天下也就不是什么困难的事了! ​ 一共48种”可亡也”的征兆，古人思想之深远，志虑之广袤，非常人所比。]]></content>
      <categories>
        <category>火花</category>
      </categories>
      <tags>
        <tag>亡徵，韩非子</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法代码]]></title>
    <url>%2F2018%2F12%2F13%2FAlgorithm%2F</url>
    <content type="text"><![CDATA[排序算法的总结图： 1.冒泡排序对一些常见的排序刷一下,可以参考 https://blog.csdn.net/sgn132/article/details/47279511 ，https://blog.csdn.net/wfq784967698/article/details/79551476 等博客。冒泡排序就是每一趟都通过两两比较相邻的值，逆序就调整顺序，遍历整个list找到最大值放到最后一个，然后list长度减1；再次遍历直到最后一个值是最大值，如此循环。计算复杂度最好时为$O（n）$,最差是$O（n^2）$，故计算复杂度为$O(n^2)$，是稳定排序。 123456789101112# 冒泡排序def bubble_sort(l): # 外层循环 len(l)遍，内层循环少一遍 for i in range(len(l)-1): for j in range(len(l) - 1): # 找出最大值，然后交换位置到最后 if l[j] &gt; l[j+1]: l[j], l[j+ 1] = l[j+1], l[j]if __name__ == "__main__": l = [5, 1, 91, 3, 2, 7] bubble_sort(l) print(l) 2.直接插入排序2.1 算法伪代码 一般来说，插入排序都采用in-place在数组上实现。具体算法描述如下： （1）从第一个元素开始，该元素可以认为已经被排序；（2）取出下一个元素，在已经排序的元素序列中从后向前扫描；（3）如果该元素（已排序）大于新元素，将该元素移到下一位置；（4）重复步骤3，直到找到已排序的元素小于或者等于新元素的位置；（5）将新元素插入到该位置后；（6）重复步骤2~5。 动态图如下：]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>冒泡排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[D3.JS]]></title>
    <url>%2F2018%2F12%2F08%2FD3.JS%2F</url>
    <content type="text"><![CDATA[1.可缩放矢量图形（SVG） SVG（Scalable Vector Graphics） 可被非常多的工具读取和修改（比如记事本）,由于使用xml格式定义，所以可以直接被当作文本文件打开，看里面的数据 SVG 与 JPEG 和 GIF 图像比起来，尺寸更小，且可压缩性更强，SVG 图就相当于保存了关键的数据点，比如要显示一个圆，需要知道圆心和半径，那么SVG 就只保存圆心坐标和半径数据，而平常我们用的位图都是以像素点的形式根据图片大小保存对应个数的像素点，因而SVG尺寸更小 SVG 是可伸缩的，平常使用的位图拉伸会发虚，压缩会变形，而SVG格式图片保存数据进行运算展示，不管多大多少，可以不失真显示。作图例子如下： （1）平面上一个圈圈：包涵圈和圈内、圈外。发现这个可缩放的矢量图是在SVG模块里面再添加一个circle模块的：’’svgCircleTutorial’’是svg的id名字、xmlns是XML命名空间，区别开在不同文件中的相同的标志、height=”250”是SVG的高度；cx=”55” cy=”55”表示坐标（55,55）、r是半径、fill是填充圈内颜色、stroke是轮廓的颜色、stroke-width是轮廓宽度。把下面代码保存到txt文件，修改文件名字为circle.html 用任意浏览器打开即可显示下图。 123&lt;svg id="svgCircleTutorial" height="250" xmlns="http://www.w3.org/2000/svg"&gt; &lt;circle id="myCircle" cx="100" cy="100" r="60" fill="#219E3E" stroke="#17301D" stroke-width="10" /&gt;&lt;/svg&gt; （2）rect 元素的 width 和 height 属性可定义矩形的高度和宽度、style属性用来定义CSS属性、CSS 的 fill 属性定义矩形的填充颜色（rgb 值、颜色名或者十六进制值）、CSS 的 stroke-width 属性定义矩形边框的宽度、CSS 的 stroke 属性定义矩形边框的颜色，如下： 1234&lt;svg&gt; &lt;rect width="150" height="150" style="fill:rgb(255,255,0);stroke-width:12;stroke:rgb(0,150,0)"/&gt;&lt;/svg&gt; （3）路径：设置线段坐标和图像填充，fill为white时在背景也是白色的情况下只有楼梯线段了。 123456&lt;svg&gt; &lt;polyline points="0 40,40 40,40 80,80 80,80 120,120 120,120 160" style="fill:blue; stroke:gray; stroke-width:3" /&gt;&lt;/svg&gt; 2.HTML例子由于html也是前端的核心，暂时了解下，直接例子理解可能好些。 （1）title和内容的位置不同： 123456789101112&lt;html&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;我是标题，我在title标签内，我显示在浏览器窗口最顶部&lt;/title&gt;&lt;/head&gt;&lt;body&gt;我是内容，后面是我的博客地址：&lt;a href="http://www.armigo.fun"&gt;这是一个链接&lt;/a&gt;下面是一张图片：&lt;br&gt;&lt;img src="http://pic.qiantucdn.com/58pic/18/06/80/21N58PICBfS_1024.jpg" width="787.2" height="492" /&gt;&lt;/body&gt;&lt;/html&gt; （2）绘制表格： 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset="utf-8"&gt;&lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;每个表格从一个 table 标签开始。每个表格行从 tr 标签开始。每个表格的数据从 td 标签开始。&lt;/p&gt;&lt;h4&gt;一列:&lt;/h4&gt;&lt;table border="2"&gt;&lt;tr&gt; &lt;td&gt;100&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;h4&gt;一行三列:&lt;/h4&gt;&lt;table border="1"&gt;&lt;tr&gt; &lt;td&gt;100&lt;/td&gt; &lt;td&gt;200&lt;/td&gt; &lt;td&gt;300&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;h4&gt;2行3列&lt;/h4&gt;&lt;table border="1"&gt;&lt;tr&gt; &lt;td&gt;11&lt;/td&gt; &lt;td&gt;22&lt;/td&gt; &lt;td&gt;33&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td&gt;14&lt;/td&gt; &lt;td&gt;25&lt;/td&gt; &lt;td&gt;36&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/body&gt;&lt;/html&gt; （3）页面布局： 12345678910111213141516171819202122&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;title&gt;Armigo&lt;/title&gt; &lt;/head&gt;&lt;body&gt;&lt;div id="container" style="width:500px"&gt;&lt;div id="header" style="background-color:#FFA500;"&gt;&lt;h1 style="margin-bottom:0;text-align:center"&gt;网页标题:(*´▽｀)ノノ&lt;/h1&gt;&lt;/div&gt;&lt;div id="menu" style="background-color:#FFD700;height:200px;width:100px;float:right;"&gt;&lt;b&gt;菜单&lt;/b&gt;&lt;br&gt;HTML&lt;br&gt;SVG&lt;br&gt;JavaScript&lt;/div&gt;&lt;div id="content" style="background-color:#EEEEEE;height:200px;width:400px;float:left;"&gt;内容：你好帅！&lt;/div&gt;&lt;div id="footer" style="background-color:#FFA500;clear:both;text-align:center;"&gt;版权 © ARMIGO.FUN&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3.CSS样式 CSS 指层叠样式表 (Cascading Style Sheets) 样式定义如何显示 HTML 元素 样式通常存储在样式表中 把样式添加到 HTML 4.0 中，是为了解决内容与表现分离的问题 外部样式表可以极大提高工作效率 外部样式表通常存储在 CSS 文件中 多个样式定义可层叠为一 4.D3.JSD3 的全称是（Data-Driven Documents），顾名思义可以知道是一个被数据驱动的文档。说简单一点，其实就是一个 JavaScript 的函数库，使用它主要是用来做数据可视化的。 （1）文字展示方式： 12345678910111213141516&lt;body&gt;&lt;p1&gt;Hello World 1&lt;/p1&gt;&lt;br&gt;&lt;p2&gt;Hello World 2&lt;/p2&gt;&lt;script src=&quot;http://d3js.org/d3.v3.min.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;script&gt;d3.select(&apos;body&apos;).selectAll(&apos;p&apos;);//选择&lt;body&gt;中所有的&lt;p&gt;，其文本内容为practise，选择集保存在变量 p 中var p=d3.select(&apos;body&apos;) .select(&apos;p1&apos;) .text(&apos;practise&apos;);//修改段落颜色和字体大小p.style(&quot;color&quot;,&quot;red&quot;) .style(&quot;font-size&quot;,&quot;36px&quot;)&lt;/script&gt;&lt;/body&gt;]]></content>
      <categories>
        <category>可视化工具</category>
      </categories>
      <tags>
        <tag>D3.JS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯模型]]></title>
    <url>%2F2018%2F12%2F03%2FNB%2F</url>
    <content type="text"><![CDATA[1.朴素贝叶斯公式1.1概念简述简介下三个重要的条件概率的公式： \begin{align*} &(1)条件概率公式: P(A|B)=P(AB)/P(B)\ ，其中P(B)>0 \\ &(2)乘法公式：P(A∩B)=P(AB)=P(A|B)P(B)=P(B|A)P(A)\\ &\quad\ 推广：P(A1A2...An)=P(A1)P(A2|A1)P(A3|A1A2)...P(An|A1...An-1)\\ &(3)全概率公式：P(A)=\sum_{i=1}^{n}P(A|B_{i})P(B_{i}) \end{align*} （1）条件概率公式设A,B是两个事件，且P(B)&gt;0,则在事件B发生的条件下，计算事件A发生的条件概率。在条件概率中，最本质的变化是样本空间缩小了——由原来的整个样本空间缩小到了给定条件的样本空间。 （2）乘法公式表示A、B同时发生的概率可以看作两个概率的乘积。推广时候，就是多个事件交集的概率可以拆成单个事件Ai的条件概率的乘积形式。 （3）全概率公式就是下图中把整个样本空间S给划分为n块B区域，其中有一块彩色A事件，那么A事件可由每个事件Bi条件下A的部分概率求和而成。 1.2贝叶斯公式（Naive Bayes）废话不多说，先上朴素贝叶斯公式： \begin{align*} P(c_i|x) &= \frac{P(x|c_i)P(c_i)}{P(c_i)}\\ &= \frac{P(x|c_i)P(c_i)}{\sum_i P(x|c_i)P(c_i)} \end{align*}其中，第一个没有代入全概率公式。$P(c_i)$是先验概率（Prior probability），就是在条件x发生之前对类c有一个经验上的估计值，这个估计值是主观的、人为的，但是也是依据一定实际观察或者经验所得。由于主观的猜测值没有令人信服的证明，所以被频率学派所疑虑。比如我们可以猜想对抛1次硬币得到正面的概率为0.5，因为不是正面就是反面（假设没有竖立情况），这就是根据的经验值，因为你并没有去做大量（无穷）的实验来证明。 $P(c_i|x)$是后验概率（Posterior probability）。比如有十个不透明盒子，只有一个盒子里有球，则猜中有球的概率为1/10。当事件 x：打开一个盒子发现没有球，发生后，此时若再打开一个盒子有球的概率就变成了1/9。后验概率是建立在先验概率的基础上，通过贝叶斯公式建立起来的。它是一种“知果求因”的思路，这里打开后的“结果”发现没有球，此时才知道“原因”的概率是1/9。 $P(x|c_i)$是似然函数（Likelyhood），在机器学习中更多的时候它表示了第i个类C中出现属性x的概率，我们希望是使得类c中出现属性x的概率越大越好，因为反过来$P(c_i|x)$概率也越大。统计学中似然函数用符号$f(x|\theta)$，其中θ是需要估计的参数θ(事先假设已经确定有唯一θ)，x是随机变量x，是假设参数θ已知后我们观察到的样本应该是什么样子的，即在此参数θ下样本似然真实样本的程度，旨在最大化下面的似然函数以求需要估计的参数θ： L(\theta) = \prod_{i=1}^{n} f(x_i|\theta)上面就是大名鼎鼎的最大似然估计（MLE），是用来估计参数的方法，还有一种叫矩估计。最大似然估计和矩估计都是频率学派的经典方法。最大似然估计就是每个变量对应似然函数$f(x_i|\theta)$的乘积结果最大化即可停止，可获估计的参数θ。关于参数估计深入一点，有一个置信区间和可信区间区别，假设我们以一批样本可能服从高斯分布，通过多个样本X估计得到参数μ、σ后，置信区间表示有90%的置信程度是当前样本下的高斯分布是以这组参数μ、σ而构造，这里的置信区间只与样本相关，与参数θ无关，频率学派 认为模型的参数是固定的,不会随着样本的变化而变化，只是当前样本产生的某个参数可信度或大或小，抽样越多，频率越接近于真实概率，估计得到的参数越可信，即假定参数是唯一的，只是我们不知道，通过不断的抽样去估计这个参数罢了。而可信区间表示我们有很多个这样的参数，这些参数组成一个可信区间，从中抽取一个概率大一点的使用，比如90%下的参数。置信区间只有一组或一个参数，而可信区间有多组或多个参数。再提一下贝叶斯学派 观点：贝叶斯学派则认为参数不是唯一的，是一个随机变量，服从一个先验分布，而样本是固定的，通过先验分布、后验分布来获取参数。 分母部分$\sum_i{P(x|c_i)P(c_i)}$叫做”证据因子“（Evidence )，以保证各类别的后验概率总和为1从而满足概率条件，表示各个类别的出现属性x的概率总和。发现贝叶斯公式后验概率与分子部分成正比： P(c_i|x)∝P(x|c_i)P(c_i)1.2贝叶斯例子例子1： 症状 职业 疾病 打喷嚏 护士 感冒 打喷嚏 农夫 过敏 头疼 建筑工人 脑震荡 头疼 建筑工人 感冒 打喷嚏 教师 感冒 头疼 教师 脑震荡 某个医院早上收了六个门诊病人，如上表，现在又来了第七个病人，是一个打喷嚏的建筑工人。请问他患上感冒的概率有多大？ 根据公式$P(c_i|x) = \frac{P(x|c_i)P(c_i)}{P(c_i)}$ 可知： P(感冒|打喷嚏，建筑工人)=\frac{P(打喷嚏，建筑工人|感冒)\ast P(感冒)}{P(打喷嚏，建筑工人)}假设打喷嚏和建筑工人是相互独立的，则公式变为： P(感冒|打喷嚏，建筑工人)=\frac{P(打喷嚏|感冒)\ast P(建筑工人|感冒)\ast P(感冒)}{P(打喷嚏)\ast P(建筑工人)}其中：$P(感冒)=\frac{1}{2}$，$P(打喷嚏|感冒)=\frac{2}{3}$，$P(建筑工人|感冒)=\frac{1}{3}$，$P(打喷嚏) \ast P(建筑工人)=\frac{1}{2}\ast\frac{1}{3}=\frac{1}{6}$ ，代入上式子即可得： P(感冒|打喷嚏，建筑工人)=\frac{\frac{2}{3}*\frac{1}{3}*{\frac{1}{2}}}{\frac{1}{6}}=\frac{2}{3}\approx0.666说明此打喷嚏的建筑工人患感冒的概率为60.6%，依次计算打喷嚏、建筑工人的条件下患过敏、脑震荡的概率，然后有三个类别的后验概率，取最大的值作为该情况的预测值即可，这种模型常用于文本分类等。 例子2： 一种癌症，得了这个癌症的人被检测出为阳性的几率为90%，未得这种癌症的人被检测出阴性的几率为90%，而人群中得这种癌症的几率为1%，一个人被检测出阳性，问这个人得癌症的几率为多少？猛地一看，被检查出阳性，而且得癌症的话阳性的概率是90%，概率蛮大的，算下来看看。 我们用$A$表示事件 “测出为阳性”, 用$B_1$表示“得癌症”, $B_2$表示“未得癌症”。根据题目，我们知道如下信息: P(A|B_{1}) = 0.9, P(A|B_{2}) = 0.1, P(B_{1}) = 0.01, P(B_{2}) = 0.99那么我们现在想得到的是已知为阳性的情况下，得癌症的几率$P(B_{1},A)：$ P(B_{1},A) = P(B_{1}) \cdot P(A|B_{1}) = 0.01 \times 0.9 = 0.009表示1000人中只有9个人检测为阳性的是真的得癌症的。再来看看检测出阳性但是没有得癌症的概率： P(B_{2},A) = P(B_{2}) \cdot P(A|B_{2}) = 0.99 \times 0.1 = 0.099即1000人中间只有99个人检测为阳性但是没有得癌症的。那么在检测出阳性的前提下得癌症的概率，即把上面得值归一化即可： P(B_{1}|A)=\frac{0.009}{0.099 + 0.009} \approx 0.083显然，检测为阳性不得癌症的概率约是0.917，那么我们检测为阳性后，也不必万念俱灰，至少Thomas Bayes告诉你，你还有救。在这点上，他果然是上帝。 通过例子知道，贝叶斯的分母可以有2种计算方式，一种是以每个事件都是独立的，连乘积的形式。另一种是全概率公式展开的形式，依据不同情况活用。具体的算法流程请参考： https://blog.csdn.net/dataningwei/article/details/54140537 ，注意到此链接中因为分母对每类后验概率一样，所以后验概率与分子成正比，所以直接最大化分子部分即可。 2.多项式朴素贝叶斯朴素贝叶斯模型最大的特点是基于贝叶斯公式和特征的条件概率是独立的，这就给我们很大的简化空间和计算的方便。多项式朴素贝叶斯（Multinomial Naive Bayes） 比如对于文本分类来说，我们的目标就是获得最大的某一类的后验概率，这里我们假设每个特征之间是独立的： f(x)=argmax_{c_{k}}\ P(c_{k}|x)=argmax_{c_{k}} \frac{P(c_{k})\prod_{i=1}^{n}P(x_{i}|c_{k})}{\sum_{k}P(c_{k})\prod_{i=1}^{n}P(x_{i}|c_{k})}由于所有类的分子一样，所以直接做大化分母即可： f(x)=argmax \ P(c_{k})\ast\prod_{i=1}^{n}P(x_{i}|c_{k})对于$P(c_k),P(x_{i}|c_{k})$计算方法如下： P(c_{k})=\frac{N_{c_{k}}+\alpha}{N+k\alpha}N是总的样本个数，k是总的类别个数，$N_{c_k}$是类别为$c_k$的样本个数，$α$是平滑值。 P(x_{i}|c_{k})=\frac{N_{c_{k},x_{i}}+\alpha}{N_{c_{k}}+n\alpha}$N_{c_{k}}$是第k类$c_{k}$的样本个数，n是特征x的取值个数，$N_{c_{k},x_{i}}$是类别为$c_k$的样本中，第i维特征的值是$x_i$的样本个数，$α$是平滑值。因为若在预测时候，若预测中有特征没有出现在训练集中，那么$P(x_{i}|c_{k})$是0，所以整个乘积都是0,这样就没有意义了。为了防止概率为0，加上一个平滑，让它较好的反映原始概率。 当α=1时，称作Laplace平滑，当0&lt;α&lt;1时，称作Lidstone平滑，α=0时不做平滑。 例子可以参考这篇文章： https://blog.csdn.net/u012162613/article/details/48323777 ，里面还有其他2种模式的解释，不错哦。 3.NB和其他NB的区别（1）相同点：都是以贝叶斯模型为基础。 （2）朴素贝叶斯和多项式贝叶斯的区别： ​ 先看完此链接，有一个基本的认识： https://blog.csdn.net/gaotihong/article/details/78803197 ，主要对NB和MNB进行了概括，可以知道不同点有词向量的表示方式不同，NB是用0或1表示词在字典中出现与否，并且每个垃圾邮件的词向量一样长；但是MNB词向量是以每篇词长度为维度，词在字典中的序号为标记表示向量。另一个不同点我自己看的，在于似然函数的表示不同，我认为这才是真正的本质区别： NB算法图： MNB算法图： 可以看到上面第一张图NB算法中最下面似然函数的表示，垃圾邮件中词的概率：P（某词 j|垃圾邮件=1）=词j在几个垃圾邮件中出现/垃圾邮件的总个数 第二张图中MNB算法的似然函数：P（词 j |1=垃圾）= 所有垃圾邮件中词j出现的次数/所有垃圾邮件的词总量 可以发现似然函数不同。总结下：个别论文说是朴素贝叶斯只关心特征（词）出现还是不出现在一个类别中，但是多项式贝叶斯以每个特征（词）在样本集里的频次来决定类标。这样对NB和MNB有一个更加清晰的认识。 （3）朴素贝叶斯和各个贝叶斯之间的区别： （1）朴素贝叶斯的假设前提有两个第一个为：各特征彼此独立；第二个：对被解释变量的影响一致，不能进行变量筛选。贝叶斯显然是没有独立的前提，即各个特征之间可以有依赖的，也可以独立。 （2）GaussianNB、MultinomialNB和BernoulliNB的似然函数的计算方式不同。GaussianNB就是似然函数$P( x_{i} | c_{k})$为高斯分布的朴素贝叶斯，MultinomialNB就是似然函数为多项式分布的朴素贝叶斯，而BernoulliNB就是似然函数为伯努利分布的朴素贝叶斯。 （3）一般来说，如果样本特征的分布大部分是连续值，使用GaussianNB会比较好。如果如果样本特征的分大部分是多元离散值，使用MultinomialNB比较合适。而如果样本特征是二元离散值或者很稀疏的多元离散值，应该使用BernoulliNB。 4.参考（1）贝叶斯的三种模型 （2）例子1来源 （3）贝叶斯和MCMC博客 （4）贝叶斯网络与朴素贝叶斯的区别]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Naive Bayes</tag>
        <tag>多项式朴素贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[向量空间模型（VSM）]]></title>
    <url>%2F2018%2F11%2F28%2F%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[1.词袋模型(BOW)​ 简单理解下词带模型，顾名思义就是一个袋里装了很多单词，可知特点：（1）假设词语无序 （2）假设词与词之间独立。Bag-of-words模型是信息检索领域常用的文档表示方法。忽略了它的单词顺序和语法、句法等要素，将其仅仅看作是若干个词汇的集合。 它具体是如何表示的？如下有一个句子： I have a pear and a strawberry. 于是可以构建一个字典，如下： {“i”:0,”have”:1,”a”:2,”pear”:3,”and”:4,”strawberry”:5} ，这个字典里面的字符可表示出所有的词，且唯一。但是字典里面词的顺序和原句子词序并不相关，这里只是特例。于是可以建立一个词向量如下： [1,1,2,1,1,1] ，这里是字典里面的词在原句子里出现的次数的统计，一一对应。于是若有若干句子组成一篇文档，则可以构建一个对应的字典，然后有一个N维的向量来表示这篇文档（词频的统计）。若要比较2篇文档相似度，也可以比较2个向量之间的余弦相似度。 2.向量空间模型(VSM)2.1VSM概念​ 向量空间模型（Vector Space Model）和词袋模型很像（个别说是一样的），都是把一个文档表示成向量的模式，而向量空间模型运用较为广泛的是TF-IDF方法。 第1：把一篇文档表示成词向量：D=[W_1,W_2,W_3,…,W_m],某D文档中有m个项。这里可以是的单个词，可以是词组等等。 第2：计算每个项的贡献度：Q =[Q_1,Q_2,Q_3,…,Q_m] ,这里的Q_i是每个项的贡献度，名字不一，也可以理解为每一项（每个词）的比重，权重。计算贡献度的方法不一。 于是每个文档便由一组词向量和对应的贡献度向量表示而成。 2.2TF-IDF方法其实就是第2步的算法不同，这里的TF-IDF分为TF（每一项的频率：Term Frequency）和IDF（逆文档率：Inverse Document Frequency），这里的TF，就是每个词在文档中的频次比例。如下： TF =\frac{N_i}{M},N_i是第i个词在某一篇文档的频次，M是该文档的总词量。IDF就是这个词在哪些文档出现了，因为语料库由K篇文档组成，每篇文档M个词。计算如下： IDF =log(\frac{K}{L+1}),K表示语料库有k篇文档，L表示包含该词的文档数量，L+1防止分母为0。那TF-IDF计算公式如下： TF-IDF =\frac{N_i}{M}*log(\frac{K}{L+1})=\frac{N_i}{M}*log(\frac{1}{D_i}),D_i是某个词出现在所有文档的中的频次比例。发现：$log_a^b​$（底数a&gt;1）函数是单调增函数，且定义域在(0,1]时值域为负值，大于1时为单调正数。由于概率是在[0,1]的，倒数就是大于等于1，这里分母取不到0，倒数就不会是正无穷 ，log(b)值就是一个正数，并且b越大IDF就越大。这里其实K/L是某个词的在语料库中的文档率的倒数。所以当K和L的比值越大，IDF就越大，说明K和L比例差的越大，对我们越有利。比如有20篇文档，若一个词在所有文档中出现了1次，K/(L+1)=20/(1+1)=10,log(10)=3.32，相对其他来说是一个蛮大的比重。那这个词对我们来说也确实是重要的，在做文档相似度的时候作用就很大，因为只要这个词出现说明该词有很好的区分性。相反，若K/L的比值很小，比如K/(L+1)=1时，log1=0,比重就很小。为什么该词比重需要设置的小？因为K/(L+1)==1，说明K和L相近，即这个词在每个文档都出现，那这个词很可能是介词、连词等没有区分性的词，比如’the’，’for’,’的’，’在’这些词很大概率出现在所有文档中，所以不重要，权重就小。当然，也可以通过设置停用词的方法来筛掉这些词，这个是数据预处理的部分了。 比如语料库中共有10篇文档，每篇800词。某一篇文档中的”战斗机” 和 “的”都出现了20次，但是”战斗机” 在2篇文档中出现，而”的”在9篇文档中出现，”的”字符的IDF=TF*log1=0，这样算下来”战斗机” 的TF-IDF的值肯定比0大，故”战斗机” 这个词更加重要。然后排序后，可以取前n个重要的词放入分类器做分类也可，计算相似度也可。其实，TF-IDF计算方法类似于交叉熵，贡献度的计算法子还有很多，按下不表。 补充一下我在weka包中遇到的问题：weka里面的TF和IDF可以分开来设置，若两者都是True,则表示TF-IDF=log（1+某文档中某个词的频次） x log（逆文档率），按照这种计算出来得值可以大于1，之前一直没想明白，因为我把TF的频次算成了频率所以一直小于1。 BOW和VSM两者的区别其实很小，我自己看来，就在于贡献度不一。 2.3熵的计算方法这里其实也算是特征选择，本文只给出了特征选择的计算方法。特征选择是一个系统的工程，有自己的方法和技术。本文特指章节2.1里面第2步骤中贡献度的计算方法。熵、联合熵、条件熵、交叉熵与相对熵的意义？如下图： （1）信息熵 前面提到了交叉熵，先理解什么是熵。熵是由香农提出的，记得有位科学家说：若自己余生也发明一个类似“熵”这样的概念，这一生也值了。可见其重要程度，后来证实熵在计算机领域，通信领域，信息论等等都是举足轻重的。到底什么是熵？直接给出离散变量x的熵公式： H(X)=-\sum\limits_{x\in\mathcal{X}}p(x)\log p(x)作用： 用来度量信息的不确定程度。举个栗子：若你在3个盒子中抽一个奖，设每个盒子抽到奖的概率=1/3，代入熵公式=- 1/3 x log(1/3) - 1/3 x log(1/3) - 1/3 x log(1/3) =0.47712。但是第二个人由于商家作弊，使得第2个盒子抽到的概率上升为0.8，其他2个都是0.1,H(X) = -0.1 x log(0.1) x 2 - 0.8 x log(0.8) = 0.277528。显然这个第二个熵变小了，但是确实符合它反映的信息量。因为第二个事件的信息量更加大。信息熵的特点是熵越小，表示信息越纯，即信息量越大。所以，这个值和我们的事实是符合的。 （2）条件熵 定义：在一个条件下，随机变量的不确定性。 ​ 条件熵公式：$H(X|Y)=-\sum\limits_{x,y}{P(X,Y)*log(P(X|Y))}$ 证明如下图： 下面证明 条件熵=联合熵-单独的熵：$H(Y|X)=H(X,Y)-H(X)$： 举个栗子吧： https://zhuanlan.zhihu.com/p/26551798 ,这篇便有。 （3）信息增益（Information Gain） IG=熵 - 条件熵即为： Grain(Y,X)=H(Y)-H(Y|X)例子请参考周志华《机器学习》P75。 （4）相对熵 相对熵又叫KL散度，是用来衡量2个概率分布的差异程度。交叉熵即KL散度的计算公式： D_{KL}(p||q)=\sum_{i=1}^np(x_i)log(\frac{p(x_i)}{q(x_i)}) \tag{1}这里的p是真实分布，比如我们遇到一个复杂的p分布，要做的事就是用一个简单的分布q来代替复杂的、不好计算的真实分布p，KL散度就是用来衡量这2个分布的差异。发现log后面p(xi)和q(xi)越接近时候，KL散度值近乎为0。就是说KL散度值越小，两个分布越接近，即可以用后面的q分布来代替真实分布p。在变分推断里会遇到这个概念，我等只能敬而远之。交叉熵在部分问题时候，也可以做为loss函数。比如：https://blog.csdn.net/tsyccnh/article/details/79163834 大致把熵过了一遍，说的好不如做得好，还是多实践吧。$Vouloir\ \ c’est\ \ pouvoir$（有志者，事竟成）。 （5）交叉熵 交叉熵参考： https://www.cnblogs.com/kyrieng/p/8694705.html#name2 3.参考1.词袋模型简介 2.逆文档频率法]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>向量空间模型</tag>
        <tag>词袋模型</tag>
        <tag>信息熵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性空间的定义]]></title>
    <url>%2F2018%2F11%2F19%2F%E7%BA%BF%E6%80%A7%E7%A9%BA%E9%97%B4%E5%AE%9A%E4%B9%89%2F</url>
    <content type="text"><![CDATA[做了一个关于线性空间的思维导图，纯粹练练思维导图的作图技术。希望有启发。 Bonne chance.]]></content>
      <categories>
        <category>矩阵分析</category>
      </categories>
      <tags>
        <tag>线性空间定义</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归和python3实现]]></title>
    <url>%2F2018%2F11%2F10%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[1.逻辑回归笔记​ 逻辑回归就是在线性回归的基础上套一个分类函数，当然也可以不用线性回归。下面是我根据周志华的逻辑回归写的笔记。具体请参考周志华的《机器学习》P57-60。第1张图是说明了逻辑回归的预测模型，第2张图是优化目标L的由来，第3张图是详细的求出一阶导数和二阶导数。我觉得这里一阶导数和二阶导数周志华这里写的不好，矩阵计算写的不清晰明朗。当然这也是自己基础不牢靠所致，共勉吧。如下： 图1：得到两个后验概率的表达式P0和P1，用极大似然估计法找出优化目标L。 图2：再把P0，P1和转化后等价的似然项代入对数似然L,简化后即可得最终优化目标L。 图3：L对2个参数的求导。 最后由牛顿法得到迭代参数公式和优化目标L。 ​ 在之前计算的时候出现了奇异矩阵，即矩阵的行列式值为0，想了下可能是之前的数据之间存在线性相关性，所以导致矩阵的秩不满秩，所以也就不可逆了，即变成了奇异矩阵。若我不做实验，就不会知道奇异矩阵，也就记不起来以前的秩了。想着一些小算法，可以重复造轮子，但是复杂的算法就算了，没必要。人生奇美，怎可废！ ​ 2.python3实现用的python3.5.1+pycharm,将就着看吧。只用了5个样本，准确率不高的，简单的实现了预测，没有深究。下面是伪代码： 上面的终止条件就是人为设定的次数或者是优化目标L达到可接受的范围，否则一直迭代。 具体实现如下,代码在这里排列的不好，但是可以运行： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from numpy import *from math import *import numpy as npfrom numpy.linalg import invdef sigmoid(x): return 1./(1.+np.exp(-x))def logistic_regression(): # 牛顿方法构建Logistic Regression , 对西瓜是否为好瓜或坏进行分类, #Y &lt;&lt;--预测为-- Sigmoid(w1*x1+w2*x2 + bias) X = np.array([[0.697,0.774,0.634,0.666,0.243], [0.46,0.376,0.264,0.091,0.267], [1,1,1,1,1]]) Y =np.array([1,1,1,0,0]) beta = np.array([[0], [0], [1]]) maxCycles = 20 #迭代次数 j = 0 #累积次数 old_L = 0 while j &lt;= maxCycles: #循环20次后者损失函数已经达到合理小的范围就停止迭代 beta_T_x = dot(beta.T[0], X) # 计算β_TX L = 0 # 损失函数 for i in range(5): L = L +(-Y[i]*beta_T_x[i] + log(1 + exp(beta_T_x[i]))) if abs(L-old_L)&lt;= 0.00001: break print('最优化目标L=',L) old_L = L grad = 0 # 一阶导数 H = 0 # 二阶导数 # 计算5个训练样本的优化目标,公式（3.27) for i in range(5): # 梯度: ∂L(β)/∂β，P60公式（3.30）。 grad = grad - dot(array([X[:,i]]).T,(Y[i] - array([[exp(beta_T_x[i])/(1+exp(beta_T_x[i]))]]))) #X[i]是横向量，需要转制，grad结果是1个数 H = H + dot(array([X[:,i]]).T,array([X[:,i]]).T.T)*((exp(beta_T_x[i])/(1+exp(beta_T_x[i]))) * (1-exp(beta_T_x[i])/(1+exp(beta_T_x[i])))) beta = beta - dot(inv(H),grad) #参数迭代公式（3.29） j += 1 #预测部分 print('beta=', beta) x1 = float(input('请输入西瓜的密度:')) x2 = float(input('请输入西瓜的含糖率:')) z = beta[0][0] * x1 + beta[1][0] * x2 + beta[2][0] print('预测值为：',sigmoid(z)) f = sigmoid(z) if(f==1): #P66公式（3.47)正、反例的比值&gt;观测的正反比例即认为正例，否则反例 print('分类器预测此为好瓜。') else: print('分类器预测此为坏瓜。')logistic_regression() 预测结果如下：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Logistic Regression</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
</search>
