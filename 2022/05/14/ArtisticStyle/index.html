<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="风格迁移," />










<meta name="description" content="1.风格迁移概述​       《A Neural Algorithm of Artistic Style》论文地址：https://arxiv.org/pdf/1508.06576.pdf ，目标是把艺术风格的图片迁移到普通图片。比如把梵高的星月夜（The Starry Night）迁移到普通图片上。 ​       CNN每一层单元可以理解为图像滤波器的集合，每个滤波器从输入图像中提取某种特征">
<meta name="keywords" content="风格迁移">
<meta property="og:type" content="article">
<meta property="og:title" content="《A Neural Algorithm of Artistic Style》图像风格迁移论文翻译+代码">
<meta property="og:url" content="http://yoursite.com/2022/05/14/ArtisticStyle/index.html">
<meta property="og:site_name" content="Hello,Armigo!">
<meta property="og:description" content="1.风格迁移概述​       《A Neural Algorithm of Artistic Style》论文地址：https://arxiv.org/pdf/1508.06576.pdf ，目标是把艺术风格的图片迁移到普通图片。比如把梵高的星月夜（The Starry Night）迁移到普通图片上。 ​       CNN每一层单元可以理解为图像滤波器的集合，每个滤波器从输入图像中提取某种特征">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2022/05/14/ArtisticStyle/1.png">
<meta property="og:image" content="http://yoursite.com/2022/05/14/ArtisticStyle/2.png">
<meta property="og:image" content="http://yoursite.com/2022/05/14/ArtisticStyle/3.png">
<meta property="og:image" content="http://yoursite.com/2022/05/14/ArtisticStyle/v1.png">
<meta property="og:image" content="http://yoursite.com/2022/05/14/ArtisticStyle/vgg19.png">
<meta property="og:image" content="http://yoursite.com/2022/05/14/ArtisticStyle/white_nose.jpg">
<meta property="og:image" content="http://yoursite.com/2022/05/14/ArtisticStyle/7.jpg">
<meta property="og:image" content="http://yoursite.com/2022/05/14/ArtisticStyle/04.png">
<meta property="og:image" content="http://yoursite.com/2022/05/14/ArtisticStyle/03.png">
<meta property="og:updated_time" content="2022-05-22T04:04:14.177Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="《A Neural Algorithm of Artistic Style》图像风格迁移论文翻译+代码">
<meta name="twitter:description" content="1.风格迁移概述​       《A Neural Algorithm of Artistic Style》论文地址：https://arxiv.org/pdf/1508.06576.pdf ，目标是把艺术风格的图片迁移到普通图片。比如把梵高的星月夜（The Starry Night）迁移到普通图片上。 ​       CNN每一层单元可以理解为图像滤波器的集合，每个滤波器从输入图像中提取某种特征">
<meta name="twitter:image" content="http://yoursite.com/2022/05/14/ArtisticStyle/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2022/05/14/ArtisticStyle/"/>





  <title>《A Neural Algorithm of Artistic Style》图像风格迁移论文翻译+代码 | Hello,Armigo!</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hello,Armigo!</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/05/14/ArtisticStyle/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Armigo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/0.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hello,Armigo!">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">《A Neural Algorithm of Artistic Style》图像风格迁移论文翻译+代码</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2022-05-14T12:04:08+08:00">
                2022-05-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CV/" itemprop="url" rel="index">
                    <span itemprop="name">CV</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2022/05/14/ArtisticStyle/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2022/05/14/ArtisticStyle/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>次阅读
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="1-风格迁移概述"><a href="#1-风格迁移概述" class="headerlink" title="1.风格迁移概述"></a>1.风格迁移概述</h1><p>​       《A Neural Algorithm of Artistic Style》论文地址：<a href="https://arxiv.org/pdf/1508.06576.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1508.06576.pdf</a> ，目标是把艺术风格的图片迁移到普通图片。比如把梵高的星月夜（The Starry Night）迁移到普通图片上。</p>
<p>​       CNN每一层单元可以理解为图像滤波器的集合，每个滤波器从输入图像中提取某种特征。CNN网络被用来训练目标识别时候，这会发展为一种表示形式：对象信息沿着处理层次结构越来越明确。<strong>即网络不同层次的响应描述了图像不同层次的信息：低层次描述小范围的边角、曲线，中层次描述方块、螺旋，高层次描述内容。</strong>什么意思？在不同层有不同的图像表现：<strong>在深度更大的层提取出来的图像特征是不仅仅限于对原始输入图像像素点值的拷贝，也会从对象提取更加高级的特征(比如图1中的d，e)；相反，浅层的网络提取出来的特征只是简单的拷贝原始图片的像素值（比如图1中的a，b，c）。因此，我们将网络较高层的特征称为内容表示层。</strong></p>
<p>​        为了获得输入图片的风格表示方式，本文采用了一种特征空间较为原始的捕捉纹理信息，这个特征空间建立在网络每层过滤器之上。这种特征空间是由不同过滤器的相关性组成，是对特征映射的空间信息的反馈。通过包含多层的特征相关性，我们得到输入图像的固定的、多尺度的表征，它捕获原图的纹理信息，但不是全局都捕捉。（到底是不全局捕捉纹理信息还是不全局实施？）</p>
<p><img src="/2022/05/14/ArtisticStyle/1.png" alt="1"></p>
<p>​    <strong>CNN。</strong>一张给定的输入图像可以表示为CNN不同过程阶段中的过滤图像的集合。随着层次结构增加（网络层），不同过滤器的数量也增加。为了减少网络中每一层的单元数量，会用一些下采样的方法（比如max-pooling）来减小过滤图像的大小。</p>
<p>​    <strong>内容重建。</strong>我们可以通过只知道网络某一层的反馈来重构输入图像，从而可视化CNN中不同处理阶段的信息。我们从原始vgg网络的’ conv1_1 ‘ (a)， ‘ conv2_1 ‘ (b)， ‘ conv3_1 ‘ (c)， ‘ conv4_1 ‘ (d)和’ conv5_1 ‘ (e)层重构输入图像。我们发现从开始的浅层网络重建图像几乎是完美的(a,b,c)。在网络的较深层的网络中，详细的像素信息被丢失，而图像的高级内容被保留（d，e）。</p>
<p>​    <strong>风格重建。</strong>在原始CNN表示的基础上，我们构建了一个新的特征空间，来捕捉输入图像的风格。<strong>风格表征是计算CNN不同层中不同特征之间的相关性。</strong>我们通过样式特征矩阵来重构输入图像的风格，样式特征矩阵是基于CNN不同子层构建。根据给定图像，这样可以创建风格相匹配的图像，同时丢弃场景的全局布局信息。（这最后一句没懂，可能是学习到了原图风格信息，但是不是照抄原图的整体布局框架，否则几乎是照抄）</p>
<p>​       再啰嗦一遍：给定一张图片，我们能够基于神经网络中不同层的风格特征矩阵来构造一幅图像。实际上，基于风格特征矩阵重建图像时，会基于输入图片生成不同纹理版本的图像，并会<strong>捕捉原图在颜色和局部结构的大致样貌特征</strong>。此外，<strong>随着网络处理层次的递进，局部图像的构造大小和复杂程度也会增加，这一结果可以解释为感受野的大小和特征复杂性的增加导致的。我们指定这种多尺度表征为风格表征方式（即风格矩阵）。</strong></p>
<p>​       <strong>本文的wow点：发现CNN网络中的内容矩阵和风格矩阵可以分开来表示。</strong>也就是说，我们可以独立地操作这两个矩阵来生成新的或者感知有意义的图片。为了证实这个发现，我们从内容图片源和风格图片源中混合生成若干图。特别地，我们用一幅描绘德国图宾根的内卡河畔的照片作为内容表征和几幅不同时期的著名艺术画像作为风格表征来配对(图2)。</p>
<p><img src="/2022/05/14/ArtisticStyle/2.png" alt="2"></p>
<p>​    这些合成图被发现可同时匹配到照片的内容特征和艺术画像各自的风格特征（细看绘画细节处理的方式话）。在保留原始照片的整体布局的同时，通过艺术画像的颜色、局部构造来组成整体的风景图。实际上，尽管合成图像的内容和照片是一样的，但这样使得照片变成了艺术画像的风格，从而使合成图像的外观类似于艺术作品。</p>
<p>​    如上所述，风格表征是在神经网络中多层网络的多尺度表征。在图2，风格表征包含来自整个网络层次结构的神经网络层（即风格矩阵是由网络中所有层的特征组成，不是单一的1、2个特征，比如梵高的《星月夜》里面有蓝色天空、黑色房子，天空中有黄色的月亮和星星，这些都是此画像的风格，都可以用单独的矩阵来表示，此为个人猜想，按照本文理论是成立的）。风格也可以通过只包含少量较低网络层来更局部地定义，从而产生不同的视觉体验。当将样式表示与网络中的更高层匹配时，局部图像的结构尺度会越来越大，从而获得更平滑、更连续的视觉体验。因此，视觉上最吸引人的图像通常是通过将样式表示与网络中的最高层相匹配来创建的。（即低层次的网络学习到的风格矩阵比较菜鸡，只能生成似像似不像艺术画像的风格，因为还没有学习好画像的风格，但是后面风格矩阵较好的提取出了风格特征，所以能够结合的更加好，生成的照片自然好看，比如图3的最后一行图。）</p>
<p><img src="/2022/05/14/ArtisticStyle/3.png" alt="1651292853251"></p>
<p>​    图3的行是随着CNN子层的增加而展现的风格的表征。我们发现局部图像的结构是可以被风格矩阵表现，并且更加高层的网络中，会增加风格矩阵的大小和复杂度。</p>
<p>​    当然，图片内容和风格的表示并不能完全的分开来（看到这，玩我呢，不过仔细想想也是，内容中有风格，风格也是内容的一部分）。当我们用一张风格图片和一张内容图片来合成一张图时候，不会存在一张可以完美适配这2个限制条件的图片。但是，我们在图像合成过程中最小化的损失函数会分别包含内容和风格2个方面，这2个可以很好的分来开。因此，我们重建图像中可以顺利地要么侧重内容要么侧重风格。（竖着看图3果然，大佬是说这些话凑字数吗，逗我呢）过分强调风格会导致生成的图片去主要学习艺术画像的样貌，有效地给生成的图片一个纹理化的版本，但是几乎没有学习到照片的内容信息（图3第1列）。当我们过分专注于内容的学习上，人们可以清楚地辨认出这张照片（的内容），但是图像的风格并没有很好的学到（图3最后1列）。在一对特定的源图对，我们可以调整内容和风格之间的权衡点，以创建具有视觉吸引力的图像。</p>
<p>​    这里我们展现了一个人造的神经网络系统：可以把图像的内容从风格中分开来，因此，可以基于任何其他图像的风格来重塑另一张图片的内容。我们通过随机选择的照片内容与几个著名的绘画风格相融，创建了若干新的艺术风格的图片。特别地，我们基于目标识别训练出的高性能深层神经网络的特征反馈上，获得了图像内容和风格的神经网络表示方式。据我们所知，这在自然图片中是第一次演示将图片的内容和风格分开来（作者的意思我超屌的，快夸夸我）。之前关于从风格中分离内容的工作是基于复杂度低得多的感官输入进行评估的，比如不同笔迹的字符或不同人脸或小人物的不同姿态图像。（作者能有啥坏心思呢，我不过实话实说罢了，前人做的没我吊，我也很无奈）</p>
<p>​    在我们的演示中，我们把一组著名艺术画的风格分别移植到了一张照片。这种做法通常在计算机视觉中叫做非真实性渲染。（比如皮克斯的卡通风格的渲染，AI换脸等）在概念上最密切相关的是改变纹理来实现艺术风格的转变。但是，这些以前的方法主要依赖于非参数技术来直接操纵图像的像素表示。相反，通过DNN训练的目标识别模型，我们在特征空间中执行操作，并清晰地展现了一幅图像的高阶内容。（这里的高阶内容大概指的是图像的风格，毕竟内容把现实的像刻下来即可，但是风格更加能表达作画人的思想）</p>
<p>​    基于对象识别训练的深层神经网络的特征以前曾被用于风格识别，以便根据艺术品创作的时期对其进行分类。分类器模型在原始网络最顶层的激活层训练而得，我们称之为内容表示。我们推测，转换为固定特征空间（例如我们的样式表示）可能会在样式分类中获得更好的性能。</p>
<p>​    总的来说，我们合成图像的方法是从不同图源中混合内容和风格，在艺术、图片风格和图片内容的单独外观感知和神经网络的表达提供了一个新的、具有吸引力的工具。我们引入2个独立的，感知上有意义的变量源来设计较为新颖的激励因素：图像的内容和外观。我们期望这种方式对广泛心理物理学、功能成像甚至神经电生理记录的视觉感知实验研究有所帮助。实际上，我们的工作对独立的捕捉图像的内容和呈现的风格提供了一种算法层面的理解。重要的是，我们风格表征的数学表达式产生了一个的清晰、可验证的假设，这在图片外观的表达上，可以降低到单个神经元级别。通过简单计算神经网络中不同类型的神经元 的相关性来表示风格特征。提取神经元的相关性在生物上是较为合理的计算方式，比如：由初级视觉系统中的所谓复杂细胞（复杂细胞：对于一束特定方向的光线可发生反应,其光接受区不分中央及周边位置）实现。我们的结果表明，沿着腹侧流的不同处理阶段，展现一个复杂细胞的计算的方式，是一种在视觉输入的外观中获取内容的独立表达的可能性方式。</p>
<p>​    总之这是这样让人非常着迷的神经系统：它可以作为一种被训练来执行生物视觉核心的可计算任务，能够自动将图像的内容和风格分开来表示。当学习目标识别时，可理解为网络必须对所有的图像变化保持不变，以维持目标的同一性。（理解为图片不管怎么变，训练好的模型该提取什么目标就是什么目标）将图像内容的变量及其外观的变量分开来表示对于这项任务非常有用的。因此，我们从风格中提取内容的能力、创造和欣赏艺术的能力，可能主要是我们视觉系统强大推理能力的一个卓越特征。</p>
<p>​    <strong>视觉皮层</strong>（英文：Visual cortex）是指<a href="https://zh.wikipedia.org/wiki/%E5%A4%A7%E8%84%91%E7%9A%AE%E5%B1%82" target="_blank" rel="noopener">大脑皮层</a>中主要负责处理<a href="https://zh.wikipedia.org/wiki/%E8%A7%86%E8%A7%89" target="_blank" rel="noopener">视觉</a>讯息的部分，位于大脑后部的<a href="https://zh.wikipedia.org/wiki/%E6%9E%95%E5%8F%B6" target="_blank" rel="noopener">枕叶</a><a href="https://zh.wikipedia.org/wiki/%E8%A7%86%E8%A7%89%E7%9A%AE%E5%B1%82#cite_note-:0-1" target="_blank" rel="noopener">[1]</a><a href="https://zh.wikipedia.org/wiki/%E8%A7%86%E8%A7%89%E7%9A%AE%E5%B1%82#cite_note-2" target="_blank" rel="noopener">[2]</a>。<a href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E7%B1%BB" target="_blank" rel="noopener">人类</a>的视觉皮层包括初级视皮层（V1，亦称纹状皮层（Striate cortex））以及纹外皮层（Extrastriate cortex，例如V2，V3，V4，V5等）。</p>
<p><img src="/2022/05/14/ArtisticStyle/v1.png" alt="34"></p>
<p>​    这里的腹侧流也是生理学方面的术语，在“<a href="https://zh.wikipedia.org/wiki/%E5%8F%8C%E6%B5%81%E5%81%87%E8%AF%B4" target="_blank" rel="noopener">双流假说</a>”模型中，初级视皮层（V1）的输出讯息到两个渠道，分别成为<strong>背侧流</strong>（Dorsal stream）和<strong>腹侧流</strong>（Ventral stream）<a href="https://zh.wikipedia.org/wiki/%E8%A7%86%E8%A7%89%E7%9A%AE%E5%B1%82#cite_note-3" target="_blank" rel="noopener">[3]</a>。如上图背侧流（红色箭头）和腹侧流（绿色箭头）。其中腹流又被称为内容通路（what pathway），处理物体识别；背流又被称为空间通路（where pathway），处理空间位置信息。</p>
<p>​    看到这里，那在人眼视觉神经系统中捕捉到的风格的表达，在腹流中展现的可能性更高。那么在以人造神经网络的过滤器上提取风格特征空间，也看做是在腹流中提取信息。当然，就像作者说的，风格和内容最终都是不可分割的，猜测哪怕是大脑在真实神经系统中处理图像的内容和位置信息，也不一定是绝对分开处理的。但我们总是试图按照奥卡姆剃刀原则去用一个较为简单的原理或者模型，去解释看到的现象。那么，单独的分开图像的内容和风格也许能解释一部分我们未知的事务，甚至居然能做出一些意想不到所谓的创造。</p>
<h1 id="2-方法"><a href="#2-方法" class="headerlink" title="2.方法"></a>2.方法</h1><p>​    正文中呈现的结果是在 VGG 网络的基础上生成的，VGG 网络是一种卷积神经网络，可在常见的视觉对象识别基准任务上与人类的表现相媲美，并在<a href="https://arxiv.org/pdf/1409.1556.pdf" target="_blank" rel="noopener">参考文献22</a>进行了介绍和广泛描述。我们使用了VGG19网络的5个池化层、第16 层的卷积层来捕捉特征空间。我们不适用任何的全连接层。该模型是公开的，可以在 caffe框架中进行把玩。为了合成照片我们发现用平均池化替代最大值池化层能提高梯度下降流、能获得一个稍微更好的吸引人的结果，这就是为什么展示的图像是使用平均池化生成的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">=================================================================</span><br><span class="line">Layer (type:depth-idx)                   Param <span class="comment">#</span></span><br><span class="line">=================================================================</span><br><span class="line">VGG19                                    --</span><br><span class="line">├─Conv2d: <span class="number">1</span><span class="number">-1</span>                            <span class="number">1</span>,<span class="number">792</span></span><br><span class="line">├─Conv2d: <span class="number">1</span><span class="number">-2</span>                            <span class="number">36</span>,<span class="number">928</span></span><br><span class="line">├─MaxPool2d: <span class="number">1</span><span class="number">-3</span>                         --</span><br><span class="line">├─Conv2d: <span class="number">1</span><span class="number">-4</span>                            <span class="number">73</span>,<span class="number">856</span></span><br><span class="line">├─Conv2d: <span class="number">1</span><span class="number">-5</span>                            <span class="number">147</span>,<span class="number">584</span></span><br><span class="line">├─MaxPool2d: <span class="number">1</span><span class="number">-6</span>                         --</span><br><span class="line">├─Conv2d: <span class="number">1</span><span class="number">-7</span>                            <span class="number">295</span>,<span class="number">168</span></span><br><span class="line">├─Conv2d: <span class="number">1</span><span class="number">-8</span>                            <span class="number">590</span>,<span class="number">080</span></span><br><span class="line">├─Conv2d: <span class="number">1</span><span class="number">-9</span>                            <span class="number">590</span>,<span class="number">080</span></span><br><span class="line">├─Conv2d: <span class="number">1</span><span class="number">-10</span>                           <span class="number">590</span>,<span class="number">080</span></span><br><span class="line">├─MaxPool2d: <span class="number">1</span><span class="number">-11</span>                        --</span><br><span class="line">├─Conv2d: <span class="number">1</span><span class="number">-12</span>                           <span class="number">1</span>,<span class="number">180</span>,<span class="number">160</span></span><br><span class="line">├─Conv2d: <span class="number">1</span><span class="number">-13</span>                           <span class="number">2</span>,<span class="number">359</span>,<span class="number">808</span></span><br><span class="line">├─Conv2d: <span class="number">1</span><span class="number">-14</span>                           <span class="number">2</span>,<span class="number">359</span>,<span class="number">808</span></span><br><span class="line">├─Conv2d: <span class="number">1</span><span class="number">-15</span>                           <span class="number">2</span>,<span class="number">359</span>,<span class="number">808</span></span><br><span class="line">├─MaxPool2d: <span class="number">1</span><span class="number">-16</span>                        --</span><br><span class="line">├─Conv2d: <span class="number">1</span><span class="number">-17</span>                           <span class="number">2</span>,<span class="number">359</span>,<span class="number">808</span></span><br><span class="line">├─Conv2d: <span class="number">1</span><span class="number">-18</span>                           <span class="number">2</span>,<span class="number">359</span>,<span class="number">808</span></span><br><span class="line">├─Conv2d: <span class="number">1</span><span class="number">-19</span>                           <span class="number">2</span>,<span class="number">359</span>,<span class="number">808</span></span><br><span class="line">├─Conv2d: <span class="number">1</span><span class="number">-20</span>                           <span class="number">2</span>,<span class="number">359</span>,<span class="number">808</span></span><br><span class="line">├─MaxPool2d: <span class="number">1</span><span class="number">-21</span>                        --</span><br><span class="line">├─Linear: <span class="number">1</span><span class="number">-22</span>                           <span class="number">102</span>,<span class="number">764</span>,<span class="number">544</span></span><br><span class="line">├─Linear: <span class="number">1</span><span class="number">-23</span>                           <span class="number">16</span>,<span class="number">781</span>,<span class="number">312</span></span><br><span class="line">├─Linear: <span class="number">1</span><span class="number">-24</span>                           <span class="number">40</span>,<span class="number">970</span></span><br><span class="line">├─ReLU: <span class="number">1</span><span class="number">-25</span>                             --</span><br><span class="line">├─Softmax: <span class="number">1</span><span class="number">-26</span>                          --</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/14/ArtisticStyle/vgg19.png" alt="4"></p>
<p>上图是VGG19网络框架，下图是白噪图像样例，本文作初始化图像矩阵用。</p>
<p><img src="/2022/05/14/ArtisticStyle/white_nose.jpg" alt="4"></p>
<p>​    通常，网络中的每一层都定义了一个非线性滤波器组，其复杂性随着层在网络中的位置而增加。因此给定一张输入图像$\vec{x}$，$\vec{x}$在CNN中的每一层编码是通过过滤器对图像的特征提取而得。某层网络若有$N_l$个不同过滤器就有$N_l$个不同的特征映射，每个特征映射大小为$M_l$。$M_l$的大小是特征映射的高乘以宽。所以这些第$l$层的特征映射可以存储在一个矩阵$F_l$，$F^{l} \in{R}^{N_{l} \times M_{l}}$，$F_{i j}^{l}$是在第$l$层在第$j$个神经元节点第$i^{th}$个过滤器的激活值。为了可视化在不同网络层次结构中被编码的图像信息（参考图1内容重建），我们对白噪声图像执行梯度下降以找到与原始图像的特征反馈相匹配的另一个图像。我们用$\vec{x}$表示输入图像，$\vec{p}$表示生成的图像矩阵，$F^l$、$P^l$是他们各自在$l$层对应的特征映射矩阵。我们然后可以用这2个特征的表征矩阵构造平方差损失函数，如下。</p>
<script type="math/tex; mode=display">
\mathcal{L}_{\text {content }}(\vec{p}, \vec{x}, l)=\frac{1}{2} \sum_{i, j}\left(F_{i j}^{l}-P_{i j}^{l}\right)^{2}\tag{1}</script><p>​    损失函数$\mathcal{L}$对激活值$F^l_{ij}$的偏导为：</p>
<script type="math/tex; mode=display">
\frac{\partial \mathcal{L}_{\text {content }}}{\partial F_{i j}^{l}}= \begin{cases}\left(F^{l}-P^{l}\right)_{i j} & \text { if } F_{i j}^{l}>0 \\ 0 & \text { if } F_{i j}^{l}<0\end{cases}\tag{2}</script><p>​    为什么小于0 偏导是0，我理解是VGG19中激活函数用的<a href="https://www.jianshu.com/p/338afb1389c9" target="_blank" rel="noopener">ReLU</a>，ReLU在大于0才有激活值，小于0时ReLU值一直为0，所以偏导没有变化，为0。从中可以使用标准误差反向传播来计算图像$\vec{x}$的梯度。因此我们可以改变随机初始化的图像$\vec{x}$，直到它在CNN的某一层产生和原始图像相同的矩阵反馈$\vec{p}$。（有点照着原图依葫芦画瓢，就是随便初始化一个图像，然后特征都给你弄的和原图差不多一样，最终生成的图还会不像原图。）图1中5个内容重建图像其中：a图片来自VGG网络的卷积1_1、b图对应卷积2_1、c图对应卷积3_1、d图对应卷积4_1、e图对应卷积5_1。</p>
<p>​    在CNN网络每一层的特征反馈之上，我们构建了一个样式表示矩阵，用于计算不同滤波器反馈的特征之间的相关性，其中期望值取代了输入图像的空间扩展。这些特征的相关性是格拉姆矩阵$G^{l} \in \mathcal{R}^{N_{l} \times N_{l}}\in \mathcal{R}^{N_{l} \times N_{l}}$给定，$G^{l}_{ij}$是第$l$层第$i$个特征向量和第$j$个特征向量的内积：</p>
<script type="math/tex; mode=display">
G_{i j}^{l}=\sum_{k} F_{i k}^{l} F_{j k}^{l}\tag{3}</script><p>​    为了生成与给定图像的风格匹配的纹理（图 1，风格重建），我们使用白噪声图像的梯度下降来找到与原始图像的风格相匹配的另一张图像。这里是最小化原始图的Gram矩阵和生成图像的Gram矩阵的均方距离实现的。所以让矩阵$\vec{a}$和矩阵$\vec{x}$分别表示原始图像、生成图像，并且$A^l$、$G^l$是它们各自在$l$层的风格特征矩阵。该层对总损失的贡献是：</p>
<script type="math/tex; mode=display">
E_{l}=\frac{1}{4 N_{l}^{2} M_{l}^{2}} \sum_{i, j}\left(G_{i j}^{l}-A_{i j}^{l}\right)^{2}\tag{4}</script><p>上面公式（4）前面为什么是$\frac{1}{4 N_{l}^{2} M_{l}^{2}}$，猜测是为了归一化，也可能因为$G$、$A$本身就是内积是平方，再作差平方就是4次方，那除以$N_l^2M_l^2$偏导时候右侧里面$G$、$A$的平方展开来消去一些值，为什么是1/4，我想基于此平方的平方需要2次平方消去，所以除以4。吴恩达提到，这个可以看作权重，看作第一个超参数训练也可以。</p>
<p>总的风格损失对应：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{\text {style }}(\vec{a}, \vec{x})=\sum_{l=0}^{L} w_{l} E_{l}\tag{5}</script><p>这里的$w_l$看作每一层风格期望在总损失里面的权重。（上面所有层的风格期望加起来不就是总的风格损失嘛）对第$l$层在第$j$个神经元节点第$i^{th}$个过滤器的激活值$F_{i j}^{l}$的偏导数可计算如下：</p>
<script type="math/tex; mode=display">
\frac{\partial E_{l}}{\partial F_{i j}^{l}}= \begin{cases}\frac{1}{N_{l}^{2} M_{l}^{2}}\left(\left(F^{l}\right)^{\mathrm{T}}\left(G^{l}-A^{l}\right)\right)_{j i} & \text { if } F_{i j}^{l}>0 \\ 0 & \text { if } F_{i j}^{l}<0\end{cases}\tag{6}</script><p>$E_l$对网络中较为浅层的激活值的偏导可以较为方便地由误差逆传播算法计算而得。图1中5种重塑的风格图片是在这些卷积层上捕捉到的风格特征矩阵而生成：a图片风格特征是VGG网络的卷积1_1生成的，风格b图对应卷积1_1、卷积2_1，风格c图对应卷积1_1、卷积2_1、卷积3_1，风格d图对应卷积1_1、卷积2_1、卷积3_1、卷积4_1，风格e图对应卷积1_1、卷积2_1、卷积3_1、卷积4_1、卷积5_1。看到这些风格矩阵生成的照片是由不同子层的卷积合成，并不是单一的，猜测是风格的特征是多样化的，那么前面说不同子层提取到不同的风格特征，所以最后用多个不同层的卷积合成的图像风格比较全，比较像艺术画的风格。</p>
<p>公式6当$F$激活值小于0时候ReLU值一直为常数0，所以偏导为0。当$F$大于0时候，对$F_{ij}^l$偏导：</p>
<p><img src="/2022/05/14/ArtisticStyle/7.jpg" alt="7"></p>
<p>这里考虑$F^l$是列向量，Gram矩阵$G^l$可以表示为$(F^l)^T*F$，这里根据<a href="https://wenku.baidu.com/view/f7fa307a580216fc700afdb9.html#" target="_blank" rel="noopener">参考10</a>，得到$G$对$F$的导数就是$2F$，这里的公式7和公式6结果其实一样，因为公式6最后是整体$ji$排序，只需要公式6进行转置即可：由于Gram矩阵的转置还是自己，再参考$(AB)^T=B^TA^T，(A-B)^T=A^T-B^T$，公式6大于0部分转置后和公式7是等价的。    </p>
<p>​    为了使得生成的图片混合油画的风格和照片的内容，我们对白噪点图像对某1层网络中照片内容矩阵的距离和对CNN网络多层网中油画风格特征矩阵的距离最小化。所以我们让$\vec{p}$是照片、$\vec{a}$是艺术画像，那么总的损失函数为：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{\text {total }}(\vec{p}, \vec{a}, \vec{x})=\alpha \mathcal{L}_{\text {content }}(\vec{p}, \vec{x})+\beta \mathcal{L}_{\text {style }}(\vec{a}, \vec{x})\tag{8}</script><p>​    这里的$α​$和$β​$是内容和风格的重建比重。在图2中，我们用卷积4_2上面去抓取内容表征，而用卷积1_1、卷积2_1、卷积3_1、卷积4_1和卷积5_1来抓取风格表征（此时在这些卷积层上的比重$w_l=1/5​$，而其它层的$w_l=0​$，就是其它层的特征不用，所以乘以0消去，只用这5层特征来抓取风格特征，并且这些层的特征比重是均匀的，估计也可以不均，不均后某些风格肯定会有较大的比重，导致图像的风格会侧重某一些风格元素）。$α/β​$的值不是$1\times10^{-3}​$（图2中B，C，D）就是$1\times10^{-4}​$（比如图2中的E，F）。图3沿着列看的话，展示了损失函数中内容和风格在不同比重下的结果，并且风格表征仅仅限于：A图片风格特征是VGG网络的卷积1_1生成的，B图风格对应卷积1_1、卷积2_1，C图风格对应卷积1_1、卷积2_1、卷积3_1，D图风格对应卷积1_1、卷积2_1、卷积3_1、卷积4_1，E图风格对应卷积1_1、卷积2_1、卷积3_1、卷积4_1、卷积5_1生成。其中$w_l​$是1除以激活层的个数，是一个非0的损失权重。</p>
<p>​    鸣谢：这项工作由德国国家学术基金会（L.A.G.）、伯恩斯坦计算神经科学中心（FKZ 01GQ1002）资助，并在德国图宾根综合神经科学中心获得优秀倡议（EXC307）（M.B.,A.S.E,L.A.G.）。</p>
<h1 id="3-吴恩达作业代码"><a href="#3-吴恩达作业代码" class="headerlink" title="3.吴恩达作业代码"></a>3.吴恩达作业代码</h1><p>本代码<a href="https://www.heywhale.com/mw/project/5de71d8bca27f8002c4ce1e2" target="_blank" rel="noopener">参考吴恩达《深度学习》L4W4作业2</a>，此为中文版本，原本是英文版本，可自行搜索。资源文件在google的colab上传后下面代码即可使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">import</span> scipy.misc</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> imshow</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line">drive.mount(<span class="string">'/content/drive'</span>)</span><br><span class="line">sys.path.append(<span class="string">'/content/drive/MyDrive/style_res'</span>)</span><br><span class="line"><span class="keyword">from</span> nst_utils <span class="keyword">import</span> *  <span class="comment"># content.drive.MyDrive.style_res</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow.compat.v1 <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.compat.v1.train <span class="keyword">import</span> AdamOptimizer <span class="keyword">as</span> Adam</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 吴恩达用的tensorflow==1.2.1，我用的2.6</span></span><br><span class="line"></span><br><span class="line">version = tf.__version__</span><br><span class="line"></span><br><span class="line">print(<span class="string">'GPU能不能用？'</span>,tf.test.is_gpu_available())</span><br><span class="line"><span class="comment">#如何在Colab中导入自定义的包: https://www.cnblogs.com/cindycindy/p/13774112.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">model = load_vgg_model(<span class="string">"/content/drive/MyDrive/style_res/pretrained-model/imagenet-vgg-verydeep-19.mat"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_content_cost</span><span class="params">(a_C, a_G)</span>:</span></span><br><span class="line">	<span class="string">'''</span></span><br><span class="line"><span class="string">    Computes the content cost</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    a_C -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image C </span></span><br><span class="line"><span class="string">    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image G</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    J_content -- scalar that you compute using equation 1 above.</span></span><br><span class="line"><span class="string">	'''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve dimensions from a_G </span></span><br><span class="line">	m, n_H, n_W, n_C = a_G.get_shape().as_list()</span><br><span class="line">	<span class="comment"># Reshape a_C and a_G</span></span><br><span class="line">	a_C_unrolled = tf.reshape(a_C,shape=(n_H*n_W,n_C))</span><br><span class="line">	a_G_unrolled = tf.reshape(a_G,shape=(n_H*n_W,n_C)) <span class="comment"># 生成的激活图像大小和输入的图像一样大</span></span><br><span class="line"></span><br><span class="line">	<span class="comment"># compute the cost with tensorflow (≈1 line)</span></span><br><span class="line">	J_content = tf.reduce_sum(tf.square(tf.subtract(a_C_unrolled,a_G_unrolled)))/(<span class="number">4</span>*n_H*n_W*n_C)</span><br><span class="line">	<span class="keyword">return</span> J_content</span><br><span class="line"></span><br><span class="line">tf.reset_default_graph()    <span class="comment"># 用于清除默认图形堆栈并重置全局默认图形</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> test:</span><br><span class="line">	tf.set_random_seed(<span class="number">1</span>)</span><br><span class="line">	a_C = tf.random_normal([<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">3</span>],mean=<span class="number">1</span>,stddev=<span class="number">4</span>)</span><br><span class="line">	a_G = tf.random_normal([<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">3</span>],mean=<span class="number">1</span>,stddev=<span class="number">4</span>)</span><br><span class="line">	J_content = compute_content_cost(a_C,a_G)</span><br><span class="line">	print(<span class="string">"J_content = "</span> + str(J_content.eval()))</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_matrix</span><span class="params">(A)</span>:</span></span><br><span class="line">    GA=tf.matmul(A,tf.transpose(A))</span><br><span class="line">    <span class="keyword">return</span> GA</span><br><span class="line"></span><br><span class="line">tf.compat.v1.reset_default_graph()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> test:</span><br><span class="line">	tf.set_random_seed(<span class="number">1</span>)</span><br><span class="line">	A = tf.random_normal([<span class="number">3</span>,<span class="number">2</span>*<span class="number">1</span>],mean=<span class="number">1</span>,stddev=<span class="number">4</span>)</span><br><span class="line">	GA = gram_matrix(A)</span><br><span class="line">	print(<span class="string">'GA='</span>+str(GA.eval()))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_layer_style_cost</span><span class="params">(a_S, a_G)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    a_S -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image S </span></span><br><span class="line"><span class="string">    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image G</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    J_style_layer -- tensor representing a scalar value, style cost defined above by equation (2)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">     <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from a_G (≈1 line)</span></span><br><span class="line">    m, n_H, n_W, n_C = a_G.get_shape().as_list()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Reshape the images to have them of shape (n_C, n_H*n_W) (≈2 lines)</span></span><br><span class="line">    a_S = tf.reshape(a_S,shape=(n_H* n_W,n_C))</span><br><span class="line">    a_G = tf.reshape(a_G,shape=(n_H* n_W,n_C))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Computing gram_matrices for both images S and G (≈2 lines)</span></span><br><span class="line">    GS = gram_matrix(tf.transpose(a_S))</span><br><span class="line">    GG = gram_matrix(tf.transpose(a_G))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Computing the loss (≈1 line)</span></span><br><span class="line">    J_style_layer =tf.reduce_sum(tf.square(tf.subtract(GS,GG)))/(<span class="number">4</span>*(n_C*n_C)*(n_W * n_H) * (n_W * n_H))</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###    </span></span><br><span class="line">    <span class="keyword">return</span> J_style_layer</span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> test:</span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)</span><br><span class="line">    a_S = tf.random_normal([<span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>], mean=<span class="number">1</span>, stddev=<span class="number">4</span>)</span><br><span class="line">    a_G = tf.random_normal([<span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>], mean=<span class="number">1</span>, stddev=<span class="number">4</span>)</span><br><span class="line">    J_style_layer = compute_layer_style_cost(a_S, a_G)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"J_style_layer = "</span> + str(J_style_layer.eval()))</span><br><span class="line"></span><br><span class="line">STYLE_LAYERS = [</span><br><span class="line">    (<span class="string">'conv1_1'</span>, <span class="number">0.2</span>),</span><br><span class="line">    (<span class="string">'conv2_1'</span>, <span class="number">0.2</span>),</span><br><span class="line">    (<span class="string">'conv3_1'</span>, <span class="number">0.2</span>),</span><br><span class="line">    (<span class="string">'conv4_1'</span>, <span class="number">0.2</span>),</span><br><span class="line">    (<span class="string">'conv5_1'</span>, <span class="number">0.2</span>)]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_style_cost</span><span class="params">(model, STYLE_LAYERS)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the overall style cost from several chosen layers</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    model -- our tensorflow model</span></span><br><span class="line"><span class="string">    STYLE_LAYERS -- A python list containing:</span></span><br><span class="line"><span class="string">                        - the names of the layers we would like to extract style from</span></span><br><span class="line"><span class="string">                        - a coefficient for each of them</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    J_style -- tensor representing a scalar value, style cost defined above by equation (2)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize the overall style cost</span></span><br><span class="line">    J_style = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> layer_name, coeff <span class="keyword">in</span> STYLE_LAYERS:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Select the output tensor of the currently selected layer</span></span><br><span class="line">        out = model[layer_name]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set a_S to be the hidden layer activation from the layer we have selected, by running the session on out</span></span><br><span class="line">        a_S = sess.run(out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set a_G to be the hidden layer activation from same layer. Here, a_G references model[layer_name]</span></span><br><span class="line">        <span class="comment"># and isn't evaluated yet. Later in the code, we'll assign the image G as the model input, so that</span></span><br><span class="line">        <span class="comment"># when we run the session, this will be the activations drawn from the appropriate layer, with G as input.</span></span><br><span class="line">        a_G = out</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute style_cost for the current layer</span></span><br><span class="line">        J_style_layer = compute_layer_style_cost(a_S, a_G)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add coeff * J_style_layer of this layer to overall style cost</span></span><br><span class="line">        J_style += coeff * J_style_layer</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> J_style</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">total_cost</span><span class="params">(J_content, J_style, alpha = <span class="number">10</span>, beta = <span class="number">40</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the total cost function</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    J_content -- content cost coded above</span></span><br><span class="line"><span class="string">    J_style -- style cost coded above</span></span><br><span class="line"><span class="string">    alpha -- hyperparameter weighting the importance of the content cost</span></span><br><span class="line"><span class="string">    beta -- hyperparameter weighting the importance of the style cost</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    J -- total cost as defined by the formula above.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈1 line)</span></span><br><span class="line">    J = alpha*J_content + beta*J_style <span class="comment"># 总损失</span></span><br><span class="line">    <span class="keyword">return</span> J</span><br><span class="line"></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> test:</span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    J_content = np.random.randn()    </span><br><span class="line">    J_style = np.random.randn()</span><br><span class="line">    J = total_cost(J_content, J_style)</span><br><span class="line">    print(<span class="string">"J = "</span> + str(J))</span><br><span class="line">tf.reset_default_graph()</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scipy,cv2</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line"><span class="keyword">from</span> google.colab.patches <span class="keyword">import</span> cv2_imshow</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CONFIG</span>:</span></span><br><span class="line">    IMAGE_WIDTH = <span class="number">400</span></span><br><span class="line">    IMAGE_HEIGHT = <span class="number">300</span></span><br><span class="line">    COLOR_CHANNELS = <span class="number">3</span></span><br><span class="line">    NOISE_RATIO = <span class="number">0.6</span></span><br><span class="line">    MEANS = np.array([<span class="number">123.68</span>, <span class="number">116.779</span>, <span class="number">103.939</span>]).reshape((<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>)) </span><br><span class="line">    VGG_MODEL = <span class="string">'pretrained-model/imagenet-vgg-verydeep-19.mat'</span> <span class="comment"># Pick the VGG 19-layer model by from the paper "Very Deep Convolutional Networks for Large-Scale Image Recognition".</span></span><br><span class="line">    STYLE_IMAGE = <span class="string">'images/stone_style.jpg'</span> <span class="comment"># Style image to use.</span></span><br><span class="line">    CONTENT_IMAGE = <span class="string">'images/content300.jpg'</span> <span class="comment"># Content image to use.</span></span><br><span class="line">    OUTPUT_DIR = <span class="string">'output/'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reshape_and_normalize_image</span><span class="params">(image)</span>:</span></span><br><span class="line">    image = np.reshape(image, ((<span class="number">1</span>,) + image.shape))    </span><br><span class="line">    <span class="comment"># Substract the mean to match the expected input of VGG16</span></span><br><span class="line">    image = image - CONFIG.MEANS    </span><br><span class="line">    <span class="keyword">return</span> image</span><br><span class="line"></span><br><span class="line">drive.mount(<span class="string">'/content/drive'</span>)</span><br><span class="line">sys.path.append(<span class="string">'/content/drive/MyDrive/style_res'</span>)</span><br><span class="line"></span><br><span class="line">content_image = io.imread(<span class="string">'/content/drive/MyDrive/style_res/images/girl.jpg'</span>)</span><br><span class="line">style_image = io.imread(<span class="string">'/content/drive/MyDrive/style_res/images/monet.jpg'</span>)</span><br><span class="line">content_image1 = cv2.cvtColor(content_image,cv2.COLOR_BGR2RGB)</span><br><span class="line">style_image1 = cv2.cvtColor(style_image,cv2.COLOR_BGR2RGB)</span><br><span class="line">cv2_imshow(content_image1)</span><br><span class="line">cv2_imshow(style_image1)</span><br><span class="line">print(content_image1.shape,content_image.shape)</span><br><span class="line">content_image = reshape_and_normalize_image(content_image1)</span><br><span class="line">style_image = reshape_and_normalize_image(style_image1)</span><br><span class="line">print(content_image.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> imshow</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_noise_image</span><span class="params">(content_image, noise_ratio = CONFIG.NOISE_RATIO)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Generates a noisy image by adding random noise to the content_image</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Generate a random noise_image</span></span><br><span class="line">    noise_image = np.random.uniform(<span class="number">-20</span>, <span class="number">20</span>, (<span class="number">1</span>, CONFIG.IMAGE_HEIGHT, CONFIG.IMAGE_WIDTH, CONFIG.COLOR_CHANNELS)).astype(<span class="string">'float32'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set the input_image to be a weighted average of the content_image and a noise_image</span></span><br><span class="line">    input_image = noise_image * noise_ratio + content_image * (<span class="number">1</span> - noise_ratio)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> input_image</span><br><span class="line"></span><br><span class="line">generated_image = generate_noise_image(content_image)</span><br><span class="line">imshow(generated_image[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> nst_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line"><span class="keyword">import</span> tensorflow.compat.v1 <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">drive.mount(<span class="string">'/content/drive'</span>)</span><br><span class="line">model = load_vgg_model(<span class="string">"/content/drive/MyDrive/style_res/pretrained-model/imagenet-vgg-verydeep-19.mat"</span>)</span><br><span class="line"></span><br><span class="line">tf.compat.v1.disable_eager_execution()</span><br><span class="line">sess = tf.InteractiveSession() <span class="comment"># tf.InteractiveSession()是一种交替式的会话方式，它让自己成为了默认的会话，也就是说用户在单一会话的情境下，不需要指明用哪个会话也不需要更改会话运行的情况下，就可以运行起来，这就是默认的好处</span></span><br><span class="line">print(<span class="string">'content_image维度='</span>,content_image.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把内容图像输入训练好的vgg</span></span><br><span class="line">sess.run(model[<span class="string">'input'</span>].assign(content_image))</span><br><span class="line"><span class="comment"># 将a_C设置为张量，为层"conv4_2"提供隐藏层激活</span></span><br><span class="line">out=model[<span class="string">'conv4_2'</span>]</span><br><span class="line"><span class="comment"># 设置a_G为张量，为同一层提供隐藏层激活。</span></span><br><span class="line">a_C = sess.run(out)</span><br><span class="line">a_G = out</span><br><span class="line">print(<span class="string">'content_image='</span>,content_image.shape,a_G.shape)</span><br><span class="line"><span class="comment"># 4.使用a_C和a_G计算内容损失。</span></span><br><span class="line">J_content = compute_content_cost(a_C,a_G)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sess.run(model[<span class="string">'input'</span>].assign(style_image))</span><br><span class="line">J_style=compute_style_cost(model,STYLE_LAYERS)</span><br><span class="line"></span><br><span class="line">J = total_cost(J_content, J_style, alpha = <span class="number">16</span>, beta = <span class="number">45</span>) <span class="comment"># 内容/风格参数比重</span></span><br><span class="line"></span><br><span class="line">optimizer = tf.train.AdamOptimizer(<span class="number">2.0</span>) <span class="comment"># 用adam优化器</span></span><br><span class="line">train_step = optimizer.minimize(J) <span class="comment"># 优化J最小化</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_image</span><span class="params">(path, image)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Un-normalize the image so that it looks good</span></span><br><span class="line">    image = image + CONFIG.MEANS</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Clip and Save the image</span></span><br><span class="line">    image = np.clip(image[<span class="number">0</span>], <span class="number">0</span>, <span class="number">255</span>).astype(<span class="string">'uint8'</span>)</span><br><span class="line">    cv2.imwrite(path,image)</span><br><span class="line"><span class="comment"># content_image = io.imread('/content/drive/MyDrive/style_res/images/louvre_small.jpg')</span></span><br><span class="line"><span class="comment"># content_image1 = cv2.cvtColor(content_image,cv2.COLOR_BGR2RGB)</span></span><br><span class="line"><span class="comment"># save_image('/content/drive/MyDrive/style_res/output/'+'11.png',content_image1)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_nn</span><span class="params">(sess, input_image, num_iterations = <span class="number">250</span>)</span>:</span> <span class="comment"># 算法迭代次数，也会生成的图片张数</span></span><br><span class="line">  <span class="comment"># 实现model_nn（）函数，该函数初始化tensorflow计算图的变量，将输入图像（初始生成的图像）作为VGG16模型的输入，并运行train_step进行训练步骤</span></span><br><span class="line">    <span class="comment"># Initialize global variables (you need to run the session on the initializer)</span></span><br><span class="line">  sess.run(tf.global_variables_initializer())<span class="comment"># 不含有tf.Variable、tf.get_Variable的环境下可以不初始化tf变量</span></span><br><span class="line">    <span class="comment"># Run the noisy input image (initial generated image) through the model. Use assign().</span></span><br><span class="line">  generated_image=sess.run(model[<span class="string">'input'</span>].assign(input_image))   <span class="comment"># assign()函数将image作为input传给model</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">       <span class="comment"># Run the session on the train_step to minimize the total cost</span></span><br><span class="line">      sess.run(train_step) <span class="comment"># 去迭代总损失</span></span><br><span class="line">       <span class="comment"># Compute the generated image by running the session on the current model['input']</span></span><br><span class="line">      generated_image = sess.run(model[<span class="string">'input'</span>])</span><br><span class="line">       <span class="comment"># Print every 20 iteration.</span></span><br><span class="line">      <span class="keyword">if</span> i%<span class="number">20</span>==<span class="number">0</span>:</span><br><span class="line">          Jt, Jc, Js = sess.run([J, J_content, J_style])</span><br><span class="line">          print(<span class="string">"Iteration "</span> + str(i) + <span class="string">" :"</span>)</span><br><span class="line">          print(<span class="string">"total cost = "</span> + str(Jt))</span><br><span class="line">          print(<span class="string">"content cost = "</span> + str(Jc))</span><br><span class="line">          print(<span class="string">"style cost = "</span> + str(Js))</span><br><span class="line">      save_image(<span class="string">'/content/drive/MyDrive/style_res/output/'</span>+str(i)+<span class="string">'.png'</span>,generated_image)</span><br><span class="line">  save_image(<span class="string">'/content/drive/MyDrive/style_res/output/generated_image.jpg'</span>, generated_image)</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">return</span> generated_image</span><br><span class="line"></span><br><span class="line">model_nn(sess,generated_image)</span><br></pre></td></tr></table></figure>
<h1 id="4-结果展示"><a href="#4-结果展示" class="headerlink" title="4.结果展示"></a>4.结果展示</h1><p><img src="/2022/05/14/ArtisticStyle/04.png" alt="031"></p>
<p>​    上图是实际跑下来的合成效果并没有很好，可能是我参数调整的不好。用<a href="https://en.wikipedia.org/wiki/Claude_Monet" target="_blank" rel="noopener">monet</a>的野罂粟花风格和卢浮宫照片迭代250次后如上图所示，效果勉强。用monet.jpg风格的油画和女孩照片去合成图片时候，最后250次的结果也是不尽人意。在下图第3次迭代时候个人觉得内容的展现较为清晰，风格还不明显，反而好看些，就是整体有点暗淡，权当做复古风格吧。</p>
<p><img src="/2022/05/14/ArtisticStyle/03.png" alt="031"></p>
<h1 id="5-参考"><a href="#5-参考" class="headerlink" title="5.参考"></a>5.参考</h1><ol>
<li>论文：<a href="https://arxiv.org/pdf/1508.06576.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1508.06576.pdf</a></li>
<li>本论文讲解、代码：<a href="https://blog.csdn.net/shenxiaolu1984/article/details/52090012" target="_blank" rel="noopener">https://blog.csdn.net/shenxiaolu1984/article/details/52090012</a></li>
<li>Gram Matrix介绍：<a href="https://blog.csdn.net/bbbeoy/article/details/108195122" target="_blank" rel="noopener">https://blog.csdn.net/bbbeoy/article/details/108195122</a></li>
<li>gram矩阵性质：<a href="https://zhuanlan.zhihu.com/p/105470826" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/105470826</a></li>
<li>腹侧流图来源：<a href="https://www.researchgate.net/figure/The-dorsal-and-ventral-streams-of-the-visual-pathway-Beyond-area-V1-shown-at_fig1_274877087" target="_blank" rel="noopener">https://www.researchgate.net/figure/The-dorsal-and-ventral-streams-of-the-visual-pathway-Beyond-area-V1-shown-at_fig1_274877087</a></li>
<li>VGGnet介绍：<a href="https://blog.csdn.net/zziahgf/article/details/79614822" target="_blank" rel="noopener">https://blog.csdn.net/zziahgf/article/details/79614822</a></li>
<li>VGG19骨架：<a href="https://www.csdn.net/tags/OtDacg5sMzY3NzMtYmxvZwO0O0OO0O0O.html" target="_blank" rel="noopener">https://www.csdn.net/tags/OtDacg5sMzY3NzMtYmxvZwO0O0OO0O0O.html</a></li>
<li>均方差损失与交叉熵损失函数介绍：<a href="https://blog.csdn.net/weixin_41888257/article/details/104894141" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41888257/article/details/104894141</a></li>
<li>吴恩达风格损失函数视频：<a href="https://mooc.study.163.com/learn/2001281004?tid=2001392030#/learn/content?type=detail&amp;id=2001728704" target="_blank" rel="noopener">https://mooc.study.163.com/learn/2001281004?tid=2001392030#/learn/content?type=detail&amp;id=2001728704</a></li>
<li>求导公式：<a href="https://wenku.baidu.com/view/f7fa307a580216fc700afdb9.html" target="_blank" rel="noopener">https://wenku.baidu.com/view/f7fa307a580216fc700afdb9.html</a></li>
<li>论文相关拓展新闻：<a href="https://www.jiemian.com/article/806628.html" target="_blank" rel="noopener">https://www.jiemian.com/article/806628.html</a></li>
<li>拼图游戏：<a href="https://gallerix.ru/code/puzzle2/?f=16081&amp;cn" target="_blank" rel="noopener">https://gallerix.ru/code/puzzle2/?f=16081&amp;cn</a></li>
<li>视频风格重建：<a href="https://zhuanlan.zhihu.com/p/141257938" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/141257938</a></li>
<li>AdaIN-style代码：<a href="https://github.com/xunhuang1995/AdaIN-style" target="_blank" rel="noopener">https://github.com/xunhuang1995/AdaIN-style</a></li>
<li>部分AI开源项目：<a href="https://www.ainav.net/archives/favorites/opensource" target="_blank" rel="noopener">https://www.ainav.net/archives/favorites/opensource</a></li>
</ol>
<h1 id="6-后记"><a href="#6-后记" class="headerlink" title="6.后记"></a>6.后记</h1><p>​    抛去功利、志趣、功力不讲，更想把这几页看做是杂志的几页，世上有少量这么一批人，他们的工作和兴趣正好契合，想这样的人大概过得较为幸福，甚至更加幸福。在这个浮躁时代，能有时间翻几页杂志，已然感到欣慰，不再奢求其它。</p>
<p>​    上面VGG19骨架去点点是不是有5个池化层、16个卷积层，至于为什么这么多？哈哈，我想做加减几层也可以有好的性能，至于能不能和VGG19一样，估计看功力了，我想肯定有很多改进版本的VGG。</p>
<p>​    这些人最稀罕把其他人吐出来的再反刍，也不管原理，拿来就用，能水论文就行，可能能水论文本身就是一种能力吧，所谓厉害的人能水顶刊，但真正有价值的又有多少，经得起考验的又有多少？我也不以为那些水论文的都不想发有价值的东西，帽子实在是少，资源实在是不均，只能发点快消品踮起脚尖，只有这样才能看得更远，活的更潇洒吧，或者做的更多吧。也看到一些真正原创性的贡献，太少了。</p>
<p>​    再看看这篇论文，不管3个作者是不是德国人，总之是在德国一些机构资助下搞得。我想这和欧洲在14-17世纪的文艺复兴是否有联系，看了下阿尔布雷希特·杜勒（Albrecht Durer）、马蒂亚斯·格鲁内瓦尔德（Matthias Grunewald）、小汉斯·霍尔拜因、卢卡斯·克拉纳赫、汉斯·穆尔彻（Hans Multscher）、 <a href="https://zh.wikipedia.org/zh-cn/%E8%92%82%E7%88%BE%E6%9B%BC%C2%B7%E9%87%8C%E9%96%80%E6%96%BD%E5%A5%88%E5%BE%B7" target="_blank" rel="noopener">蒂尔曼·里门施耐德</a>等等一批杰出画家、雕刻家的作品，哪怕是放到现在也是不可多得的佳作。在15和16世纪的德国大师擅长的一个专业领域就是 <a href="https://gallerix.asia/pedia/graphic-art/" target="_blank" rel="noopener">图形艺术 </a>。 当时的许多画家在这一领域都取得了卓越的成就，其绘画和版画的重要性常常超过其绘画的重要性。当然这只是马后炮的臆想。但现在科学在艺术、科技、云计算、医学、深度学习、企业生产、自动驾驶等多领域的融合下，也许能迸发一些新思考，幸运的话有意想不到的结果。本文就是最好的例子，用的深度学习是类脑神经网络，数学原理是用Gram矩阵去提取风格特征、凸优化理论等，图像风格是大师的油画，内容是成像技术生成的照片，再用高性能显卡加速计算后可以得到一些模型。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/风格迁移/" rel="tag"># 风格迁移</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2022/01/22/tf-low-api/" rel="next" title="TF2低阶API笔记">
                <i class="fa fa-chevron-left"></i> TF2低阶API笔记
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/0.png"
                alt="Armigo" />
            
              <p class="site-author-name" itemprop="name">Armigo</p>
              <p class="site-description motion-element" itemprop="description">Pour un poisson salé, le plus important est heureux!</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">44</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">52</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/people/mou-mou-mou-49-9/activities" target="_blank" title="ZhiHu">
                      
                        <i class="fa fa-fw fa-globe"></i>ZhiHu</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-风格迁移概述"><span class="nav-text">1.风格迁移概述</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-方法"><span class="nav-text">2.方法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-吴恩达作业代码"><span class="nav-text">3.吴恩达作业代码</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-结果展示"><span class="nav-text">4.结果展示</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-参考"><span class="nav-text">5.参考</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-后记"><span class="nav-text">6.后记</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Armigo</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      访问人次
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  









  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three-waves.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'uLg1tJlQxJ5XwsDvzjI3tdRl-gzGzoHsz',
        appKey: 'IttKXBNG1WAsU13RknBgeRrP',
        placeholder: '来呀，快活呀≧∀≦',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.staticfile.org/MathJax/MathJax-2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
